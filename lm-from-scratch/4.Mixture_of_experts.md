# Mixture of Experts (MoE)

## 1. Introduction & Motivation
- MoE now central to high-performance systems (e.g., rumored GPT-4 as "GPT-MoE-1-BT" from NVIDIA leak; Grok, DeepSeek-V3, LLaMA 4)  
- In 2025, MoE outperforms dense architectures at same FLOPs (clear advantage in compute scaling); adopted East/West  
- Core goal: Maximize model quality per FLOP budget  
- Mental model correction: MoE ≠ domain-specialized experts (e.g., coding vs. English); instead, sparse activation of identical subnetworks (focus on FFNs)  
- Basic architecture: Standard transformer, but replace dense FFN with router + multiple smaller experts (sparse: activate k << total experts per token)  
  - Advantage: Same FLOPs as dense (if expert size ≈ dense FFN), but more parameters (e.g., for memorization)  
- Evidence of superiority:  
  - Fedus et al. 2022 (Switch Transformers): At fixed training FLOPs, more experts → lower loss (e.g., 128 experts beat dense)  
  - OlMoE (AI2, 2024): MoE converges faster than dense (pink MoE line steeper than teal dense)  
  - DeepSeek-V2 plot: Activated params vs. MMLU – MoE excels (sleight: ignores inactive params)  
- Systems benefit: Natural expert parallelism (shard experts across devices; route tokens sparsely → efficient multi-node scaling)  
- Open-source trajectory: Chinese labs pioneered (Qwen, DeepSeek); West catching up (Mixtral, Grok – closed, LLaMA 4 open MoE)  

| Model | Total Params | Active Params | Key Insight |
|-------|--------------|---------------|-------------|
| DeepSeek-V2 | 236B | 21B | MoE efficiency in real deployment |
| LLaMA 4 | Sparse MoE | N/A | Recent open adoption |
| Grok (xAI) | MoE | N/A | Frontier-scale sparse |

## 2. Challenges & Why Not Standard (Yet)
- Complexity: Systems overhead (routing across 256+ experts); infrastructure for multi-node sharding  
  - Q: Non-FLOP costs (e.g., bias loads) amplified in MoE? A: Yes, routing adds wall-clock overhead; expert-per-device ideal but hard  
- Optimization: Non-differentiable routing (discrete top-k decisions) → unstable/heuristic objectives  
- Trade-offs: Benefits shine at large scale (multi-node); smaller models may not justify complexity  
- Despite this, empirical wins make MoE "worth hyping" for FLOP-constrained settings  

## 3. Core MoE Components
- **Router/Gate**: Scores tokens → selects top-k experts (softmax-normalized logits → top-k)  
  - Equation: Gate scores = W_gate · h (hidden state); select top-k via argmax  
  - Output: Weighted sum of selected expert outputs (g_i * E_i(h))  
- **Experts**: Identical FFNs (e.g., SwiGLU); 2 shared (always active) + many fine-grained (sparse)  
- **Sparsity**: Top-k (k=2-8 typical); fine-grained experts (small, many) vs. coarse (large, few)  
  - Fine-grained: Better load balancing, but higher routing/comm costs  
- **Balancing Losses**: Prevent collapse (all tokens to few experts)  
  - Expert-level: Load balance (importance * switch fraction)  
  - Device-level: For sharding (balance across GPUs/nodes)  
  - Sequence-wise (DeepSeek-V3): Per-sequence balancing (mitigates OOD inference overload)  
- **Aux Loss-Free Variant** (DeepSeek-V3): Increment/decrement capacity factor b_i based on load (no extra loss term)  

| Routing Variant | Description | Pros/Cons |
|-----------------|-------------|-----------|
| Top-K | Softmax → select k highest | Simple; unstable if k=1 |
| Noisy Top-K | Add noise to logits | Stabilizes; prevents mode collapse |
| Switch Transformers | k=1 + capacity factor | Sparse; balancing via aux loss |
| DeepSeek (v1-v3) | Top-6/8; sigmoid gating (v3) | Balanced; top-M devices (v2+) |

## 4. Historical/Foundational Papers
- **Google Roots**: Fedus et al. 2022 (Switch Transformers) – 7x speedup with experts; OlMoE ablations confirm  
- **Open-Source Pioneers**:  
  - Qwen1.5-MoE: Upcycle dense 7B → MoE (copy FFNs, perturb, train router) → 2.7B active matches 7B dense  
  - DeepSeek-MoE (v1, ~2023): 16B total/2.8B active; 2 shared + 64 experts; top-6 routing; beats dense on benchmarks (fixed FLOPs)  
    - Naive sparse < smart (switch) < dense? No: Sparse > dense consistently  
- Trajectory shows architectures stabilize early (DeepSeek v1 ≈ v3 core)  

## 5. DeepSeek MoE Evolution (Core Case Study)
- **v1 (DeepSeek-MoE)**: 16B total/2.8B active; 2 shared + 64 fine-grained; top-6; aux losses (expert/device); standard top-k softmax  
- **v2**: 236B total/21B active; identical core; adds:  
  - Top-M devices: Subset to top-M devices first → top-k within (controls comm; e.g., avoid scattering to 100+ devices)  
  - Output comm balancing loss (balance return traffic)  
- **v3 (671B total/37B active)**: Core unchanged; refinements:  
  - Sigmoid gating (softer than softmax; normalized to 1)  
  - Aux loss-free (capacity b_i adjustment) + sequence-wise aux loss (OOD robustness)  
  - Drops: Output comm loss (jettisoned for simplicity)  
  - Keeps: Top-M devices  
- Key: "If it works, don't change" – v1 architecture nailed; scale via engineering  

| Version | Total/Active Params | Experts | Routing/Balancing Innovations |
|---------|---------------------|---------|-------------------------------|
| v1 | 16B/2.8B | 2 shared + 64 | Top-6; expert/device aux losses |
| v2 | 236B/21B | Same | +Top-M devices; +output comm loss |
| v3 | 671B/37B | Same | +Sigmoid; aux-free + seq-wise |

## 6. Non-MoE Components in DeepSeek-V3
- **MLA (Multi-head Latent Attention)**: KV cache compression  
  - Project h_t → low-dim c (latent); cache c; up-project to K/V on-demand  
  - Merge W_uK · Q via associativity → no extra FLOPs  
  - Compress queries too (training memory)  
  - RoPE compatibility: Apply to non-compressed dims (subtle fix)  
- **MTP (Multi-Token Prediction)**: Lightweight 1-layer transformer predicts +1 token ahead (despite diagram for multi)  
  - Loss: Standard next-token + future-token (shifts hidden states)  
  - Boosts: Reasoning/multi-step tasks (added detail: ~1-2% MMLU lift in similar works)  

## 7. Practical Tricks & Mitigations
- **Upcycling**: Copy dense FFNs → perturb → train router from scratch  
  - Wins: MiniCPM (dense → MoE: non-trivial perf bump); Qwen (2.7B active ≈ 7B dense)  
  - Cost-effective: Train dense, "upgrade" to larger effective model  
- **Overfitting**: MoE risks (more params); mitigate with massive SFT (DeepSeek-V3: 1.4M examples)  
- **Inference**: Sparse activation → low FLOP cost; but route OOD sequences carefully (seq-wise balancing helps)  

## 8. Q&A & Takeaways
- Q: Top-M in v3? A: Yes (kept); not all additions persist  
- Overall: MoE = sparsity for param efficiency; routing heuristics surprisingly robust; systems (comm, sharding) key at scale  
- 2025 Context: MoE dominant for FLOP-optimal training; future simplification hoped (e.g., differentiable routing?)  
- Added Detail: xAI's Grok-1.5 (2024) used early MoE; LLaMA 4's open release (mid-2025) democratized sparse scaling, enabling 1T+ param open models affordably  

This lecture demystifies MoE as "messy but magical" – empirical evolution from Google to DeepSeek shows sparse FFNs as the 2025 scaling king, blending architecture with systems for frontier efficiency.