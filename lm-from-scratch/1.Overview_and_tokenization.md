# Lecture 1: Course Overview and Tokenization

## 1. Why This Course Exists – The Real Motivation
- Crisis in the field: Researchers are increasingly disconnected from the underlying technology  
  - 8 years ago: researchers trained their own models  
  - 6 years ago: at least fine-tuned BERT-like models  
  - Today: many researchers only prompt closed APIs  
- Leaky abstractions: “string in → string out” hides everything; hard to do fundamental research without controlling the whole stack  
- Goal: enable fundamental research by forcing students to build everything from scratch  
- Philosophy: “To understand it, you have to build it”

## 2. The Scale Problem
- Frontier models (GPT-4 rumored 1.8T params, $100M+ training, xAI 200k H100 cluster, >$500B investment over 4 years) are out of reach  
- No public details (GPT-4 paper: “due to competitive landscape and safety, we disclose nothing”)  
- Small models are not always representative of frontier behavior:
  - FLOPs distribution changes with scale (small models: attention ≈ MLP; large models: MLP dominates)  
  - Emergent abilities (in-context learning, reasoning) appear suddenly at certain scale (Jason Wei 2022)

## 3. Three Types of Knowledge the Course Aims to Teach
1. Mechanics – how everything works (implementable at small scale)  
2. Mindset – “squeeze every FLOP”, take scaling seriously, maximize accuracy per resource (the real “bitter lesson”)  
3. Intuitions – which data/architecture decisions work (hardest to teach; scale-dependent)

Important clarification on the Bitter Lesson (Rich Sutton):
- Not “scale > algorithms”  
- Correct reading: “algorithms × scale” → efficiency is paramount at large scale  
- 2012–2019 ImageNet training saw 44× algorithmic speed-up (faster than Moore’s law)

Core framing question for the entire course:  
**“Given fixed compute + data budget, what is the best model you can build?”**

## 4. Brief History of Language Models
- Shannon → n-gram era (Google trained 5-grams on >2T tokens in 2007)  
- 2010s: neural LMs (Bengio 2003), seq2seq, Adam, attention, Transformer (2017), MoE, model parallelism research  
- Foundation model era: ELMo → BERT → T5  
- OpenAI’s pivotal contribution: take all existing ingredients + extreme engineering + scaling mindset → GPT-2 → GPT-3  
- Open-source wave: EleutherAI → Meta (OPT, LLaMA), BLOOM, DeepSeek, Alibaba Qwen, etc.

Levels of openness:
- Closed (GPT-4)  
- Open weights (LLaMA)  
- Open weights + detailed paper  
- Fully open-source (weights + exact data + full recipe) – still rare

## 5. Course Structure – Five Pillars (all centered on efficiency)

| Unit | Goal | Assignment Highlights |
|------|------|-----------------------|
| 1. Basics | Full pipeline working (tokenizer + model + training) | Implement BPE tokenizer from scratch, Transformer (no PyTorch nn.Transformer allowed), AdamW, training loop. Train on TinyStories/OpenWebText → leaderboard (90 min H100 budget) |
| 2. Systems | Maximize hardware utilization | Write Triton kernels, data/tensor/pipeline parallelism (baby FSDP), inference (prefill vs decode, speculative decoding) |
| 3. Scaling Laws | Predict optimal model size / tokens from small experiments | “Training API” with fake loss oracle → fit Chinchilla-style scaling laws → predict best hyperparams at large scale |
| 4. Data | Turn raw internet dumps into high-quality training data | Process real Common Crawl → filtering, deduplication, classifier-based quality → leaderboard (min perplexity per token budget) |
| 5. Alignment | Turn raw pretrained model into useful assistant | SFT → DPO → GRPO (Group Relative Policy Optimization – DeepSeek’s simplified RL method) |

Current regime: compute-constrained (GPU-poor) → aggressive filtering, 1-epoch training, BPE instead of byte models, etc.  
Frontier labs are shifting to data-constrained regime → design decisions will change dramatically.

## 6. Tokenization Section (Deep Dive)

### Why Tokenization Matters
- Bridges raw text (Unicode strings) ↔ integer sequences the model consumes  
- Must be reversible (encode ↔ decode round-trip)  
- Controls sequence length → directly impacts quadratic attention cost

### Evolution of Approaches (all shown live)

| Approach            | Vocab Size | Bytes/token | Problems |
|---------------------|------------|-------------|-----------------------------------------------|
| Character-level     | ~100k–150k | ~1.5        | Huge vocab waste on rare characters |
| Byte-level          | 256        | 1.0         | Very long sequences → attention blow-up |
| Word-level          | unbounded  | ~4–5        | Unknown word problem → <UNK> tokens |
| Subword (BPE)       | 32k–128k   | ~3–4        | Sweet spot used by almost all frontier models |

### Byte Pair Encoding (BPE) – the algorithm (1994 → GPT-2)

1. Start with raw text → UTF-8 bytes (0–255)  
2. Pre-tokenize (GPT-2 uses regex `'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+`)  
3. On each pre-tokenized segment, repeatedly:
   - Count all adjacent pairs  
   - Merge the most frequent pair into a new token (vocab index 256, 257, …)  
   - Update the training corpus representation  
   - Repeat num_merges times (typically 30k–100k merges)

Live demo with “cat hat” example:
- Initial: `[99,97,116,32,104,97,116]`  
- Merge `t h` → 256  
- Merge `th e` → 257  
- Merge `257 a` → 258  
- Sequence shrinks while vocab grows → better compression

Encoding new strings:
- Convert to bytes → apply merges in exactly the same order they were learned → final token IDs

### Practical Observations
- GPT-2/GPT-4 tokenizer compression ≈ 3–4 bytes per token  
- Leading spaces are intentional (from pre-tokenizer regex)  
- Numbers are split left-to-right (no semantic grouping)  
- BPE is still the dominant tokenizer in 2025 frontier models (tokenizer-free byte models promising but not yet scaled)

## 7. Takeaways & Outlook
- Efficiency is the unifying principle across everything  
- The course deliberately stays close to frontier best practices using only public/open information  
- Next lecture: deep dive into PyTorch internals, tensor cores, precise FLOP accounting, and resource monitoring

This lecture sets the tone: no black boxes, no starter code, extreme ownership of every component, and relentless focus on squeezing performance out of limited resources – exactly the mindset needed to eventually build frontier-scale models.