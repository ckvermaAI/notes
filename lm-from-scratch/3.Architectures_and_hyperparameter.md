# Architecture And Hyperparameters  

## 1. Introduction & Logistics
- Theme: Learn from others' experiences via evolutionary analysis of 19 new dense LLM releases in the last year (e.g., Command A, OLMo 2 Furious, SmolLM, Phi-4, Gemma 3, Qwen 2.5, InternLM, and many more)  
- Spreadsheet compiled of architecture changes from 2017 (original Transformer) to 2025  
- Sections: Architecture variations (activations, feed-forwards, attention, position embeddings); hyperparameters (hidden dim, inner projection, vocab size); brief on attention variants  
- No strong consensus on many choices (e.g., LayerNorm vs. RMSNorm, serial vs. parallel layers), but convergence on "LLaMA-like" architectures since ~2023  
- One near-universal choice: Pre-norm (since early GPT models)  
- Lecture draws from papers; ablations often from older works (e.g., 2020) due to fewer detailed industry ablations today

## 2. Transformer Recap & Variants
- Original Transformer (2017): Position embeddings at bottom; multi-head attention; post-norm LayerNorms after blocks; residual stream; MLP; final softmax  
- Student implementation variant: Pre-norm (LayerNorm before blocks); Rotary Position Embeddings (RoPE); SwiGLU in feed-forward; linear layers emit biases  
- Rationale: Modern consensus for better performance/stability (details below)  
- Recent innovations: Annual lecture updates needed (e.g., new papers like Command A, OLMo 2)  
- Convergent evolution: e.g., Position embeddings shifted from absolute/relative/Alibi to RoPE by 2023  

## 3. Architecture Variations Overview
- Survey shows variations but trends (e.g., all use pre-norm or equivalent)  
- Key themes: Stability, efficiency (flops + memory movement), empirical wins from ablations  

### LayerNorm & Normalization
- Pre-norm vs. Post-norm: Original used post-norm (LayerNorm after subcomponents in residual stream)  
  - Pre-norm (LayerNorm before non-residual blocks) dominant since early models (e.g., all modern LMs except OPT-350M, likely a mistake)  
  - Reasons: More stable training; removes need for LR warmup; constant gradient sizes (no blow-up); fewer loss spikes  
  - Evidence: Salazar & Nguyen (pre-norm + stability tricks match post-norm with warmup on MT tasks); Xiong 2020 (BERT tasks); gradient attenuation plots  
- New innovation (2024-2025): "Double norm" – LayerNorms both before and after blocks (Grok, Gemma 2); or after FFN/MHA (OLMo 2)  
  - Improves stability for large models; question from audience: Why LayerNorm in residual bad? → Messes with identity gradients in deep nets (e.g., LSTMs/SSMs struggle without)  
- LayerNorm vs. RMSNorm: Original used LayerNorm (subtract mean, divide by std dev + epsilon, scale by gamma, shift by beta)  
  - RMSNorm (drop mean subtraction and beta bias): Dominant since ~2023 (LLaMA, PaLM, Chinchilla, T5)  
  - Reasons: Equivalent performance; faster (fewer ops/parameters); optimizes memory movement (critical beyond flops)  
  - Evidence: Ivanov et al. 2023 – Normalizations are 0.17% flops but 25% runtime due to memory overhead; Narang et al. 2020 ablation – RMSNorm: 3.68 steps/sec vs. 3.5 (vanilla), lower final loss  
  - Exception: Cohere's Command A and R+ use LayerNorm (reason unclear)  
- Bias terms: Original FFN had biases (x → linear + bias → ReLU → linear + bias)  
  - Modern: Drop biases (pure matrix multiplies); stabilizes large-net training; no performance hit  
  - Theme: Optimization stability + efficiency  

### Activations & Feed-Forward Layers
- Activations: Original used ReLU (simple, but dying gradients)  
  - GeLU (Gaussian Error Linear Unit, ~2016): Smoother; dominant in GPT-3 era (probabilistic smoothing)  
  - Variants: QuickGeLU (faster approx.); SiLU/Swish (β=1); SwiGLU (SiLU gated; Shazeer 2020 – "divine benevolence," no deep explanation but empirically strong)  
  - SoLU (Softmax Linear Unit): Softmax for stability + gating; LLaMA 1 used but dropped for SwiGLU in LLaMA 2+  
  - Recent: ReLU^2 (squared ReLU) in Qwen 2.5 (simpler, efficient); GeGLU in PaLM/Chinchilla; SwiGLU in most 2024-2025 models (LLaMA 3+, Gemma 2, OLMo 2)  
  - Ablations: Shazeer 2020 – SwiGLU/SoLU beat GeLU/ReLU on perplexity; Dehghani 2023 – SoLU/GeLU edge; no universal winner, but SwiGLU converged  
- Feed-Forward (FFN/MLP) Size: Original: 4x hidden dim (d_ff = 4*d_model)  
  - Modern: 3-4x common; SmolLM: 2x (smaller models); ablation needed for scale  
- Gated vs. Non-Gated: Original non-gated (two linears + activation)  
  - Gated: Extra projection + element-wise multiply (e.g., GLU variants); increases params but effective capacity  
  - Evidence: Shazeer 2020 – Gated (GLU) > non-gated; Fedus 2022 – Gated stabilizes large models  
  - Modern: Most use gated (SwiGLU/GeGLU); non-gated in smaller/old models  

### Attention & Heads
- Serial vs. Parallel: Original serial (MHA then FFN)  
  - Parallel: Run MHA/FFN in parallel, add outputs (PaLM); saves a norm/add but minor perf hit  
  - Modern: Mix; LLaMA/GPT serial; PaLM/Gemma parallel  
  - Evidence: Narang 2021 – Parallel slightly worse but faster  
- Head Sizes: Original: Even split (d_model / num_heads)  
  - Modern: Often power-of-2 (128); multi-query/grouped-query for inference (see below)  

### Position Embeddings
- Original: Absolute (learned/fixed sinusoidal)  
  - Variants: Relative (T5 bias); Alibi (linear bias); RoPE (rotary, 2019)  
  - Convergence: RoPE since 2023 (all models); rotates queries/keys for relative positioning  
  - Reasons: Better length extrapolation; empirically superior  
  - Ablations: Su et al. 2021 – RoPE > others on extrapolation  

## 4. Hyperparameters
- Choices: Hidden dim (d_model), FFN inner dim (multiple of hidden), num layers, num heads, vocab size, etc.  
- Select intelligently (not randomly); use scaling laws (next lecture)  
- Hidden dim: Balance with layers (deeper > wider sometimes)  
- FFN inner: 3-4x hidden common  
- Vocab size: Larger reduces sequence length (better for quadratic attention) but more params  
- Heads: 8-32; tie to efficiency  

## 5. Stability Interventions (New in 2024-2025)
- Motivation: Large models unstable (loss spikes, NaNs); softmax issues (exp overflow/underflow, large logits)  
- Z-Loss: Auxiliary loss (α * (log Z)^2) on output softmax normalizer Z (Devlin 2014 MT; PaLM pioneer in LM)  
  - Forces log Z ≈ 0 (stable); used in Baichuan 2, DCLM, OLMo 2 (α=1e-4)  
- QK Norm: LayerNorm on Q/K before dot product (Dehghani 2023 ViT; Chameleon/Idefics multimodal)  
  - Bounds softmax inputs; stabilizes; Gemma 2, DCLM, OLMo 2  
- Soft-Cap Logits: Tanh clip on attention logits (Gemma 2, OLMo 2); less popular  
  - Evidence: NVIDIA ablation – Soft-cap worsens perplexity; QK norm improves (aggressive LR)  
- LayerNorms: Extra norms shockingly effective for stability without perf loss  
- Joke: "LayerNorm all the things"  

## 6. Attention Variants
- GQA/MQA: For inference efficiency (KV cache bottleneck)  
  - MQA: Multi queries, single K/V head (shares KV)  
  - GQA: Groups (reduce KV heads by factor)  
  - Flops same; memory access lower (arithmetic intensity: high with large batch/seq)  
  - Inference: Autoregressive → poor intensity (n/d term); MQA/GQA fix (divide by heads)  
  - Evidence: MQA hurts quality slightly; GQA balances  
- Sparse/Sliding Window: Original sparse (OpenAI 2019); local windows + diagonals  
  - Modern: Sliding window (local attention per layer; receptive field = window * layers)  
- Hybrid: LLaMA 4/Gemma/Cohere Command A – Every 4 blocks: 1 full attention (no RoPE); 3 sliding window (with RoPE)  
  - Enables 10M+ token context; controls compute; RoPE only local (better extrapolation)  

## 7. Q&A and Takeaways
- Q: Future-proof lessons? → Empirical evolution; stability/efficiency key; memory > flops  
- Q: QKV norm at inference? → Yes (learned params)  
- Overall: Architectures evolve empirically; stability critical at scale; efficiency beyond flops (memory movement); SwiGLU/RoPE/RMSNorm/pre-norm consensus; new: double norm, z-loss, QK norm, hybrid attention  
- Next: MOE, DeepSeek V3 (push to after if needed)  

Added details: As of Dec 05, 2025, LLaMA 4's 10M context is state-of-the-art; references to 2025 models align with transcript; explanations grounded in scaling trends (e.g., memory-bound GPUs like H100/A100).