# PyTorch Primitives and Resource Accounting  


## 1. Motivation via Napkin Math
- Training a 70B-parameter transformer on 15T tokens with 1 024 × H100s  
  → ~6 × 70e9 parameters × 15e12 tokens = total FLOPs  
  → H100 peak ~1 PFLOP/s (bf16 tensor cores), MFU ≈ 0.5 → ~144 days
- Largest model you can fit in 8 × H100 (80 GB each) with AdamW, no tricks  
  → ~40B parameters (16 bytes/parameter: params + grads + Adam m/v in FP32)
- Core message: Efficiency is everything → you must be able to do precise resource accounting (memory + FLOPs) on a napkin.

## 2. Memory Accounting – Floating-Point Formats
| Format   | Bits | Bytes | Dynamic Range | Precision | Typical Use in 2025 |
|----------|------|-------|----------------|-----------|---------------------|
| FP32     | 32   | 4     | ≈10⁻³⁸..10³⁸   | High      | Parameters, optimizer states, gradients (safe default) |
| FP16     | 16   | 2     | Very small     | Medium    | Rarely used for training (underflow issues) |
| BF16     | 16   | 2     | Same as FP32   | Low       | Forward/backward matmuls (standard for speed) |
| FP8 (E4M3/E5M2) | 8 | 1 | Two variants   | Very low  | H100-only, cutting-edge mixed-precision training |

Mixed-precision recipe (2025 best practice):
- Store parameters, gradients, optimizer states in FP32
- Cast to BF16 (or FP8) only for the actual matmul-heavy forward/backward
- Attention softmax often kept in FP32 for numerical stability

## 3. Tensors in PyTorch – The Fundamentals
- Tensors are just pointers + metadata (data pointer + shape + stride + dtype + device)
- Strides explain views:
  ```python
  x = torch.arange(6).view(2,3)        # [[0,1,2],[3,4,5]]
  y = x[:,1, :]                         # view, same storage
  x.transpose(0,1)                     # non-contiguous view
  x.contiguous()                       # forces copy
  ```
- Always know where your tensor lives: `.to('cuda')` or create directly on GPU
- Use `torch.cuda.memory_allocated()` for sanity checks

## 4. Helpful Libraries for Clarity
- Einops (`rearrange`, `reduce`, `einsum`) – replaces mysterious `-1`, `-2` indexing
  ```python
  # instead of x @ w.transpose(-2,-1)
  einsum(x, w, "b s h1, b s h2 -> b s h1 h2")
  ```
- Highly recommended for Assignment 1 (cleaner, less bug-prone code)

## 5. Compute Accounting – FLOPs
- One floating-point operation (FLOP) = one multiply or one add
- Matrix multiplication A (m×n) × B (n×p) → 2×m×n×p FLOPs (multiply + add for each output element)
- Rule of thumb for almost all modern models (including transformers):
  - Forward pass ≈ 2 × #parameters × #tokens
  - Backward pass ≈ 4 × #parameters × #tokens
  → Total training ≈ 6 × #parameters × #tokens FLOPs
- This “6N” rule is why the napkin calculation at the beginning used 6×P×T

## 6. Model FLOPs Utilization (MFU)
```
MFU = (actual achieved FLOPs/s) / (theoretical peak from NVIDIA datasheet)
```
- >50% is good, >70–80% is excellent for large models
- Measured only on model FLOPs (ignores data loading, all-reduce, etc.)
- BF16/FP16 usually gives higher raw TFLOP/s than FP32, but peak numbers are optimistic

## 7. Memory Breakdown for Training (per-GPU)
| Component               | Size (in #parameters) | Typical dtype | Bytes/parameter |
|-------------------------|-----------------------|---------------|-----------------|
| Model parameters        | 1×                    | FP32          | 4               |
| Gradients               | 1×                    | FP32          | 4               |
| Optimizer states (AdamW)| 2× (momentum + variance)| FP32         | 8               |
| Activations             | Depends on B, T, layers | BF16/FP32   | highly variable |
→ Rough total without activations ≈ 18–20 bytes/parameter for AdamW + BF16 compute  
→ With activations → easily 30–60+ bytes/parameter for long sequences

## 8. Parameter Initialization
- Default `torch.nn.init.normal_` or `kaiming_uniform_` → variance explodes with depth
- Standard fix: divide by √fan_in (Xavier/Glorot initialization)
  ```python
  torch.nn.init.normal_(w, std=1.0 / math.sqrt(w.shape[0]))
  ```

## 9. Implementing a Custom Optimizer (AdaGrad example)
```python
class AdaGrad(torch.optim.Optimizer):
    def __step__(self, params, ...):
        for p in params:
            g = p.grad
            state = self.state[p]
            if 'g2' not in state:
                state['g2'] = torch.zeros_like(p)
            state['g2'] += g * g
            p.data.addcdiv_(g, state['g2'].sqrt() + 1e-8, value=-lr)
```
- Assignment 1 requires you to implement AdamW from scratch (no `torch.optim.AdamW`)

## 10. Training Loop Essentials
- Always set deterministic seeds (multiple sources: torch, cuda, numpy, python random)
- Use `torch.save` / `torch.load` for both model + optimizer + step count (checkpointing is mandatory)
- Data loading tip: `np.memmap` for huge tokenized datasets (don’t load 2 TB into RAM

## 11. Advanced Topics Touched On
- Activation checkpointing (trade compute for memory by recomputing activations in backward)
- Mixed-precision tools in PyTorch (`torch.cuda.amp`, `torch.compile` + `bfloat16`)
- Training is much harder in low precision than inference (you can quantize to 4-bit or lower after training)

## 12. Key Takeaways
1. You must be able to compute exact memory and FLOPs for any model on a napkin.
2. 6×#params×#tokens is the dominant term for training cost in 2025-scale models.
3. BF16 + FP32 master weights is the sweet spot for speed/stability.
4. Views are free, `contiguous()` and `reshape` can allocate.
5. Einops makes code readable and less error-prone.
6. MFU > 50% is table stakes for serious training runs.
7. Assignment 1 will force you to apply every single one of these concepts to a full transformer implementation.

This lecture turns the abstract “efficiency is everything” mantra from Lecture 1 into concrete, quantifiable engineering practice — the foundation you need before diving into the transformer architecture itself.