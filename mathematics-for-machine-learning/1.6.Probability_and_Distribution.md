# Probability and Distributions

## Introduction to Probability

Probability is the study of uncertainty, often conceptualized as the fraction of times an event occurs or as a degree of belief about an event. It is used to quantify the likelihood of outcomes in experiments, particularly in machine learning, where uncertainty arises in data, models, and predictions. Central to this study is the concept of a *random variable*, a function mapping outcomes of random experiments to properties of interest, and its associated *probability distribution*, which measures the likelihood of specific outcomes or sets of outcomes. 

## 6.1 Construction of a Probability Space

### Overview

Probability theory establishes a mathematical framework to describe random outcomes of experiments, enabling automated reasoning and generalizing logical reasoning. For instance, repeated coin tosses reveal patterns in average outcomes, which probability theory models systematically.

### 6.1.1 Probability and Random Variables

- **Sample Space ($Ω$)**: The set of all possible outcomes of an experiment. For example, for two coin tosses, $Ω = \{hh, tt, ht, th\}$, where "h" denotes heads and "t" tails. The sample space is also known by other names in different contexts, such as "state space," "sample description space," or "possibility space," but these terms may have specific meanings in other fields (e.g., dynamical systems).
  
- **Event Space ($A$)**: A collection of subsets of $Ω$, representing potential results observable at the experiment's end. For discrete probability distributions, $A$ is often the power set of $Ω$, meaning it includes all possible subsets of outcomes.

- **Probability Measure ($P$)**: A function assigning a number $P(A)$ to each event $A \in A$, representing the probability or degree of belief in the event's occurrence. Probabilities must satisfy $0 \leq P(A) \leq 1$ for any event $A$, and the total probability over the sample space must be $P(Ω) = 1$.

In machine learning, direct reference to the probability space is often avoided. Instead, focus is placed on probabilities of quantities of interest, represented in a *target space* ($T$). A *random variable* $X: Ω \rightarrow T$ maps outcomes from $Ω$ to values in $T$, facilitating analysis in a more convenient space. For example, in the case of two coin tosses, a random variable $X$ might count the number of heads, mapping $hh \rightarrow 2$, $ht \rightarrow 1$, $th \rightarrow 1$, and $tt \rightarrow 0$, with $T = \{0, 1, 2\}$. The probability of events in $T$, denoted $P_X(S)$ for subsets $S \subseteq T$, is derived from the underlying probability space.

The term "random variable" is noted as misleading, as it is neither random nor a variable but a deterministic function. For finite $Ω$ and $T$, a random variable can be thought of as a lookup table.

The nature of the target space $T$ determines the type of random variable:

- **Discrete Random Variable**: When $T$ is finite or countably infinite (Section 6.2.1).
- **Continuous Random Variable**: When $T = \mathbb{R}$ or $T = \mathbb{R}^D$ (Section 6.2.2).

**Example 6.1**: A toy example models a funfair game of drawing two coins (with replacement) from a bag containing US (\$) and UK (£) coins. The sample space is $Ω = \{(\$, \$), (\$, £), (£, \$), (£, £)\}$, with a probability of drawing a \$ coin at 0.3. A random variable $X$ counts the number of $ coins drawn, mapping to $T = \{0, 1, 2\}$. The draws are independent (Section 6.4.5), and the probability mass function (pmf) of $X$ is calculated as:

- $P(X = 2) = P((\$, \$)) = P(\$) \cdot P(\$) = 0.3 \cdot 0.3 = 0.09$
- $P(X = 1) = P((\$, £)) + P((£, \$)) = 0.3 \cdot 0.7 + 0.7 \cdot 0.3 = 0.42$
- $P(X = 0) = P((£, £)) = P(£) \cdot P(£) = 0.7 \cdot 0.7 = 0.49$

This example illustrates how probabilities in $Ω$ are transformed via $X$ to probabilities in $T$, using the concept of the *pre-image* of a set $S \subseteq T$, defined as $X^{-1}(S) = \{\omega \in Ω : X(\omega) \in S\}$. The probability is then $P_X(S) = P(X \in S) = P(X^{-1}(S))$. The function $P_X$, or equivalently $P \circ X^{-1}$, is the *law* or *distribution* of the random variable $X$.

### 6.1.2 Statistics

Probability and statistics address different aspects of uncertainty. Probability starts with a model (using random variables to capture uncertainty) and derives outcomes, while statistics observes data and infers the underlying process. Machine learning aligns closely with statistics, aiming to construct models that represent data-generating processes. A key concern in machine learning is *generalization error* (Chapter 8), which involves assessing performance on future, unseen data, relying on both probability and statistics.

## 6.2 Discrete and Continuous Probabilities

The description of probability distributions depends on whether the target space $T$ is discrete or continuous, affecting how probabilities are specified.

- **Discrete Random Variables**: Probabilities are specified for specific values, $P(X = x)$, using the *probability mass function* (pmf).
- **Continuous Random Variables**: Probabilities are specified over intervals, $P(a \leq X \leq b)$, often using the *cumulative distribution function* (cdf), $P(X \leq x)$.

The text uses "univariate distribution" for single random variables (states denoted by non-bold $x$) and "multivariate distribution" for multiple random variables (states denoted by bold $x$).

### 6.2.1 Discrete Probabilities

For discrete random variables, the probability distribution can be visualized as a multidimensional array. For two random variables $X$ and $Y$, the *joint probability* $P(X = x_i, Y = y_j)$ is the probability of both events occurring, calculated as $P(X = x_i, Y = y_j) = \frac{n_{ij}}{N}$, where $n_{ij}$ is the number of events with states $x_i$ and $y_j$, and $N$ is the total number of events. The joint probability is also the probability of the intersection, $P(X = x_i \cap Y = y_j)$.

The *marginal probability* $P(X = x)$ is the probability of $X = x$ regardless of $Y$, obtained by summing over all possible states of $Y$. Similarly, $P(Y = y)$ is the marginal probability of $Y$. Notationally, $X \sim p(x)$ indicates that $X$ is distributed according to $p(x)$.

The *conditional probability* $P(Y = y \mid X = x)$ is the fraction of instances where $Y = y$ given $X = x$, often written as $p(y \mid x)$.

**Example 6.2**: Consider two random variables $X$ (five states) and $Y$ (three states), with joint frequencies $n_{ij}$ and total events $N$. Column sums $c_i = \sum_{j=1}^3 n_{ij}$ and row sums $r_j = \sum_{i=1}^5 n_{ij}$ are used to compute marginal probabilities:

- $P(X = x_i) = \frac{c_i}{N} = \frac{\sum_{j=1}^3 n_{ij}}{N}$
- $P(Y = y_j) = \frac{r_j}{N} = \frac{\sum_{i=1}^5 n_{ij}}{N}$

For discrete random variables with finite states, probabilities sum to one:

- $\sum_{i=1}^5 P(X = x_i) = 1$
- $\sum_{j=1}^3 P(Y = y_j) = 1$

Conditional probabilities are computed as fractions of row or column totals:

- $P(Y = y_j \mid X = x_i) = \frac{n_{ij}}{c_i}$
- $P(X = x_i \mid Y = y_j) = \frac{n_{ij}}{r_j}$

In machine learning, discrete probability distributions model *categorical variables* (variables with a finite set of unordered values), such as features (e.g., university degree for salary prediction) or labels (e.g., letters in handwriting recognition). They are also used in probabilistic models combining continuous distributions (Chapter 11).

### 6.2.2 Continuous Probabilities

Continuous random variables have target spaces as intervals of the real line, $\mathbb{R}$, or higher-dimensional spaces, $\mathbb{R}^D$. 

**Definition 6.1 (Probability Density Function)**: A function $f: \mathbb{R}^D \rightarrow \mathbb{R}$ is a *probability density function* (pdf) if:

1. $\forall \boldsymbol{x} \in \mathbb{R}^D: f(\boldsymbol{x}) \geq 0$
2. Its integral exists and $\int_{\mathbb{R}^D} f(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} = 1$

For a continuous random variable $X$, the probability over an interval is:

- $P(a \leq X \leq b) = \int_a^b f(x) \mathrm{d} x$

This association is the *law* or *distribution* of $X$. Unlike discrete random variables, the probability of a continuous random variable taking a specific value, $P(X = x)$, is zero, as intervals where $a = b$ have zero measure.

**Definition 6.2 (Cumulative Distribution Function)**: The *cumulative distribution function* (cdf) of a multivariate real-valued random variable $X$ with states $\boldsymbol{x} \in \mathbb{R}^D$ is:

- $F_X(\boldsymbol{x}) = P(X_1 \leq x_1, \ldots, X_D \leq x_D)$

where $X = [X_1, \ldots, X_D]^{\top}$ and $\boldsymbol{x} = [x_1, \ldots, x_D]^{\top}$. The cdf can be expressed as an integral of the pdf:

- $F_X(\boldsymbol{x}) = \int_{-\infty}^{x_1} \cdots \int_{-\infty}^{x_D} f(z_1, \ldots, z_D) \mathrm{d} z_1 \cdots \mathrm{d} z_D$

The text distinguishes two concepts under the term "distribution":

1. The pdf $f(x)$, a non-negative function integrating to one.
2. The law of $X$, associating $X$ with $f(x)$.

For brevity, the text often uses $p(x)$ to denote both pdfs and cdfs, relying on context, but emphasizes the need for careful distinction in Section 6.7.

### 6.2.3 Contrasting Discrete and Continuous Distributions

Both discrete and continuous distributions must satisfy normalization (total probability equals one), but their properties differ:

- **Discrete**: Probabilities for each state lie in $[0, 1]$, with $\sum_{x \in T} P(X = x) = 1$.
- **Continuous**: The pdf $f(x)$ integrates to one, $\int f(x) \mathrm{d} x = 1$, but $f(x)$ itself may exceed 1, as it represents density, not probability.

**Example 6.3 (Uniform Distribution)**: This example illustrates differences using the uniform distribution, where each state is equally likely.

- **Discrete Uniform**: Let $Z$ be a discrete random variable with states $\{-1.1, 0.3, 1.5\}$. The pmf is:

  - $P(Z = z) = \frac{1}{3}$ for $z \in \{-1.1, 0.3, 1.5\}$

  This can be visualized as a graph with states on the x-axis and probabilities on the y-axis (Figure 6.3(a)).

- **Continuous Uniform**: Let $X$ be a continuous random variable on $[0.9, 1.6]$. The pdf must satisfy $\int_{0.9}^{1.6} p(x) \mathrm{d} x = 1$, so:

  - $p(x) = \frac{1}{1.6 - 0.9} = \frac{1}{0.7} \approx 1.43$ for $0.9 \leq x \leq 1.6$

  Here, the pdf exceeds 1, but the total area under the curve equals 1 (Figure 6.3(b)).

The example emphasizes that discrete states may have no inherent order (e.g., colors), but numerical states (e.g., $\{-1.1, 0.3, 1.5\}$) allow ordering, useful for computing expected values (Section 6.4.1).

Machine learning literature often blurs distinctions between sample space $Ω$, target space $T$, and random variable $X$, using $p(x)$ ambiguously. Table 6.1 clarifies nomenclature:

| **Type**      | **"Point Probability"**       | **"Interval Probability"**       |
|---------------|-------------------------------|----------------------------------|
| **Discrete**  | $P(X = x)$ (pmf)             | Not applicable                  |
| **Continuous**| $p(x)$ (pdf)                 | $P(X \leq x)$ (cdf)             |

The text notes the technically incorrect but common practice of using "probability distribution" for both pmfs and pdfs, relying on context for clarity.

## 6.3 Sum Rule, Product Rule, and Bayes' Theorem

Probability theory extends logical reasoning, with its rules derived from the desiderata of Section 6.1.1 (Jaynes, 2003). In probabilistic modeling (Section 8.4), two fundamental rules underpin all operations: the sum rule and the product rule.

For random variables $X$ and $Y$ with joint distribution $p(\boldsymbol{x}, \boldsymbol{y})$, marginal distributions $p(\boldsymbol{x})$ and $p(\boldsymbol{y})$, and conditional distribution $p(\boldsymbol{y} \mid \boldsymbol{x})$, the rules are:

- **Sum Rule (Marginalization Property)**: The marginal distribution is obtained by summing (discrete) or integrating (continuous) over the other variable:

  - $p(\boldsymbol{x}) = \sum_{\boldsymbol{y} \in Y} p(\boldsymbol{x}, \boldsymbol{y})$ if $\boldsymbol{y}$ is discrete
  - $p(\boldsymbol{x}) = \int_Y p(\boldsymbol{x}, \boldsymbol{y}) \mathrm{d} \boldsymbol{y}$ if $\boldsymbol{y}$ is continuous

  where $Y$ is the target space of $Y$. For multivariate distributions with $\boldsymbol{x} = [x_1, \ldots, x_D]^{\top}$, marginalization over all but one variable yields:

  - $p(x_i) = \int p(x_1, \ldots, x_D) \mathrm{d} \boldsymbol{x}_{\backslash i}$

  where $\backslash i$ denotes all variables except $x_i$. The sum rule poses computational challenges in probabilistic modeling, as high-dimensional sums or integrals are computationally hard, lacking polynomial-time exact algorithms.

- **Product Rule**: The joint distribution can be factorized into a marginal and a conditional distribution:

  - $p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{y} \mid \boldsymbol{x}) p(\boldsymbol{x})$

  Since $p(\boldsymbol{x}, \boldsymbol{y})$ is symmetric, it also implies $p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{x} \mid \boldsymbol{y}) p(\boldsymbol{y})$. For discrete random variables, this is expressed using pmfs, and for continuous, using pdfs.

**Bayes' Theorem**: In Bayesian statistics and machine learning, Bayes' theorem is used to infer unobserved (latent) variables $\boldsymbol{x}$ from observed variables $\boldsymbol{y}$, given prior knowledge $p(\boldsymbol{x})$ and a relationship $p(\boldsymbol{y} \mid \boldsymbol{x})$. It is derived from the product rule:

- $p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{x} \mid \boldsymbol{y}) p(\boldsymbol{y})$
- $p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{y} \mid \boldsymbol{x}) p(\boldsymbol{x})$

Equating these, Bayes' theorem is:

- $p(\boldsymbol{x} \mid \boldsymbol{y}) = \frac{p(\boldsymbol{y} \mid \boldsymbol{x}) p(\boldsymbol{x})}{p(\boldsymbol{y})}$

where:

- **Prior ($p(\boldsymbol{x})$)**: Represents prior knowledge about $\boldsymbol{x}$ before observing data. The prior must be non-zero for all plausible $\boldsymbol{x}$, even if rare.
- **Likelihood ($p(\boldsymbol{y} \mid \boldsymbol{x})$)**: Describes the relationship between $\boldsymbol{x}$ and $\boldsymbol{y}$, often called the "probability of $\boldsymbol{y}$ given $\boldsymbol{x}$" (or "likelihood of $\boldsymbol{x}$ given $\boldsymbol{y}$," but not the likelihood of $\boldsymbol{y}$. It is a distribution in $\boldsymbol{y}$, not $\boldsymbol{x}$).
- **Posterior ($p(\boldsymbol{x} \mid \boldsymbol{y})$)**: The quantity of interest, expressing what is known about $\boldsymbol{x}$ after observing $\boldsymbol{y}$.
- **Marginal Likelihood/Evidence ($p(\boldsymbol{y})$)**: Normalizes the posterior, computed as:

  - $p(\boldsymbol{y}) = \int p(\boldsymbol{y} \mid \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} = \mathbb{E}_X[p(\boldsymbol{y} \mid \boldsymbol{x})]$

  The marginal likelihood is the expected likelihood under the prior and is crucial for Bayesian model selection (Section 8.6). Its computation is often challenging due to the integral.

Bayes' theorem inverts the relationship $p(\boldsymbol{y} \mid \boldsymbol{x})$ to $p(\boldsymbol{x} \mid \boldsymbol{y})$, earning it the name "probabilistic inverse." In Bayesian statistics, the posterior encapsulates all information from the prior and data, making it valuable for decision-making systems, such as model-based reinforcement learning, where using the full posterior improves robustness (Deisenroth et al., 2015). Focusing on posterior statistics (e.g., maximum a posteriori estimation, Section 8.3) may lose information, as discussed further in Chapter 9 on linear regression.

## 6.4 Summary Statistics and Independence

Summary statistics and comparisons of random variables are essential for understanding distributions. A *statistic* of a random variable is a deterministic function of that variable. This section covers means, variances, independence, and inner products of random variables.

### 6.4.1 Means and Covariances

**Definition 6.3 (Expected Value)**: The *expected value* of a function $g: \mathbb{R} \rightarrow \mathbb{R}$ of a univariate random variable $X \sim p(x)$ is:

- For continuous $X$:

  - $\mathbb{E}_X[g(x)] = \int_X g(x) p(x) \mathrm{d} x$

- For discrete $X$:

  - $\mathbb{E}_X[g(x)] = \sum_{x \in X} g(x) p(x)$

where $X$ is the target space of $X$. For discrete random variables, numerical outcomes are assumed, enabling summation. For multivariate random variables $X = [X_1, \ldots, X_D]^{\top}$, the expected value is defined element-wise:

- $\mathbb{E}_X[g(\boldsymbol{x})] = \left[ \begin{array}{c} \mathbb{E}_{X_1}[g(x_1)] \\ \vdots \\ \mathbb{E}_{X_D}[g(x_D)] \end{array} \right] \in \mathbb{R}^D$

The expected value is central to probability and machine learning, with foundational concepts derivable from it (Whittle, 2000).

**Definition 6.4 (Mean)**: The *mean* of a random variable $X$ with states $\boldsymbol{x} \in \mathbb{R}^D$ is an average, defined as the expected value with $g$ as the identity function:

- $\mathbb{E}_X[\boldsymbol{x}] = \left[ \begin{array}{c} \mathbb{E}_{X_1}[x_1] \\ \vdots \\ \mathbb{E}_{X_D}[x_D] \end{array} \right] \in \mathbb{R}^D$

where for each $d$:

- $\mathbb{E}_{X_d}[x_d] = \int_X x_d p(x_d) \mathrm{d} x_d$ (continuous)
- $\mathbb{E}_{X_d}[x_d] = \sum_{x_d \in X_d} x_d p(x_d)$ (discrete)

The mean summarizes the central tendency of a distribution.

**Variance and Covariance**: Variance measures the spread of a distribution, and covariance measures the relationship between pairs of random variables. These are defined in terms of expected values and are crucial for understanding distributions, particularly in the exponential family (Section 6.6), where they capture all necessary information.

**Definition 6.5 (Covariance (Univariate))**. The covariance between two univariate random variables $ X, Y \in \mathbb{R} $ is given by the expected product of their deviations from their respective means, i.e.,

$\text{Cov}_{X,Y}[x,y] := \mathbb{E}_{X,Y}[(x - \mathbb{E}_{X}[x])(y - \mathbb{E}_{Y}[y])] $

*Remark*. When the random variable associated with the expectation or covariance is clear by its arguments, the subscript is often suppressed (for example, $ \mathbb{E}_{X}[x] $ is often written as $ \mathbb{E}[x] $).

By using the linearity of expectations, the expression in Definition 6.5 can be rewritten as the expected value of the product minus the product of the expected values, i.e., $\text{Cov}[x,y] = \mathbb{E}[xy] - \mathbb{E}[x]\mathbb{E}[y]$

The covariance of a variable with itself $ \text{Cov}[x,x] $ is called the *variance* and is denoted by $ V_X[x] $. The square root of the variance is called the *standard deviation* and is often denoted by $ \sigma(x) $. The notion of covariance can be generalized to multivariate random variables.

**Definition 6.6 (Covariance (Multivariate))**. If we consider two multivariate random variables $ X $ and $ Y $ with states $ x \in \mathbb{R}^D $ and $ y \in \mathbb{R}^E $ respectively, the *covariance* between $ X $ and $ Y $ is defined as

$\text{Cov}[x,y] = \mathbb{E}[xy^\top] - \mathbb{E}[x]\mathbb{E}[y]^\top = \text{Cov}[y,x]^\top \in \mathbb{R}^{D\times E}$

### 6.4.2 Independence

Two random variables $X$ and $Y$ are *independent* if their joint distribution factorizes into the product of their marginals:

- $p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{x}) p(\boldsymbol{y})$

Independence implies that knowledge of one variable provides no information about the other, simplifying computations and modeling in machine learning.

### 6.4.3 Inner Products of Random Variables

Comparing random variables can also involve computing inner products, such as covariance, which measures how two variables vary together. This is particularly useful in understanding dependencies and is foundational in techniques like principal component analysis (Chapter 10).

## 6.5 Common Distributions

This section introduces common probability distributions, emphasizing their properties and applications in machine learning. Key distributions include:

- **Bernoulli Distribution**: Models binary outcomes (e.g., coin flips), with parameter $\mu \in (0, 1)$ representing the probability of success.
- **Binomial Distribution**: Models the number of successes in $n$ independent Bernoulli trials.
- **Multinomial Distribution**: Generalizes the binomial to multiple categories.
- **Gaussian (Normal) Distribution**: Widely used due to its mathematical properties, modeling continuous data with mean $\mu$ and variance $\sigma^2$ (univariate) or covariance $\Sigma$ (multivariate).
- **Uniform Distribution**: Models equal likelihood over a finite set (discrete) or interval (continuous).
- **Beta Distribution**: Models probabilities or proportions, often used as a conjugate prior in Bayesian analysis.
- **Dirichlet Distribution**: Generalizes the Beta to multiple categories, used in mixture models and topic modeling.
- **Gamma Distribution**: Models positive continuous variables, often used as a conjugate prior for precision in Gaussian distributions.
- **Wishart Distribution**: Models covariance matrices, used in multivariate Gaussian settings.

**Example 6.8 (Bernoulli)**: The Bernoulli distribution is:

- $p(x \mid \mu) = \mu^x (1 - \mu)^{1 - x}, \quad x \in \{0, 1\}$

**Example 6.9 (Gaussian)**: The univariate Gaussian pdf is:

- $p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right)$

For multivariate $\boldsymbol{x} \in \mathbb{R}^D$:

- $p(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2 \pi)^{D/2} |\boldsymbol{\Sigma}|^{1/2}} \exp \left( -\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) \right)$

where $\boldsymbol{\Sigma}$ is the covariance matrix, and $|\boldsymbol{\Sigma}|$ is its determinant.

### 6.5.1 Conjugate Priors

In Bayesian statistics, a *conjugate prior* for a likelihood function ensures that the posterior is in the same distribution family as the prior, simplifying computations.

**Example 6.12 (Bernoulli with Beta Prior)**: For a Bernoulli likelihood $p(x \mid \mu) = \mu^x (1 - \mu)^{1 - x}$, the conjugate prior is a Beta distribution:

- $p(\mu \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \mu^{\alpha - 1} (1 - \mu)^{\beta - 1}$

Given $N$ observations $\boldsymbol{x} = [x_1, \ldots, x_N]^{\top}$, the posterior is also Beta, with updated parameters reflecting the data.

### 6.5.2 Product of Gaussians

The product of two Gaussian distributions $\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{a}, \boldsymbol{A})$ and $\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{b}, \boldsymbol{B})$ is an unnormalized Gaussian $c \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{c}, \boldsymbol{C})$, with parameters derived via completing the square or exponential family forms (Exercise 6.10). This property is crucial in Bayesian inference, particularly for Gaussian processes and Kalman filtering.

## 6.6 Conjugacy and the Exponential Family

### 6.6.1 Conjugate Distributions

Conjugate distributions simplify Bayesian inference by ensuring the posterior remains in the same family as the prior.

### 6.6.2 Sufficient Statistics

A *statistic* of a random variable is a deterministic function, such as the sample mean $\hat{\mu} = \frac{1}{N} \sum_{n=1}^N x_n$ for gaussian data. Sir Ronald Fisher introduced *sufficient statistics*, which capture all information needed to infer distribution parameters.

**Theorem 6.14 (Fisher-Neyman)** Let $ X $ have probability density function $ p(x \mid \theta) $. Then the statistics $ \phi(x) $ are sufficient for $ \theta $ if and only if $ p(x \mid \theta) $ can be written in the form: $p(x \mid \theta) = h(x) g_{\theta}(\phi(x))$

where $ h(x) $ is a distribution independent of $ \theta $ and $ g_{\theta} $ captures all the dependence on $ \theta $ via sufficient statistics $ \phi(x) $.

If $ p(x \mid \theta) $ does not depend on $ \theta $, then $ \phi(x) $ is trivially a sufficient statistic for any function $ \phi $. The more interesting case is that $ p(x \mid \theta) $ is dependent only on $ \phi(x) $ and not $ x $ itself. In this case, $ \phi(x) $ is a sufficient statistic for $ \theta $.

Sufficient statistics are critical in parametric statistics, where they enable efficient estimation without needing the full dataset.

In machine learning, the number of parameters needed to describe a distribution may grow with data (non-parametric statistics), but certain distributions have finite-dimensional sufficient statistics, namely the exponential family.

### 6.6.3 Exponential Family

The *exponential family* is a broad class of distributions with finite-dimensional sufficient statistics, unifying many common distributions (e.g., Bernoulli, Gaussian, Binomial, Beta, Dirichlet). It is parameterized by $\theta \in \mathbb{R}^D$ and has the form:

- $p(\boldsymbol{x} \mid \boldsymbol{\theta}) = h(\boldsymbol{x}) \exp (\langle \boldsymbol{\theta}, \boldsymbol{\phi}(\boldsymbol{x}) \rangle - A(\boldsymbol{\theta}))$

where:

- $\boldsymbol{\phi}(\boldsymbol{x})$ is the vector of sufficient statistics.
- $\boldsymbol{\theta}$ are the *natural parameters*.
- $h(\boldsymbol{x})$ is a base measure, often absorbable into $\boldsymbol{\phi}(\boldsymbol{x})$ by adding a constrained parameter.
- $A(\boldsymbol{\theta})$ is the *log-partition function*, ensuring normalization.

An intuitive view is:

- $p(\boldsymbol{x} \mid \boldsymbol{\theta}) \propto \exp (\boldsymbol{\theta}^{\top} \boldsymbol{\phi}(\boldsymbol{x}))$

**Historical Note**: The exponential family was independently discovered by Edwin Pitman, Georges Darmois, and Bernard Koopman in 1935-1936, highlighting its fundamental role in statistics.

**Example 6.13 (Gaussian as Exponential Family)**: The univariate Gaussian $\mathcal{N}(\mu, \sigma^2)$ is in the exponential family with sufficient statistics $\boldsymbol{\phi}(x) = \left[ \begin{array}{c} x \\ x^2 \end{array} \right]$ and natural parameters $\boldsymbol{\theta} = \left[ \begin{array}{c} \frac{\mu}{\sigma^2} \\ -\frac{1}{2 \sigma^2} \end{array} \right]$.

**Example 6.14 (Bernoulli as Exponential Family)**: The Bernoulli distribution $p(x \mid \mu) = \mu^x (1 - \mu)^{1 - x}$ is expressed as:

- $p(x \mid \mu) = \exp \left( x \log \frac{\mu}{1 - \mu} + \log (1 - \mu) \right)$

with $\boldsymbol{\phi}(x) = x$, $\boldsymbol{\theta} = \log \frac{\mu}{1 - \mu}$, $h(x) = 1$, and $A(\boldsymbol{\theta}) = \log (1 + \exp (\boldsymbol{\theta}))$. The relationship $\mu = \frac{1}{1 + \exp (-\boldsymbol{\theta})}$ is the *sigmoid function*, widely used in logistic regression and neural networks.

The exponential family facilitates conjugate priors, as every member has a conjugate prior of the form:

- $p(\boldsymbol{\theta} \mid \gamma) = h_c(\boldsymbol{\theta}) \exp \left( \left\langle \left[ \begin{array}{c} \gamma_1 \\ \gamma_2 \end{array} \right], \left[ \begin{array}{c} \boldsymbol{\theta} \\ -A(\boldsymbol{\theta}) \end{array} \right] \right\rangle - A_c(\gamma) \right)$

**Example 6.15 (Bernoulli Conjugate Prior)**: The conjugate prior for the Bernoulli is derived as a Beta distribution, confirming its form via the exponential family framework.

Advantages of the exponential family include finite-dimensional sufficient statistics, easy derivation of conjugate distributions, optimal maximum likelihood estimation, and concave log-likelihood functions, aiding efficient optimization (Chapter 7).