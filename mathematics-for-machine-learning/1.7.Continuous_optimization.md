# Continuous Optimization

## Introduction to Continuous Optimization

Continuous optimization is a cornerstone of machine learning, where algorithms are expressed as numerical optimization problems to find optimal model parameters. This chapter explores methods for training machine learning models by optimizing an objective function, typically formulated to be minimized. The "goodness" of parameters is defined by the objective function or probabilistic model, examples of which appear later in the book. Assuming differentiability (covered in Chapter 5), gradients guide the search for optima.

The chapter divides continuous optimization into two branches (Figure 7.1):
1. **Unconstrained Optimization**: Finding minima without restrictions.
2. **Constrained Optimization**: Finding minima subject to constraints.

Most machine learning objective functions aim for minimization, likened to finding the "valleys" of a function. Gradients point "uphill," so optimization moves "downhill" (opposite the gradient). For unconstrained problems, this is the core idea, detailed in Section 7.1. Constrained problems introduce additional concepts (Section 7.2), and convex optimization (Section 7.3) offers guarantees of global optima.

An example function, $l(x) = x^4 + 7x^3 + 5x^2 - 17x + 3$, illustrates key concepts. Its global minimum is near $x = -4.5$ (value ~ -47), with a local minimum at $x = 0.7$. The gradient, $dl(x)/dx = 4x^3 + 21x^2 + 10x - 17$, identifies stationary points (roots where gradient is zero). The second derivative, $d^2l(x)/dx^2 = 12x^2 + 42x + 10$, determines their nature (positive for minima, negative for maxima). This cubic gradient has three roots, including a maximum near $x = -1.4$.

In $\mathbb{R}^D$, continuous optimization applies to real-valued data and models, contrasting with combinatorial optimization for discrete variables.

---

## 7.1 Optimization Using Gradient Descent

### Core Concept
Gradient descent is a first-order method for minimizing a differentiable function $f: \mathbb{R}^d \rightarrow \mathbb{R}$, where analytic solutions are unavailable. It iteratively steps in the direction of the negative gradient, which points to steepest descent (from Chapter 5). The update rule is:

$x_{i+1} = x_i - \gamma_i (\nabla f(x_i))^\top$,

where $\gamma_i \geq 0$ is the step-size (or learning rate), and gradients are row vectors by convention. For small $\gamma_i$, $f(x_{i+1}) \leq f(x_i)$, converging to a local minimum.

### 7.1.1 Step-Size
Step-size choice is critical:
- **Too small**: Slow convergence.
- **Too large**: Overshooting, divergence, or oscillation.

Adaptive methods adjust $\gamma_i$ based on local function properties:
- If $f$ increases, undo the step and reduce $\gamma$.
- If $f$ decreases, increase $\gamma$ for efficiency.

### 7.1.2 Gradient Descent with Momentum
Momentum improves convergence in poorly conditioned problems by adding memory of past updates, smoothing oscillations. The update becomes:

$x_{i+1} = x_i - \gamma_i (\nabla f(x_i))^\top + \alpha \Delta x_i$,

$\Delta x_i = x_i - x_{i-1} = \alpha \Delta x_{i-1} - \gamma_{i-1} (\nabla f(x_{i-1}))^\top$,

where $\alpha \in [0, 1]$ weights the previous step. Like a heavy ball resisting direction changes, momentum averages noisy gradients, aiding convergence.

### 7.1.3 Stochastic Gradient Descent (SGD)
For large datasets, computing $\nabla L(\theta) = \sum_{n=1}^N \nabla L_n(\theta)$ in batch gradient descent (Equation 7.15) is costly. SGD approximates the gradient using a subset (mini-batch) of data:

$\theta_{i+1} = \theta_i - \gamma_i \sum_{n \in subset} (\nabla L_n(\theta_i))^\top$.

For a single example (mini-batch size 1), it’s $\theta_{i+1} = \theta_i - \gamma_i (\nabla L_n(\theta_i))^\top$. The subset provides an unbiased estimate of the true gradient, ensuring convergence if $\gamma_i$ decreases appropriately (Bottou, 1998). Small mini-batches are fast but noisy, potentially escaping poor local minima, while large mini-batches leverage optimized matrix operations but are computationally expensive. SGD excels in large-scale problems (e.g., deep learning, Dean et al., 2012).

---

## 7.2 Constrained Optimization and Lagrange Multipliers

### Problem Formulation
Constrained optimization minimizes $f(x)$ subject to $g_i(x) \leq 0$, $i = 1, ..., m$:

$\min_x f(x)$, subject to $g_i(x) \leq 0$ $\forall$ $i$.

An indicator function $J(x) = f(x) + \sum_{i=1}^m 1(g_i(x))$, where $1(z) = 0$ if $z \leq 0$, $\infty$ otherwise, 

converts this to an unconstrained problem, but it’s impractical due to the infinite step.

### Lagrange Multipliers
The Lagrangian relaxes constraints with multipliers $\lambda_i \geq 0$: $\mathfrak{L}(x, \lambda) = f(x) + \sum_{i=1}^m \lambda_i g_i(x)$, where $\lambda$ are dual variables. 

The dual problem is: $\max_{\lambda \geq 0} \mathfrak{D}(\lambda)$ where $\mathfrak{D}(\lambda) = min_{x \in \mathbb{R}^d} \mathfrak{L}(x, \lambda)$,

The **minimax inequality** states $\max_y \min_x \varphi(x, y) \leq \min_x \max_y \varphi(x, y)$, implying weak duality: $\min_x \max_{\lambda \geq 0} \mathfrak{L}(x, \lambda) \geq \max_{\lambda \geq 0} \min_x \mathfrak{L}(x, \lambda)$. For fixed $\lambda$, $\min_x \mathfrak{L}(x, \lambda)$ is unconstrained and affine in $\lambda$, making $\mathfrak{D}(\lambda)$ concave and the outer maximization efficient.

### Equality Constraints
For additional equality constraints $h_j(x) = 0$, $j = 1, ..., n$, the Lagrangian becomes:

$\mathfrak{L}(x, \lambda, \mu) = f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^n \mu_j h_j(x)$,

where $\mu_j$ are unconstrained multipliers. Equality constraints are modeled as two inequalities ($h_j(x) \leq 0$, $-h_j(x) \leq 0$).

---

## 7.3 Convex Optimization

### Convexity Basics
Convex optimization guarantees global optimality when $f$ and $g_i$ are convex functions, and $h_j = 0$ define convex sets. 
1) A set $\mathcal{C}$ is convex if $\theta x + (1-\theta) y \in \mathcal{C}$ for $x, y \in \mathcal{C}$, $0 \leq \theta \leq 1$. 
2) A function $f: \mathbb{R}^D \rightarrow \mathbb{R}$ is convex if: $f(\theta x + (1-\theta) y) \leq \theta f(x) + (1-\theta) f(y)$.

For differentiable $f$, convexity holds if $f(y) \geq f(x) + \nabla f(x)^\top (y-x)$. If twice differentiable, $f$ is convex if its Hessian $\nabla^2 f(x)$ is positive semidefinite. The epigraph (set above $f$) is convex, and concave functions are negatives of convex ones.

### Example 7.3: Negative Entropy
$f(x) = x log_2 x$ (for $x > 0$) is convex. 

At $x = 2$, $y = 4$, $f(3) \approx 4.75 \leq 5 = 0.5 f(2) + 0.5 f(4)$. \
Gradient $\nabla f = log_2 x + 1/log_e 2$ confirms $f(4) = 8 \geq f(2) + \nabla f(2) \cdot 2 \approx 6.9$.

### Example 7.4: Closure Properties
Non-negative weighted sums of convex functions (e.g., $\alpha f_1 + \beta f_2$, $\alpha, \beta \geq 0$) are convex, a property called Jensen’s inequality.

### 7.3.1 Linear Programming
- For $\min_x c^\top x$, subject to $Ax \leq b$, the Lagrangian is $\mathfrak{L}(x, \lambda) = c^\top x + \lambda^\top (Ax - b)$. 
- Setting $\nabla_x \mathfrak{L} = c + A^\top \lambda = 0$, the dual is: $\max_\lambda -b^\top \lambda$, subject to $c + A^\top \lambda = 0$, $\lambda \geq 0$.
- Example 7.5 illustrates this with $c = [5, 3]^\top$, $A$ and $b$ defining a feasible region (Figure 7.9).

### 7.3.2 Quadratic Programming
- For $\min_x (\frac{1}{2} x^\top Q x + c^\top x$), subject to $Ax \leq b$ ($Q$ positive definite), the Lagrangian is $\mathfrak{L}(x, \lambda) = \frac{1}{2} x^\top Q x + c^\top x + \lambda^\top (Ax - b)$. 
- Solving $\nabla_x \mathfrak{L} = Qx + c + A^\top \lambda = 0$ gives $x = -Q^{-1}(c + A^\top \lambda)$, and the dual is:
- $\max_\lambda -\frac{1}{2} (c + A^\top \lambda)^\top Q^{-1} (c + A^\top \lambda) - \lambda^\top b$, subject to $\lambda \geq 0$.
- Example 7.6 shows this with $Q = \begin{bmatrix} 2 & 1 \\ 1 & 4 \end{bmatrix}$, $c = \begin{bmatrix} 5 \\ 3 \end{bmatrix}^\top$ (Figure 7.4).

### 7.3.3 Legendre-Fenchel Transform
- The convex conjugate of $f: \mathbb{R}^D \rightarrow \mathbb{R}$ is $f^*(s) = \sup_{x \in \mathbb{R}^D} (\langle s, x \rangle - f(x))$. For convex, differentiable $f$, it relates to tangents (e.g., $f(x) = x^2$, $f^*(s) = \frac{s^2}{4}$). 
- Example 7.7 computes $f(y) = \frac{\lambda}{2} y^\top K^{-1} y$’s conjugate as $f^*(\alpha) = \frac{1}{2\lambda} \alpha^\top K \alpha$. 
- Example 7.8 shows $\mathcal{L}(t) = \sum_i \ell_i(t_i)$ has conjugate $\mathcal{L}^*(z) = \sum_i \ell_i^*(z_i)$. Example 7.9 links this to Lagrange duality for $\min_x f(Ax) + g(x)$.

---