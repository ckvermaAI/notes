# Vector Calculus

## Introduction
Vector calculus is a cornerstone of machine learning, providing the mathematical tools to optimize objective functions by finding model parameters that best explain data. This chapter explores how optimization problems—such as those in linear regression, neural network auto-encoders, and Gaussian mixture models—rely on gradient-based methods. These methods use derivatives to navigate the parameter space efficiently. The chapter introduces functions as mappings between quantities (e.g., inputs $x \in \mathbb{R}^D$ to outputs $f(x)$), emphasizing their role in machine learning and setting the stage for differentiation techniques.

- **Key Applications**:
  - Linear regression: Optimize weights to maximize likelihood.
  - Neural network auto-encoders: Minimize reconstruction error using the chain rule.
  - Gaussian mixture models: Adjust mixture parameters to fit data distributions.
- **Function Definition**: A function $f: \mathbb{R}^D \to \mathbb{R}$ maps inputs $x$ to outputs $f(x)$, with $\mathbb{R}^D$ as the domain and $f(x)$ as the codomain.

---

## 5.1 Differentiation of Univariate Functions
This section revisits the basics of differentiation for functions of a single variable, $f: \mathbb{R} \to \mathbb{R}$, which is foundational for extending to multivariate cases.

### Difference Quotient and Derivative
- **Difference Quotient**: Measures the average slope of a function $f(x)$ between two points:
  - $ \frac{\delta y}{\delta x} := \frac{f(x + \delta x) - f(x)}{\delta x} $
  - Represents the secant line’s slope in a graph.
- **Derivative**: The limit of the difference quotient as $\delta x \to 0$, giving the tangent’s slope:
  - $ \frac{d f}{d x} := \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} $
  - Indicates the direction of steepest ascent.

### Example 5.2: Derivative of a Polynomial
- For $f(x) = x^n$, the derivative is derived using the binomial expansion:
  - $ \frac{d f}{d x} = \lim_{h \to 0} \frac{(x + h)^n - x^n}{h} = n x^{n-1} $
  - Demonstrates the power rule in action.

### 5.1.1 Taylor Series
- **Taylor Polynomial**: Approximates $f(x)$ around $x_0$ using $n$ derivatives:
  - $ T_n(x) := \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} (x - x_0)^k $
- **Taylor Series**: An infinite sum for smooth functions ($f \in C^\infty$):
  - $ T_\infty(x) = \sum_{k=0}^\infty \frac{f^{(k)}(x_0)}{k!} (x - x_0)^k $
  - At $x_0 = 0$, this becomes the Maclaurin series.
- **Remark**: For a polynomial of degree $k \leq n$, the Taylor polynomial $T_n$ is exact, as higher derivatives vanish.

### Example 5.3: Taylor Polynomial of $x^4$
- At $x_0 = 1$, compute derivatives up to $k=6$:
  - $f(1) = 1$, $f'(1) = 4$, $f''(1) = 12$, $f^{(3)}(1) = 24$, $f^{(4)}(1) = 24$, $f^{(5)}(1) = 0$, $f^{(6)}(1) = 0$
  - $ T_6(x) = 1 + 4(x-1) + 6(x-1)^2 + 4(x-1)^3 + (x-1)^4 $, which equals $x^4$.

### Example 5.4: Taylor Series of $\sin(x) + \cos(x)$
- At $x_0 = 0$, derivatives cycle with period 4:
  - $f(0) = 1$, $f'(0) = 1$, $f''(0) = -1$, $f^{(3)}(0) = -1$, $f^{(4)}(0) = 1$
  - $ T_\infty(x) = 1 + x - \frac{1}{2!} x^2 - \frac{1}{3!} x^3 + \frac{1}{4!} x^4 + \cdots = \cos(x) + \sin(x) $

### 5.1.2 Differentiation Rules
- **Product Rule**: $ (f(x)g(x))' = f'(x)g(x) + f(x)g'(x) $
- **Quotient Rule**: $ \left(\frac{f(x)}{g(x)}\right)' = \frac{f'(x)g(x) - f(x)g'(x)}{(g(x))^2} $
- **Sum Rule**: $ (f(x) + g(x))' = f'(x) + g'(x) $
- **Chain Rule**: $ (g(f(x)))' = g'(f(x)) f'(x) $

### Example 5.5: Chain Rule
- For $h(x) = (2x + 1)^4$, let $f(x) = 2x + 1$, $g(f) = f^4$:
  - $f'(x) = 2$, $g'(f) = 4f^3$
  - $h'(x) = 4(2x + 1)^3 \cdot 2 = 8(2x + 1)^3$

---

## 5.2 Partial Differentiation and Gradients
This section extends differentiation to functions of multiple variables, $f: \mathbb{R}^n \to \mathbb{R}$, introducing partial derivatives and gradients.

### Definition 5.5: Partial Derivative
- For $f(x_1, \ldots, x_n)$, the partial derivative with respect to $x_i$ is:
  - $ \frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x)}{h} $
- **Gradient**: A row vector of partial derivatives:
  - $ \nabla_x f = \left[ \frac{\partial f}{\partial x_1} \cdots \frac{\partial f}{\partial x_n} \right] \in \mathbb{R}^{1 \times n} $
  - Also called the Jacobian for scalar-valued functions.

### Example 5.6: Partial Derivatives with Chain Rule
- For $f(x, y) = (x + 2y^3)^2$:
  - $ \frac{\partial f}{\partial x} = 2(x + 2y^3) \cdot 1 = 2(x + 2y^3) $
  - $ \frac{\partial f}{\partial y} = 2(x + 2y^3) \cdot 6y^2 = 12(x + 2y^3)y^2 $

### Example 5.7: Gradient
- For $f(x_1, x_2) = x_1^2 x_2 + x_1 x_2^3$:
  - $ \frac{\partial f}{\partial x_1} = 2x_1 x_2 + x_2^3 $
  - $ \frac{\partial f}{\partial x_2} = x_1^2 + 3x_1 x_2^2 $
  - $ \nabla_x f = \left[ 2x_1 x_2 + x_2^3 \quad x_1^2 + 3x_1 x_2^2 \right] $

### 5.2.1 Basic Rules of Partial Differentiation
- **Product Rule**: $ \frac{\partial}{\partial x} (f(x)g(x)) = \frac{\partial f}{\partial x} g(x) + f(x) \frac{\partial g}{\partial x} $
- **Sum Rule**: $ \frac{\partial}{\partial x} (f(x) + g(x)) = \frac{\partial f}{\partial x} + \frac{\partial g}{\partial x} $
- **Chain Rule**: $ \frac{\partial}{\partial x} (g(f(x))) = \frac{\partial g}{\partial f} \frac{\partial f}{\partial x} $

### 5.2.2 Chain Rule
- For $f(x_1(t), x_2(t))$, the total derivative is:
  - $ \frac{d f}{d t} = \frac{\partial f}{\partial x_1} \frac{\partial x_1}{\partial t} + \frac{\partial f}{\partial x_2} \frac{\partial x_2}{\partial t} $
- **Example 5.8**: $f(x_1, x_2) = x_1^2 + 2x_2$, with $x_1 = \sin t$, $x_2 = \cos t$:
  - $ \frac{d f}{d t} = 2 \sin t \cos t - 2 \sin t = 2 \sin t (\cos t - 1) $
- For $f(x_1(s,t), x_2(s,t))$, partial derivatives are:
  - $ \frac{\partial f}{\partial s} = \frac{\partial f}{\partial x_1} \frac{\partial x_1}{\partial s} + \frac{\partial f}{\partial x_2} \frac{\partial x_2}{\partial s} $
  - $ \frac{\partial f}{\partial t} = \frac{\partial f}{\partial x_1} \frac{\partial x_1}{\partial t} + \frac{\partial f}{\partial x_2} \frac{\partial x_2}{\partial t} $
  - Gradient as matrix multiplication: $ \frac{d f}{d (s,t)} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial (s,t)} $

### Remark: Gradient Verification
- Numerically check gradients using finite differences:
  - Compare $ \frac{f(x + h) - f(x)}{h} $ with analytic gradient, ensuring small error (e.g., $< 10^{-6}$).

---

## 5.3 Gradients of Vector-Valued Functions
This section generalizes gradients to vector-valued functions, $f: \mathbb{R}^n \to \mathbb{R}^m$, where the output is a vector.

### Definition 5.6: Jacobian
- First-order partial derivative with respect to $x_i$:
  - $ \frac{\partial f}{\partial x_i} = \left[ \frac{\partial f_1}{\partial x_i} \cdots \frac{\partial f_m}{\partial x_i} \right]^T \in \mathbb{R}^m $
- **Jacobian**: An $m \times n$ matrix of all partial derivatives:
  - $ J = \frac{d f}{d x} = \left[ \frac{\partial f}{\partial x_1} \cdots \frac{\partial f}{\partial x_n} \right] = \left[ \begin{array}{ccc} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{array} \right] $
- For $m=1$, reduces to the gradient row vector.

### Example 5.9: Linear Function
- For $f(x) = A x$, where $A \in \mathbb{R}^{M \times N}$:
  - $ \frac{\partial f_i}{\partial x_j} = A_{ij} $
  - $ \frac{d f}{d x} = A $

### Example 5.10: Chain Rule
- For $h(t) = \exp((t \cos t)(t \sin t)^2)$:
  - $ \frac{d h}{d t} = \exp(x_1 x_2^2) (x_2^2 (\cos t - t \sin t) + 2 x_1 x_2 (\sin t + t \cos t)) $
  - Where $x_1 = t \cos t$, $x_2 = t \sin t$.

### Example 5.11: Least-Squares Loss
- For $y = \Phi \theta$, $L(e) = \|e\|^2$, $e(\theta) = y - \Phi \theta$:
  - $ \frac{\partial L}{\partial e} = 2 e^T $, $ \frac{\partial e}{\partial \theta} = -\Phi $
  - $ \frac{\partial L}{\partial \theta} = -2 (y^T - \theta^T \Phi^T) \Phi $

### Jacobian Determinant
- The determinant of the Jacobian, $| \det(J) |$, gives the scaling factor for area/volume changes under transformations (e.g., from unit square to parallelogram).

---

## 5.4 Gradients of Matrices
Gradients of matrices with respect to vectors or matrices yield tensors, handled via partial derivatives or vectorization.

### Example 5.12: Vector with Respect to Matrix
- For $f = A x$:
  - $ \frac{\partial f_i}{\partial A_{pq}} = x_q $ if $p=i$, else 0
  - $ \frac{d f}{d A} \in \mathbb{R}^{M \times (M \times N)} $, structured with $x^T$ in appropriate rows.

### Example 5.13: Matrix with Respect to Matrix
- For $K = R^T R$:
  - $ K_{pq} = r_p^T r_q $
  - $ \frac{\partial K_{pq}}{\partial R_{ij}} $ varies based on indices, yielding a $ (N \times N) \times (M \times N) $ tensor.

---

## 5.5 Useful Identities for Computing Gradients
- Key identities for machine learning:
  - $ \frac{\partial}{\partial X} f(X)^T = \left( \frac{\partial f(X)}{\partial X} \right)^T $
  - $ \frac{\partial}{\partial X} \det(f(X)) = \det(f(X)) tr(f(X)^{-1} \frac{\partial f(X)}{\partial X}) $
  - $ \frac{\partial x^T B x}{\partial x} = x^T (B + B^T) $

---

## 5.6 Backpropagation and Automatic Differentiation
Backpropagation efficiently computes gradients in deep networks using the chain rule.

### 5.6.1 Gradients in a Deep Network
- For $y = f_K(f_{K-1}(\cdots f_1(x)))$:
  - Loss $L(\theta) = \| y - f_K(\theta, x) \|^2$
  - Gradients: $ \frac{\partial L}{\partial \theta_i} = \frac{\partial L}{\partial f_K} \prod_{j=i+1}^{K-1} \frac{\partial f_{j+1}}{\partial f_j} \frac{\partial f_{j+1}}{\partial \theta_i} $

### Example 5.14: Computation Graph
- For $f(x) = \sqrt{x^2 + \exp(x^2)} + \cos(x^2 + \exp(x^2))$:
  - Intermediate variables simplify computation, and backpropagation yields $ \frac{d f}{d x} $ efficiently.

---

## 5.7 Higher-Order Derivatives
- **Hessian**: Matrix of second-order partial derivatives, symmetric for $C^2$ functions:
  - $ H = \nabla^2 f = \left[ \begin{array}{cc} \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\ \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} \end{array} \right] $

---

## 5.8 Linearization and Multivariate Taylor Series
- **Linear Approximation**: $ f(x) \approx f(x_0) + \nabla_x f(x_0) (x - x_0) $
- **Multivariate Taylor Series**: $ f(x) = \sum_{k=0}^\infty \frac{D_x^k f(x_0)}{k!} \delta^k $, where $\delta = x - x_0$.
- **Example 5.15**: For $f(x,y) = x^2 + 2xy + y^3$ at $(1,2)$, the series matches the original polynomial up to degree 3.

---