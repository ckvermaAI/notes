# When Models Meet Data

## Overview
Chapter 8 serves as the foundational chapter for the second part of *Mathematics for Machine Learning*, bridging the mathematical concepts introduced in the first part (e.g., linear algebra, probability, optimization) with their application to machine learning. It introduces the core components of machine learning—data, models, and learning—and outlines three key frameworks: empirical risk minimization, parameter estimation via maximum likelihood, and probabilistic modeling. These frameworks are applied to the four pillars of machine learning: regression (Chapter 9), dimensionality reduction (Chapter 10), density estimation (Chapter 11), and classification (Chapter 12). The chapter aims to provide practical methods for applying mathematical foundations rather than delving into advanced concepts, making it accessible while offering a gateway to broader literature.

## 8.1 Data, Models, and Learning
This section establishes the three fundamental components of machine learning: **data**, **models**, and **learning**. It poses the central question, "What do we mean by good models?" and explores how "good" is defined through performance on unseen data, requiring metrics like accuracy or distance from ground truth.

### 8.1.1 Data as Vectors
- **Representation**: Data must be computer-readable and numerical, typically in a tabular format where rows are instances (examples) and columns are features. For example, Table 8.1 shows a human resource database with non-numerical data (e.g., Name, Gender, Degree), which is converted into a numerical format in Table 8.2 (e.g., Gender as -1/+1, Postcode as latitude/longitude).
- **Preprocessing**: Features may require domain-specific encoding (e.g., converting categorical variables like Gender into numbers) or normalization (shifting/scaling to mean 0, variance 1). The book assumes data is preprocessed into $D$-dimensional real-valued vectors ${\boldsymbol{x}_n}$, which are called features, attributes, or covariates.
- **Supervised Learning Example**: Using Table 8.2, predicting annual salary (${y_n}$ - label) from age (${\boldsymbol{x}_n}$) is a supervised learning task, with the dataset as ${\{(\boldsymbol{x}_1, y_1), \ldots, (\boldsymbol{x}_N, y_N)\}}$ and the feature matrix as ${\boldsymbol{X} \in \mathbb{R}^{N \times D}}$ where $n=1, \ldots, N$ indexes $N$ examples, and $d=1, \ldots, D$ indexes $D$ features.

### 8.1.2 Models as Functions
- **Definition**: A model is a predictor function ${f: \mathbb{R}^D \rightarrow \mathbb{R}}$, mapping $D$-dimensional input vectors to scalar outputs. For simplicity, the book focuses on linear functions ${f(\boldsymbol{x}) = \boldsymbol{\theta}^{\top} \boldsymbol{x} + \theta_0}$, where ${\boldsymbol{\theta}}$ and ${\theta_0}$ are parameters.
- **Purpose**: This restricts the scope to problems solvable with basic linear algebra (Chapters 2 and 3), avoiding functional analysis. Figure 8.2 illustrates a linear predictor for salary prediction.

### 8.1.3 Models as Probability Distributions
- **Motivation**: Data often includes noise, and predictors should quantify uncertainty. Probability theory (Chapter 6) provides this language, modeling predictors as distributions over functions rather than single functions.
- **Scope**: The book limits itself to finite-dimensional parameter distributions (e.g., multivariate probability distributions), avoiding stochastic processes. Figure 8.3 shows a Gaussian uncertainty for a prediction.

### 8.1.4 Learning is Finding Parameters
- **Goal**: Learning finds parameters so the predictor performs well on unseen data. It involves three phases:
  1. **Training/Parameter Estimation**: Adjusting the model based on training data.
  2. **Hyperparameter Tuning/Model Selection**: Choosing model structure or settings (e.g., number of components).
  3. **Prediction/Inference**: Applying a trained predictor to new data.
- **Approaches**: Non-probabilistic models use empirical risk minimization (Section 8.2), while probabilistic models use maximum likelihood (Section 8.3) or Bayesian inference (Section 8.4).
- **Generalization**: Cross-validation (Section 8.2.4) simulates performance on unseen data, balancing fit to training data with simplicity via regularization or priors.

## 8.2 Empirical Risk Minimization
This section introduces empirical risk minimization (ERM), a "probability-free" approach to learning where predictors are functions, popularized by support vector machines (Chapter 12). It involves four design choices:

### 8.2.1 Hypothesis Class of Functions
- **Setup**: Given $N$ examples ${\boldsymbol{x}_n \in \mathbb{R}^D}$ and labels ${y_n \in \mathbb{R}}$, find a predictor ${f(\cdot, \boldsymbol{\theta}): \mathbb{R}^D \rightarrow \mathbb{R}}$ with parameters ${\boldsymbol{\theta}}$ such that ${f(\boldsymbol{x}_n, \boldsymbol{\theta}^*) \approx y_n}$.
- **Example 8.1**: Ordinary least-squares regression uses affine functions ${f(\boldsymbol{x}_n, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\top} \boldsymbol{x}_n}$, where ${\boldsymbol{x}_n}$ includes a constant feature ${x^{(0)}=1}$. This is equivalent to ${\theta_0 + \sum_{d=1}^D \theta_d x_n^{(d)}}$.

### 8.2.2 Loss Function for Training
- **Definition**: A loss function ${\ell(y_n, \hat{y}_n)}$ measures prediction error, where ${\hat{y}_n = f(\boldsymbol{x}_n, \boldsymbol{\theta})}$. The empirical risk is the average loss: ${\mathbf{R}_{\text {emp}}(f, \boldsymbol{X}, \boldsymbol{y}) = \frac{1}{N} \sum_{n=1}^N \ell(y_n, \hat{y}_n)}$, with ${\boldsymbol{X} \in \mathbb{R}^{N \times D}}$ and ${\boldsymbol{y} \in \mathbb{R}^N}$.
- **Assumption**: Data is independent and identically distributed (i.i.d.), justifying the empirical mean as an estimate of the population mean.
- **Example 8.2**: Least-squares regression uses squared loss ${\ell(y_n, \hat{y}_n) = (y_n - \hat{y}_n)^2}$, leading to ${\min_{\boldsymbol{\theta} \in \mathbb{R}^D} \frac{1}{N} \sum_{n=1}^N (y_n - \boldsymbol{\theta}^{\top} \boldsymbol{x}_n)^2}$ or ${\min_{\boldsymbol{\theta} \in \mathbb{R}^D} \frac{1}{N} \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\theta}\|^2}$, solvable via normal equations (Chapter 9).
- **Goal**: Minimize expected risk ${\mathbf{R}_{\text {true}}(f) = \mathbb{E}_{x, y}[\ell(y, f(x))]}$ over all possible data, approximated by test set performance.

### 8.2.3 Regularization to Reduce Overfitting
- **Overfitting**: A rich hypothesis class can memorize training data (low ${\mathbf{R}_{\text {emp}}}$), but fail on test data (high ${\mathbf{R}_{\text {true}}}$), especially with small datasets.
- **Solution**: Split data into training and test sets; regularization adds a penalty to ERM to favor simpler models.
- **Example 8.3**: Regularized least squares modifies the objective to ${\min_{\boldsymbol{\theta}} \frac{1}{N} \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\theta}\|^2 + \lambda \|\boldsymbol{\theta}\|^2}$, where ${\lambda}$ trades off fit and parameter magnitude, reducing overfitting.

### 8.2.4 Cross-Validation to Assess Generalization Performance
- **Method**: $K$-fold cross-validation splits data into $K$ chunks, using $K-1$ for training (${\mathcal{R}}$) and 1 for validation (${\mathcal{V}}$), averaging performance across all splits: ${\mathbb{E}_{\mathcal{V}}[R(f, \mathcal{V})] \approx \frac{1}{K} \sum_{k=1}^K R(f^{(k)}, \mathcal{V}^{(k)})}$.
- More precisely, for each partition $k$ the training data $R(k)$ produces a predictor $f(k)$, which is then applied to validation set $V(k)$ to compute the empirical risk $R(f(k), V(k))$. We cycle through all possible partitionings of validation and training sets and compute the average generalization error of the predictor.
- **Advantages**: Balances training and validation set sizes; parallelizable.
- **Disadvantages**: Computationally costly for expensive models; nested cross-validation (Section 8.6.1) may be needed for hyperparameters.

## 8.3 Parameter Estimation via Maximum Likelihood
This section uses probability distributions to model uncertainty, contrasting with ERM’s function-based approach.

### 8.3.1 Maximum Likelihood Estimation (MLE)
- The **idea** behind maximum likelihood estimation (MLE) is to define a function of the parameters that enables us to find a model that fits the data estimation well. The estimation problem is focused on the *likelihood* function, or likelihood more precisely its negative logarithm.
- **Definition**: For data represented by a random variable ${\boldsymbol{x}}$ and for a family of probability densities $p(x | \theta)$ parametrized
by $\theta$, ${p(\boldsymbol{x} \mid \boldsymbol{\theta})}$, the negative log-likelihood is ${\mathcal{L}(\boldsymbol{\theta}) = -\log p(\boldsymbol{x} \mid \boldsymbol{\theta})}$, and it is minimized to find ${\boldsymbol{\theta}}$.
- **Supervised Case**: Given ${(\boldsymbol{x}_n, y_n)}$, specify ${p(y_n \mid \boldsymbol{x}_n, \boldsymbol{\theta})}$. Assuming i.i.d. data, ${p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) = \prod_{n=1}^N p(y_n \mid \boldsymbol{x}_n, \boldsymbol{\theta})}$, and ${\mathcal{L}(\boldsymbol{\theta}) = -\sum_{n=1}^N \log p(y_n \mid \boldsymbol{x}_n, \boldsymbol{\theta})}$.
- **Example 8.4**: Gaussian likelihood ${p(y_n \mid \boldsymbol{x}_n, \boldsymbol{\theta}) = \mathcal{N}(y_n \mid \boldsymbol{x}_n^{\top} \boldsymbol{\theta}, \sigma^2)}$ assumes noise ${\varepsilon_n \sim \mathcal{N}(0, \sigma^2)}$.
- **Example 8.5**: Negative log-likelihood becomes ${\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2 \sigma^2} \sum_{n=1}^N (y_n - \boldsymbol{x}_n^{\top} \boldsymbol{\theta})^2 + \text{constant}}$, equivalent to least-squares.

### 8.3.2 Priors as Regularizers
- **MAP Estimation**: Add a prior ${p(\boldsymbol{\theta})}$, minimizing ${-\log p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) - \log p(\boldsymbol{\theta})}$. For a Gaussian prior ${p(\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{\theta} \mid 0, \tau^2 I)}$, this mirrors regularized least squares with ${\lambda = \frac{\sigma^2}{\tau^2}}$.

### 8.3.3 Overfitting
- **Issue**: MLE can overfit with small datasets or complex models, fitting noise rather than signal.
- **Solution**: Priors or regularization mitigate this, akin to ERM’s approach.

## 8.4 Probabilistic Modeling and Inference
This section extends probabilistic models beyond point estimates.

### 8.4.1 Learning and Inference
- **Prediction**: Probabilistic models yield ${p(y \mid \boldsymbol{x}, \mathcal{X}, \mathcal{Y})}$, integrating over parameters or using point estimates.

### 8.4.2 Bayesian Inference
- **Posterior**: ${p(\boldsymbol{\theta} \mid \mathcal{X}) = \frac{p(\mathcal{X} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathcal{X})}}$, where ${p(\mathcal{X}) = \int p(\mathcal{X} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}}$.
- **Prediction**: ${p(\boldsymbol{x}) = \int p(\boldsymbol{x} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}}$, averaging over all ${\boldsymbol{\theta}}$.
- **Challenges**: Integration is often intractable without conjugate priors, requiring approximations like MCMC or variational inference.
- **Applications**: Used in topic modeling, reinforcement learning, and recommender systems.

### 8.4.3 Latent-Variable Models
- **Definition**: Introduce latent variables ${z}$ alongside parameters ${\boldsymbol{\theta}}$, e.g., ${p(\boldsymbol{x} \mid \boldsymbol{z}, \boldsymbol{\theta})}$.
- **Likelihood**: ${p(\boldsymbol{x} \mid \boldsymbol{\theta}) = \int p(\boldsymbol{x} \mid \boldsymbol{z}, \boldsymbol{\theta}) p(\boldsymbol{z}) \mathrm{d} \boldsymbol{z}}$.
- **Learning**: Use expectation maximization (EM) or Bayesian inference; marginalization is challenging without conjugate priors.
- **Examples**: PCA (Chapter 10), Gaussian mixture models (Chapter 11).


## 8.5 Directed Graphical Models
### 8.5.1 Graph Semantics
- **Definition**: Nodes are random variables; directed edges denote conditional dependencies (e.g., ${p(b \mid a)}$).
- **Joint Distribution**: ${p(\boldsymbol{x}) = \prod_{k=1}^K p(x_k \mid \mathrm{Pa}_k)}$, where ${\mathrm{Pa}_k}$ are parents.
- **Example 8.7**: ${p(a, b, c) = p(c \mid a, b) p(b \mid a) p(a)}$ yields a fully connected graph. (c depends directly on a and b, b depends directly on a, 
a depends neither on b nor on c)
- **Example 8.8**: A sparser graph factorizes as ${p(x_1, x_2, x_3, x_4, x_5) = p(x_1) p(x_5) p(x_2 \mid x_5) p(x_3 \mid x_1, x_2) p(x_4 \mid x_2)}$.

### 8.5.2 Conditional Independence and d-Separation
- **d-Separation**: Determines if ${\mathcal{A} \perp\!\!\!\perp \mathcal{B} \mid \mathcal{C}}$ by checking if all paths between ${\mathcal{A}}$ and ${\mathcal{B}}$ are blocked by ${\mathcal{C}}$.


## 8.6 Model Selection
### 8.6.1 Nested Cross-Validation
- The inner cross-validation level is used to estimate the performance of a particular choice of model or hyperparameter on a internal validation set.
- The outer level is used to estimate generalization performance for the best choice of model chosen by the inner loop.
- We can test different model and hyperparameter choices in the inner loop. To distinguish the two levels, the set used to estimate test set the generalization performance is often called the *test set* and the set used validation set for choosing the best model is called the *validation set*.

### 8.6.2 Bayesian Model Selection
- **Occam’s Razor**: Simpler models are favored automatically via Bayes’ theorem: ${p(M_k \mid \mathcal{D}) \propto p(M_k) p(\mathcal{D} \mid M_k)}$, where ${p(\mathcal{D} \mid M_k) = \int p(\mathcal{D} \mid \boldsymbol{\theta}_k) p(\boldsymbol{\theta}_k \mid M_k) \mathrm{d} \boldsymbol{\theta}_k}$.
- **MAP**: ${M^* = \arg \max_{M_k} p(M_k \mid \mathcal{D})}$.

### 8.6.3 Bayes Factors
- **Comparison**: ${\frac{p(M_1 \mid \mathcal{D})}{p(M_2 \mid \mathcal{D})} = \frac{p(M_1)}{p(M_2)} \frac{p(\mathcal{D} \mid M_1)}{p(\mathcal{D} \mid M_2)}}$; uniform priors simplify to Bayes factor ${\frac{p(\mathcal{D} \mid M_1)}{p(\mathcal{D} \mid M_2)}}$.

## Conclusion
Chapter 8 provides a comprehensive framework for understanding machine learning through data representation, model specification, and learning strategies, preparing readers for the practical applications in subsequent chapters.
