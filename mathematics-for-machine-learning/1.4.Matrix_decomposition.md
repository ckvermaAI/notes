# Matrix Decompositions

## Introduction
Chapter 4 of *Mathematics for Machine Learning* explores matrix decompositions, building on the vector manipulations, projections, and linear mappings covered in Chapters 2 and 3. Matrices are pivotal in representing linear transformations and data, such as in datasets where rows might represent individuals and columns their features (e.g., height, weight). This chapter introduces methods to summarize matrices with characteristic numbers (determinants and eigenvalues), decompose them into interpretable factors, and use these decompositions for approximations. Key decomposition techniques include the Cholesky decomposition, matrix diagonalization, and singular value decomposition (SVD). The chapter concludes with a matrix taxonomy and highlights applications in machine learning, such as dimensionality reduction and density estimation.

---

## 4.1 Determinant and Trace
### Overview
Determinants and traces are scalar functions that characterize square matrices ($A \in \mathbb{R}^{n \times n}$). The determinant, denoted $det(A)$ or $|A|$, maps a square matrix to a real number and is crucial for analyzing systems of linear equations. The trace, $tr(A)$, is the sum of diagonal elements.

### Determinant
- **Definition**: For a square matrix $A$, the determinant is written as:
  $det(A) = \left|\begin{array}{cccc}
  a_{11} & a_{12} & \ldots & a_{1n} \\
  a_{21} & a_{22} & \ldots & a_{2n} \\
  \vdots & & \ddots & \vdots \\
  a_{n1} & a_{n2} & \ldots & a_{nn}
  \end{array}\right|$
  
- **Examples**:
  - For $n=1$: $det(A) = a_{11}$.
  - For $n=2$: $det(A) = a_{11}a_{22} - a_{12}a_{21}$.
  - For $n=3$ (Sarrus’ rule): $det(A) = a_{11}a_{22}a_{33} + a_{21}a_{32}a_{13} + a_{31}a_{12}a_{23} - a_{31}a_{22}a_{13} - a_{11}a_{32}a_{23} - a_{21}a_{12}a_{33}$.
- **Invertibility**: A matrix $A$ is invertible if and only if $det(A) \neq 0$ (Theorem 4.1). For a $2 \times 2$ matrix, $A^{-1} = \frac{1}{det(A)} \begin{bmatrix} a_{22} & -a_{12} \\ -a_{21} & a_{11} \end{bmatrix}$, valid when $det(A) \neq 0$.
- **Geometric Interpretation**: The determinant represents the signed volume of the parallelepiped spanned by the matrix’s column vectors. For $n=2$, $|det(A)|$ is the area of a parallelogram; for $n=3$, it’s the volume of a parallelepiped. The sign indicates orientation relative to the standard basis.
  - Example: For vectors $r = \begin{bmatrix} 2 \\ 0 \\ -8 \end{bmatrix}$, $g = \begin{bmatrix} 6 \\ 1 \\ 0 \end{bmatrix}$, $b = \begin{bmatrix} 1 \\ 4 \\ -1 \end{bmatrix}$, forming $A = [r, g, b]$, $det(A) = 186$, so the volume is 186.
- **Triangular Matrices**: For an upper- or lower-triangular matrix $T$, $det(T) = \prod_{i=1}^n T_{ii}$.
- **Laplace Expansion**: For an $n \times n$ matrix, the determinant can be computed recursively:
  - Along column $j$: $det(A) = \sum_{k=1}^n (-1)^{k+j} a_{kj} det(A_{k,j})$.
  - Along row $j$: $det(A) = \sum_{k=1}^n (-1)^{k+j} a_{jk} det(A_{j,k})$, where $A_{k,j}$ is the $(n-1) \times (n-1)$ submatrix excluding row $k$ and column $j$.
  - Example: For $A = \begin{bmatrix} 1 & 2 & 3 \\ 3 & 1 & 2 \\ 0 & 0 & 1 \end{bmatrix}$, expansion along the first row yields $det(A) = -5$, confirmed by Sarrus’ rule.
- **Properties**:
  - $det(AB) = det(A)det(B)$.
  - $det(A) = det(A^\top)$.
  - $det(A^{-1}) = \frac{1}{det(A)}$ if $A$ is invertible.
  - Similar matrices have the same determinant.
  - Adding a multiple of a row/column to another doesn’t change $det(A)$.
  - Scaling a row/column by $\lambda$ scales $det(A)$ by $\lambda$; $det(\lambda A) = \lambda^n det(A)$.
  - Swapping two rows/columns flips the sign of $det(A)$.
- **Rank Connection**: $det(A) \neq 0$ if and only if $rk(A) = n$ (Theorem 4.3), i.e., $A$ is full rank and invertible. In other words, $A$ is invertible if and only if it is full rank.
- **Computation**: Gaussian elimination can compute $det(A)$ by reducing $A$ to triangular form, leveraging the above properties.

### Trace
- **Definition**: $tr(A) = \sum_{i=1}^n a_{ii}$.
- **Properties**:
  - $tr(A + B) = tr(A) + tr(B)$.
  - $tr(\alpha A) = \alpha tr(A)$.
  - $tr(I_n) = n$.
  - $tr(AB) = tr(BA)$ for $A \in \mathbb{R}^{n \times k}$, $B \in \mathbb{R}^{k \times n}$.
  - Invariant under cyclic permutations: $tr(AKL) = tr(KLA)$.
  - For vectors $x, y \in \mathbb{R}^n$, $tr(xy^\top) = tr(y^\top x) = y^\top x$ (special case of cyclic permutation).
- **Basis Independence**: For a linear mapping $\Phi: V \to V$, the trace of its matrix representation is invariant under basis change, as $tr(S^{-1}AS) = tr(AS^{-1}S) = tr(A)$.

### Characteristic Polynomial
- **Definition**: $p_A(\lambda) = det(A - \lambda I) = c_0 + c_1 \lambda + \cdots + c_{n-1} \lambda^{n-1} + (-1)^n \lambda^n$, where $c_0 = det(A)$, $c_{n-1} = (-1)^{n-1} tr(A)$.
- **Purpose**: Roots of $p_A(\lambda)$ are eigenvalues, linking determinants and traces to the next section.

---

## 4.2 Eigenvalues and Eigenvectors
### Overview
Eigenvalues and eigenvectors characterize a square matrix $A \in \mathbb{R}^{n \times n}$ and its associated linear mapping by identifying directions that are only scaled, not rotated.

### Definitions
- **Eigenvalue and Eigenvector**: $\lambda \in \mathbb{R}$ is an eigenvalue and $x \in \mathbb{R}^n \setminus \{0\}$ an eigenvector if $Ax = \lambda x$ (*Eigen equation*).
- **Equivalent Conditions**:
  - $(A - \lambda I)x = 0$ has a non-trivial solution.
  - $rk(A - \lambda I) < n$.
  - $det(A - \lambda I) = 0$.
- **Multiplicities**:
  - **Algebraic Multiplicity**: Number of times $\lambda_i$ is a root of $p_A(\lambda)$.
  - **Geometric Multiplicity**: Dimension of the eigenspace $E_{\lambda_i}$, i.e., number of linearly independent eigenvectors for $\lambda_i$.
- **Eigenspace**: $E_\lambda = \{x \mid (A - \lambda I)x = 0\}$, the null space of $A - \lambda I$.
- **Eigenspectrum**: Set of all eigenvalues.
- Eigenvectors are non-unique; if $x$ is an eigenvector, so is $cx$ for $c \neq 0$.
- **Collinearity and Codirection**: Two vectors that point in the same direction are called codirected. Two vectors are collinear if they codirected point in the same or the opposite direction.

### Computation
- **Theorem 4.8**: $\lambda$ is an eigenvalue if and only if $p_A(\lambda) = 0$.
- **Example**: For $A = \begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}$:
  - $p_A(\lambda) = (4 - \lambda)(3 - \lambda) - 2 = \lambda^2 - 7\lambda + 10 = (2 - \lambda)(5 - \lambda)$.
  - Eigenvalues: $\lambda_1 = 2$, $\lambda_2 = 5$.
  - Eigenvectors: For $\lambda = 5$, $E_5 = span\left(\begin{bmatrix} 2 \\ 1 \end{bmatrix}\right)$; for $\lambda = 2$, $E_2 = span\left(\begin{bmatrix} 1 \\ -1 \end{bmatrix}\right)$.

### Properties
- $A$ and $A^\top$ share eigenvalues, not necessarily eigenvectors.
- Similar matrices share eigenvalues, making them basis-invariant.
- Symmetric, positive definite matrices have positive real eigenvalues.

### Other details

**Theorem 4.12.** The eigenvectors $ x_1, \ldots, x_n $ of a matrix $ A \in \mathbb{R}^{n \times n} $ with $ n $ distinct eigenvalues $ \lambda_1, \ldots, \lambda_n $ are linearly independent. This theorem states that eigenvectors of a matrix with $ n $ distinct eigenvalues form a basis of $ \mathbb{R}^n $.

**Definition 4.13.** A square matrix $ A \in \mathbb{R}^{n \times n} $ is defective if it possesses fewer than $ n $ linearly independent eigenvectors.

A non-defective matrix $ A \in \mathbb{R}^{n \times n} $ does not necessarily require $ n $ distinct eigenvalues, but it does require that the eigenvectors form a basis of $ \mathbb{R}^n $. Looking at the eigenspaces of a defective matrix, it follows that the sum of the dimensions of the eigenspaces is less than $ n $. Specifically, a defective matrix has at least one eigenvalue $ \lambda_i $ with an algebraic multiplicity $ m > 1 $ and a geometric multiplicity of less than $ m $.

**Remark.** A defective matrix cannot have $ n $ distinct eigenvalues, as distinct eigenvalues have linearly independent eigenvectors (Theorem 4.12).

**Theorem 4.14.** Given a matrix $ A \in \mathbb{R}^{m \times n} $, we can always obtain a symmetric, positive semidefinite matrix $ S \in \mathbb{R}^{n \times n} $ by defining  $S := A^\top A.$

**Remark.** If $ rk(A) = n $, then $ S := A^\top A $ is symmetric, positive definite.


### Geometric Intuition
- Eigenvectors indicate directions stretched by $\lambda$; negative $\lambda$ flips the direction.
- Example mappings (Figure 4.4):
  - $A_1 = \begin{bmatrix} \frac{1}{2} & 0 \\ 0 & 2 \end{bmatrix}$: Scales axes, $det(A_1) = 1$.
  - $A_2 = \begin{bmatrix} 1 & \frac{1}{2} \\ 0 & 1 \end{bmatrix}$: Shears, repeated $\lambda = 1$, $det(A_2) = 1$.
  - $A_3$: Rotation by 30°, complex eigenvalues, $det(A_3) = 1$.
  - $A_4 = \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix}$: Collapses to 1D, $\lambda = 0, 2$, $det(A_4) = 0$.
  - $A_5 = \begin{bmatrix} 1 & \frac{1}{2} \\ \frac{1}{2} & 1 \end{bmatrix}$: Shear and stretch, $det(A_5) = \frac{3}{4}$.

### Applications
- **PageRank**: Eigenvector for the largest eigenvalue of a web transition matrix ranks page importance.
- **Neural Networks**: Eigenspectrum of a symmetrized connectivity matrix (e.g., C. elegans) reveals network structure.

### Spectral Theorem
- **Theorem 4.15**: Symmetric $A$ has real eigenvalues and an orthonormal basis of eigenvectors.
- **Example**: For $A = \begin{bmatrix} 3 & 2 & 2 \\ 2 & 3 & 2 \\ 2 & 2 & 3 \end{bmatrix}$, eigenvalues are $\lambda = 1$ (multiplicity 2), $\lambda = 7$. Gram-Schmidt orthogonalizes the eigenspace $E_1$.

### Determinant and Trace Connection
- **Theorem 4.16**: $det(A) = \prod_{i=1}^n \lambda_i$.
- **Theorem 4.17**: $tr(A) = \sum_{i=1}^n \lambda_i$.
- **Intuition**: Eigenvalues scale areas (determinant) and perimeters (trace) of transformed shapes.

---

## 4.3 Cholesky Decomposition
### Overview
For symmetric, positive definite (SPD) matrices, the Cholesky decomposition provides a "square root" factorization: $A = LL^\top$, where $L$ is lower triangular with positive diagonal entries.

### Properties
- Exists and is unique for SPD matrices.
- Useful for efficient computation and sampling in machine learning (e.g., Gaussian processes).

---

## 4.4 Matrix Diagonalization
### Overview
A matrix $A \in \mathbb{R}^{n \times n}$ is diagonalizable if $A = PDP^{-1}$, where $D$ is diagonal (eigenvalues) and $P$ contains eigenvectors. Requires $n$ linearly independent eigenvectors (non-defective).

### Conditions
- Distinct eigenvalues guarantee diagonalizability (Theorem 4.12).
- Defective matrices (fewer than $n$ independent eigenvectors) are not diagonalizable.

---

## 4.5 Singular Value Decomposition (SVD)
### Overview
SVD generalizes eigendecomposition to any $A \in \mathbb{R}^{m \times n}$: $A = U \Sigma V^\top$, where $U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ are orthogonal, and $\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with singular values $\sigma_i \geqslant 0$.

### Geometric Intuition
- Decomposes $A$ into:
  1. Basis change in domain ($V^\top$).
  2. Scaling and dimension adjustment ($\Sigma$).
  3. Basis change in codomain ($U$).
- Example: Mapping a 2D grid to 3D (Figure 4.9).

### Construction
- **Right-Singular Vectors (V)**: Eigenvectors of $A^\top A$, with $\sigma_i^2$ as eigenvalues.
- **Left-Singular Vectors (U)**: $u_i = \frac{1}{\sigma_i} A v_i$, orthonormalized images.
- **Singular Values**: $\sigma_i = \sqrt{\lambda_i}$ of $A^\top A$ or $A A^\top$.
- **Example**: For $A = \begin{bmatrix} 1 & 0 & 1 \\ -2 & 1 & 0 \end{bmatrix}$, SVD yields $U$, $\Sigma$, $V^\top$.

### Comparison with Eigendecomposition
- SVD applies to all matrices; eigendecomposition requires square, non-defective matrices.
- $U$ and $V$ are orthogonal; $P$ in eigendecomposition may not be.
- $\Sigma$ has non-negative entries; $D$ can have negative or complex entries.

### Application: Movie Ratings
- $A \in \mathbb{R}^{4 \times 3}$ (movies × viewers) decomposes into stereotypical movies ($U$) and viewers ($V$), linked by $\Sigma$. Reveals themes like sci-fi or art house preferences.

---

## 4.6 Matrix Approximation
### Overview
SVD enables low-rank approximations: $A = \sum_{i=1}^r \sigma_i u_i v_i^\top$. Truncating to $k < r$ terms gives $\hat{A}(k) = \sum_{i=1}^k \sigma_i u_i v_i^\top$.

### Properties
- **Rank-1 Matrices**: $A_i = u_i v_i^\top$.
- **Optimality**: Eckart-Young theorem states $\hat{A}(k)$ minimizes $||A - B||_2$ for $rk(B) \leq k$, with error $||A - \hat{A}(k)||_2 = \sigma_{k+1}$.
- **Example**: Image compression (Stonehenge) reduces storage from 2.7M to 16K numbers at rank-5.

### Application: Movie Ratings (Continued)
- Rank-1 captures sci-fi lovers; rank-2 includes art house, closely approximating the original matrix.

---

## 4.7 Matrix Phylogeny
### Overview
A taxonomy (Figure 4.13) categorizes matrices:
- **All Matrices**: $\mathbb{R}^{m \times n}$, SVD applies.
- **Square**: $\mathbb{R}^{n \times n}$, determinants and traces apply.
- **Invertible**: $det(A) \neq 0$.
- **Non-Defective**: $n$ independent eigenvectors, diagonalizable.
- **Normal**: $A^\top A = A A^\top$, includes orthogonal ($A^\top = A^{-1}$) and symmetric ($A = A^\top$).
- **Symmetric**: Real eigenvalues; includes positive definite (Cholesky) and diagonal matrices.

---