# Linear Algebra

This chapter, sourced from *Mathematics for Machine Learning* by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020), provides a comprehensive introduction to linear algebra, emphasizing its significance in machine learning and general mathematics. Below is a detailed summary of the key concepts, definitions, theorems, and examples covered in the chapter, enriched with additional insights where relevant.

---

## Introduction to Linear Algebra

Linear algebra is introduced as the study of vectors and rules for their manipulation, forming an algebra—a set of objects (symbols) and rules to operate on them. The chapter distinguishes between familiar "geometric vectors" (denoted with arrows, e.g., $\vec{x}$) and more general vectors (denoted in bold, e.g., $\boldsymbol{x}$), which are central to the discussion.

### Vectors as Abstract Objects
Vectors are defined abstractly as objects that can be:
1. Added together to produce another vector of the same kind.
2. Multiplied by scalars to produce another vector of the same kind.

Examples of vector objects include:
1. **Geometric Vectors**: Familiar from high school, these are directed segments in 2D or 3D space (e.g., Figure 2.1(a)). Addition of two geometric vectors $\vec{x} + \vec{y} = \vec{z}$ results in another geometric vector, and scalar multiplication $\lambda \vec{x}$ scales the vector, preserving its type.
2. **Polynomials**: These are abstract vectors (e.g., Figure 2.1(b)). Two polynomials can be added or scaled, resulting in another polynomial, satisfying the vector properties.
3. **Audio Signals**: Represented as sequences of numbers, audio signals can be added or scaled, making them vectors.
4. **Elements of $\mathbb{R}^n$**: Tuples of $n$ real numbers (e.g., $\boldsymbol{a} = [1, 2, 3]^T \in \mathbb{R}^3$). Addition and scalar multiplication are performed component-wise, making them vectors. This representation is emphasized due to its relevance in computational implementations (arrays in programming languages).

### Focus on $\mathbb{R}^n$
The chapter focuses on vectors in $\mathbb{R}^n$ because most linear algebra algorithms are formulated in this space, and it loosely corresponds to data representations in computing. Finite-dimensional vector spaces are emphasized, where there is a one-to-one correspondence between any vector type and $\mathbb{R}^n$.

### Closure and Vector Spaces
A key mathematical concept introduced is **closure**: the set of all objects resulting from proposed operations (e.g., vector addition and scalar multiplication). Starting with a small set of vectors, the set of all vectors obtainable by adding and scaling them forms a **vector space** (detailed in Section 2.4). Vector spaces are foundational to machine learning.

### Relevance to Machine Learning
Linear algebra is highlighted as crucial for machine learning, with applications in:
- **Chapter 3**: Geometry via inner products and norms.
- **Chapter 5**: Vector calculus, relying on matrix operations.
- **Chapter 9**: Linear regression, using linear algebra for least-squares solutions.
- **Chapter 10**: Principal component analysis (PCA), leveraging projections (Section 3.8) for dimensionality reduction.

### Additional Resources
The chapter is based on lecture notes and books by Drumm and Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann (2015), and Pavel Grinfeld’s Linear Algebra series. Online resources like Gilbert Strang’s MIT course and 3Blue1Brown’s Linear Algebra Series are recommended for further learning.

---

## 2.1 Systems of Linear Equations

Systems of linear equations are central to linear algebra, as many problems can be formulated in this form, and linear algebra provides tools to solve them.

### General Form
A system of linear equations is given by:
$$
\begin{aligned}
a_{11} x_1 + \cdots + a_{1n} x_n &= b_1 \\
\vdots \\
a_{m1} x_1 + \cdots + a_{mn} x_n &= b_m
\end{aligned}
$$
where $a_{ij}, b_i \in \mathbb{R}$ are known, and $x_j$ are unknowns. Solutions are $n$-tuples $(x_1, \ldots, x_n) \in \mathbb{R}^n$ satisfying all equations.

### Example 2.1: Optimal Production Plan
A company produces products $N_1, \ldots, N_n$ requiring resources $R_1, \ldots, R_m$. Producing one unit of $N_j$ requires $a_{ij}$ units of $R_i$. The goal is to produce $x_j$ units of $N_j$ using exactly $b_i$ units of $R_i$. This leads to the system:
$$
a_{i1} x_1 + \cdots + a_{in} x_n = b_i, \quad i=1, \ldots, m
$$

### Example 2.2: Solutions to Systems
Three cases are illustrated:
1. **No Solution**:
   $$
   \begin{aligned}
   x_1 + x_2 + x_3 &= 3 \\
   x_1 - x_2 + 2x_3 &= 2 \\
   x_1 + x_3 &= 1
   \end{aligned}
   $$
   Adding the first two equations gives $2x_1 + 3x_3 = 5$, contradicting the third ($x_1 + x_3 = 1$). No solution exists.
2. **Unique Solution**:
   $$
   \begin{aligned}
   x_1 + x_2 + x_3 &= 3 \\
   x_1 - x_2 + 2x_3 &= 2 \\
   x_2 + x_3 &= 2
   \end{aligned}
   $$
   Solving yields $x_1 = 1, x_3 = 1, x_2 = 1$, so $(1, 1, 1)$ is the unique solution.
3. **Infinitely Many Solutions**:
   $$
   \begin{aligned}
   x_1 + x_2 + x_3 &= 3 \\
   x_1 - x_2 + 2x_3 &= 2 \\
   x_1 + x_3 &= 5
   \end{aligned}
   $$
   The third equation is redundant (derived from the first two). Solving gives $x_1 = \frac{5}{2} - \frac{3}{2}a, x_2 = \frac{1}{2} + \frac{1}{2}a, x_3 = a$, where $a \in \mathbb{R}$ is a free variable, yielding infinitely many solutions.

### Geometric Interpretation
Geometrically, each equation in a system defines a hyperplane in $\mathbb{R}^n$. The solution set is their intersection, which can be:
- Empty (parallel hyperplanes, no solution).
- A point (unique solution).
- A line, plane, or higher-dimensional subspace (infinitely many solutions).

For two variables ($x_1, x_2$), each equation is a line in the $x_1x_2$-plane. The intersection can be:
- A point (unique solution, e.g., solving $4x_1 + 4x_2 = 5$ and $2x_1 - 4x_2 = 1$ gives $(1, \frac{1}{4})$).
- A line (same line, infinitely many solutions).
- Empty (parallel lines, no solution).

In three dimensions, equations define planes, intersecting in a plane, line, point, or empty set.

### Matrix Representation
Systems are compactly represented using matrices:
$$
\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}
$$
where $\boldsymbol{A} = [a_{ij}] \in \mathbb{R}^{m \times n}$, $\boldsymbol{x} = [x_1, \ldots, x_n]^T \in \mathbb{R}^n$, and $\boldsymbol{b} = [b_1, \ldots, b_m]^T \in \mathbb{R}^m$. This form is explored further in Section 2.3.

---

## 2.2 Matrices

Matrices are central to linear algebra, used to represent systems of linear equations and linear mappings (Section 2.7).

### Definition 2.1: Matrix
A real-valued $(m, n)$ matrix $\boldsymbol{A}$ is an $m \times n$-tuple of elements $a_{ij}$, arranged in $m$ rows and $n$ columns:
$$
\boldsymbol{A} = \begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}, \quad a_{ij} \in \mathbb{R}
$$
- **Rows and Columns**: Rows are $(1, n)$-matrices (row vectors), and columns are $(m, 1)$-matrices (column vectors).
- **Set of Matrices**: $\mathbb{R}^{m \times n}$ denotes all such matrices.
- **Vector Representation**: A matrix $\boldsymbol{A} \in \mathbb{R}^{m \times n}$ can be stacked into a vector $\boldsymbol{a} \in \mathbb{R}^{mn}$ (Figure 2.4).

### 2.2.1 Matrix Addition and Multiplication

#### Matrix Addition
The sum of two matrices $\boldsymbol{A}, \boldsymbol{B} \in \mathbb{R}^{m \times n}$ is element-wise:
$$
\boldsymbol{A} + \boldsymbol{B} = \begin{bmatrix}
a_{11} + b_{11} & \cdots & a_{1n} + b_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} + b_{m1} & \cdots & a_{mn} + b_{mn}
\end{bmatrix} \in \mathbb{R}^{m \times n}
$$

#### Matrix Multiplication
For $\boldsymbol{A} \in \mathbb{R}^{m \times n}$ and $\boldsymbol{B} \in \mathbb{R}^{n \times k}$, the product $\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B} \in \mathbb{R}^{m \times k}$ has elements:
$$
c_{ij} = \sum_{l=1}^n a_{il} b_{lj}, \quad i=1, \ldots, m, \quad j=1, \ldots, k
$$
This is the dot product of the $i$-th row of $\boldsymbol{A}$ with the $j$-th column of $\boldsymbol{B}$ (Section 3.2).

**Remarks**:
- Matrices can only be multiplied if their "neighboring" dimensions match (e.g., $\boldsymbol{A} \in \mathbb{R}^{n \times k}$, $\boldsymbol{B} \in \mathbb{R}^{k \times m}$).
- Matrix multiplication is not element-wise (unlike Hadamard product, $\boldsymbol{A} \odot \boldsymbol{B}$, common in programming).
- Matrix multiplication is not commutative: $\boldsymbol{A} \boldsymbol{B} \neq \boldsymbol{B} \boldsymbol{A}$ (even if both are defined, results may differ in dimension, Figure 2.5).

**Example 2.3**:
For $\boldsymbol{A} = \begin{bmatrix} 1 & 2 & 3 \\ 3 & 2 & 1 \end{bmatrix} \in \mathbb{R}^{2 \times 3}$ and $\boldsymbol{B} = \begin{bmatrix} 0 & 2 \\ 1 & -1 \\ 0 & 1 \end{bmatrix} \in \mathbb{R}^{3 \times 2}$:
- $\boldsymbol{A} \boldsymbol{B} = \begin{bmatrix} 2 & 3 \\ 2 & 5 \end{bmatrix} \in \mathbb{R}^{2 \times 2}$
- $\boldsymbol{B} \boldsymbol{A} = \begin{bmatrix} 6 & 4 & 2 \\ -2 & 0 & 2 \\ 3 & 2 & 1 \end{bmatrix} \in \mathbb{R}^{3 \times 3}$

#### Identity Matrix
**Definition 2.2**: The identity matrix $\boldsymbol{I}_n \in \mathbb{R}^{n \times n}$ has 1s on the diagonal and 0s elsewhere:
$$
\boldsymbol{I}_n = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}
$$
For any $\boldsymbol{A} \in \mathbb{R}^{m \times n}$, $\boldsymbol{I}_m \boldsymbol{A} = \boldsymbol{A} \boldsymbol{I}_n = \boldsymbol{A}$.

#### Properties of Matrices
- **Associativity**:
  - $(\boldsymbol{A} \boldsymbol{B}) \boldsymbol{C} = \boldsymbol{A} (\boldsymbol{B} \boldsymbol{C})$ for $\boldsymbol{A} \in \mathbb{R}^{m \times n}$, $\boldsymbol{B} \in \mathbb{R}^{n \times p}$, $\boldsymbol{C} \in \mathbb{R}^{p \times q}$.
- **Distributivity**:
  - $(\boldsymbol{A} + \boldsymbol{B}) \boldsymbol{C} = \boldsymbol{A} \boldsymbol{C} + \boldsymbol{B} \boldsymbol{C}$ for $\boldsymbol{A}, \boldsymbol{B} \in \mathbb{R}^{m \times n}$, $\boldsymbol{C} \in \mathbb{R}^{n \times p}$.
  - $\boldsymbol{A} (\boldsymbol{C} + \boldsymbol{D}) = \boldsymbol{A} \boldsymbol{C} + \boldsymbol{A} \boldsymbol{D}$ for $\boldsymbol{A} \in \mathbb{R}^{m \times n}$, $\boldsymbol{C}, \boldsymbol{D} \in \mathbb{R}^{n \times p}$.

### 2.2.2 Inverse and Transpose

#### 1) Inverse
**Definition 2.3**: For a square matrix $\boldsymbol{A} \in \mathbb{R}^{n \times n}$, the inverse $\boldsymbol{A}^{-1} \in \mathbb{R}^{n \times n}$ satisfies:
$$
\boldsymbol{A} \boldsymbol{A}^{-1} = \boldsymbol{I}_n = \boldsymbol{A}^{-1} \boldsymbol{A}
$$
- If $\boldsymbol{A}^{-1}$ exists, $\boldsymbol{A}$ is **regular/invertible/nonsingular**; otherwise, it is **singular/noninvertible**.
- The inverse, if it exists, is unique.
- General computation of inverses is discussed in Section 2.3.

**Remark (2x2 Matrix Inverse)**: For $\boldsymbol{A} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \in \mathbb{R}^{2 \times 2}$, the inverse is:
$$
\boldsymbol{A}^{-1} = \frac{1}{a_{11} a_{22} - a_{12} a_{21}} \begin{bmatrix} a_{22} & -a_{12} \\ -a_{21} & a_{11} \end{bmatrix}
$$
if $a_{11} a_{22} - a_{12} a_{21} \neq 0$ (determinant, detailed in Chapter 4).

**Example 2.4**:
Matrices $\boldsymbol{A} = \begin{bmatrix} 1 & 2 & 1 \\ 4 & 4 & 5 \\ 6 & 7 & 7 \end{bmatrix}$ and $\boldsymbol{B} = \begin{bmatrix} -7 & -7 & 6 \\ 2 & 1 & -1 \\ 4 & 5 & -4 \end{bmatrix}$ are inverses, as $\boldsymbol{A} \boldsymbol{B} = \boldsymbol{I} = \boldsymbol{B} \boldsymbol{A}$.

#### 2) Transpose
**Definition 2.4**: The transpose of $\boldsymbol{A} \in \mathbb{R}^{m \times n}$ is $\boldsymbol{A}^T \in \mathbb{R}^{n \times m}$, where $(\boldsymbol{A}^T)_{ij} = a_{ji}$. Columns of $\boldsymbol{A}$ become rows of $\boldsymbol{A}^T$.

**Properties**:
- $\boldsymbol{A} \boldsymbol{A}^{-1} = \boldsymbol{I} = \boldsymbol{A}^{-1} \boldsymbol{A}$
- $(\boldsymbol{A} \boldsymbol{B})^{-1} = \boldsymbol{B}^{-1} \boldsymbol{A}^{-1}$
- $(\boldsymbol{A} + \boldsymbol{B})^{-1} \neq \boldsymbol{A}^{-1} + \boldsymbol{B}^{-1}$
- $(\boldsymbol{A}^T)^T = \boldsymbol{A}$
- $(\boldsymbol{A} \boldsymbol{B})^T = \boldsymbol{B}^T \boldsymbol{A}^T$
- $(\boldsymbol{A} + \boldsymbol{B})^T = \boldsymbol{A}^T + \boldsymbol{B}^T$

#### Symmetric Matrix
**Definition 2.5**: A matrix $\boldsymbol{A} \in \mathbb{R}^{n \times n}$ is symmetric if $\boldsymbol{A} = \boldsymbol{A}^T$. Only square matrices can be symmetric.

**Properties**:
- If $\boldsymbol{A}$ is invertible, so is $\boldsymbol{A}^T$, and $(\boldsymbol{A}^{-1})^T = (\boldsymbol{A}^T)^{-1}$.
- The sum of symmetric matrices is symmetric.
- The product of symmetric matrices is not necessarily symmetric (e.g., $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$).

### 2.2.3 Multiplication by a Scalar
For $\boldsymbol{A} \in \mathbb{R}^{m \times n}$ and $\lambda \in \mathbb{R}$, the scalar multiple $\lambda \boldsymbol{A}$ has elements $(\lambda \boldsymbol{A})_{ij} = \lambda a_{ij}$. Properties include:
- **Associativity**:
  - $(\lambda \psi) \boldsymbol{C} = \lambda (\psi \boldsymbol{C})$ for $\boldsymbol{C} \in \mathbb{R}^{m \times n}$.
  - $\lambda (\boldsymbol{B} \boldsymbol{C}) = (\lambda \boldsymbol{B}) \boldsymbol{C} = \boldsymbol{B} (\lambda \boldsymbol{C})$ for $\boldsymbol{B} \in \mathbb{R}^{m \times n}$, $\boldsymbol{C} \in \mathbb{R}^{n \times k}$.
- **Transpose**: $(\lambda \boldsymbol{C})^T = \lambda \boldsymbol{C}^T$.
- **Distributivity**:
  - $(\lambda + \psi) \boldsymbol{C} = \lambda \boldsymbol{C} + \psi \boldsymbol{C}$ for $\boldsymbol{C} \in \mathbb{R}^{m \times n}$.
  - $\lambda (\boldsymbol{B} + \boldsymbol{C}) = \lambda \boldsymbol{B} + \lambda \boldsymbol{C}$ for $\boldsymbol{B}, \boldsymbol{C} \in \mathbb{R}^{m \times n}$.

**Example 2.5**:
For $\boldsymbol{C} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$, $(\lambda + \psi) \boldsymbol{C} = \lambda \boldsymbol{C} + \psi \boldsymbol{C}$ is verified element-wise.

### 2.2.4 Compact Representations of Systems of Linear Equations
A system like:
$$
\begin{aligned}
2x_1 + 3x_2 + 5x_3 &= 1 \\
4x_1 - 2x_2 - 7x_3 &= 8 \\
9x_1 + 5x_2 - 3x_3 &= 2
\end{aligned}
$$
is compactly written as:
$$
\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}, \quad \text{where} \quad \boldsymbol{A} = \begin{bmatrix} 2 & 3 & 5 \\ 4 & -2 & -7 \\ 9 & 5 & -3 \end{bmatrix}, \quad \boldsymbol{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}, \quad \boldsymbol{b} = \begin{bmatrix} 1 \\ 8 \\ 2 \end{bmatrix}
$$
Here, $\boldsymbol{A} \boldsymbol{x}$ is a linear combination of $\boldsymbol{A}$'s columns, scaled by $\boldsymbol{x}$'s components (detailed in Section 2.5).

---

## 2.3 Solving Systems of Linear Equations

This section focuses on solving $\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}$, introducing algorithms and concepts to find particular and general solutions.

### 2.3.1 Particular and General Solution

**Example**:
Consider:
$$
\begin{bmatrix}
1 & 0 & 8 & -4 \\
0 & 1 & 2 & 12
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4
\end{bmatrix}
= \begin{bmatrix}
42 \\ 8
\end{bmatrix}
$$
- **Particular Solution**: A solution is $[42, 8, 0, 0]^T$, found by inspection (42 times column 1 + 8 times column 2 equals $\boldsymbol{b}$).
- **General Solution**: To find all solutions, consider the homogeneous system $\boldsymbol{A} \boldsymbol{x} = \mathbf{0}$. Express non-pivot columns (3 and 4) in terms of pivot columns (1 and 2):
  - Column 3: $\begin{bmatrix} 8 \\ 2 \end{bmatrix} = 8 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 2 \begin{bmatrix} 0 \\ 1 \end{bmatrix}$, so $\mathbf{0} = 8 \boldsymbol{c}_1 + 2 \boldsymbol{c}_2 - \boldsymbol{c}_3$, scaled by $\lambda_1$.
  - Column 4: $\begin{bmatrix} -4 \\ 12 \end{bmatrix} = -4 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 12 \begin{bmatrix} 0 \\ 1 \end{bmatrix}$, so $\mathbf{0} = -4 \boldsymbol{c}_1 + 12 \boldsymbol{c}_2 - \boldsymbol{c}_4$, scaled by $\lambda_2$.
  - General solution:
    $$
    \boldsymbol{x} = \begin{bmatrix} 42 \\ 8 \\ 0 \\ 0 \end{bmatrix} + \lambda_1 \begin{bmatrix} 8 \\ 2 \\ -1 \\ 0 \end{bmatrix} + \lambda_2 \begin{bmatrix} -4 \\ 12 \\ 0 \\ -1 \end{bmatrix}, \quad \lambda_1, \lambda_2 \in \mathbb{R}
    $$

**General Approach**:
1. Find a particular solution to $\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}$.
2. Find all solutions to $\boldsymbol{A} \boldsymbol{x} = \mathbf{0}$ (homogeneous solution).
3. Combine: General solution = particular solution + homogeneous solution.

Neither solution is unique.

### 2.3.2 Elementary Transformations
To solve general systems, **elementary transformations** are used to simplify the system while preserving solutions:
- Swap two equations (rows).
- Multiply an equation (row) by $\lambda \in \mathbb{R} \backslash \{0\}$.
- Add two equations (rows).

**Example 2.6**:
Solve:
$$
\begin{aligned}
-2x_1 + 4x_2 - 2x_3 - x_4 + 4x_5 &= -3 \\
4x_1 - 8x_2 + 3x_3 - 3x_4 + x_5 &= 2 \\
x_1 - 2x_2 + x_3 - x_4 + x_5 &= 0 \\
x_1 - 2x_2 - 3x_4 + 4x_5 &= a
\end{aligned}
$$
- Form the **augmented matrix** $[\boldsymbol{A} | \boldsymbol{b}]$:
  $$
  \begin{bmatrix}
  -2 & 4 & -2 & -1 & 4 & | & -3 \\
  4 & -8 & 3 & -3 & 1 & | & 2 \\
  1 & -2 & 1 & -1 & 1 & | & 0 \\
  1 & -2 & 0 & -3 & 4 & | & a
  \end{bmatrix}
  $$
- Apply transformations (e.g., swap rows, subtract multiples) to reach **row-echelon form (REF)**:
  $$
  \begin{bmatrix}
  1 & -2 & 1 & -1 & 1 & | & 0 \\
  0 & 0 & -1 & 1 & -3 & | & 2 \\
  0 & 0 & 0 & -3 & 6 & | & -3 \\
  0 & 0 & 0 & 0 & 0 & | & a + 1
  \end{bmatrix}
  $$
- The system is solvable only if $a = -1$ (last row implies $0 = a + 1$).
- **Particular Solution**: For $a = -1$, solve the REF system to get $\boldsymbol{x} = [2, 0, -1, 1, 0]^T$.
- **General Solution**: Identify basic variables ($x_1, x_3, x_4$) and free variables ($x_2, x_5$). The general solution is:
  $$
  \boldsymbol{x} = \begin{bmatrix} 2 \\ 0 \\ -1 \\ 1 \\ 0 \end{bmatrix} + \lambda_1 \begin{bmatrix} 2 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} + \lambda_2 \begin{bmatrix} 2 \\ 0 \\ -1 \\ 2 \\ 1 \end{bmatrix}, \quad \lambda_1, \lambda_2 \in \mathbb{R}
  $$

#### Row-Echelon Form (REF)
**Definition 2.6**:
A matrix is in REF if:
- Rows of all zeros are at the bottom.
- In nonzero rows, the **pivot** (first nonzero entry, or leading coefficient) is strictly right of the pivot in the row above, forming a "staircase" structure.

**Remarks**:
- **Basic and Free Variables**: Variables corresponding to pivots are basic; others are free.
- **Particular Solution**: Express $\boldsymbol{b}$ using pivot columns, solving right to left.

#### Reduced Row-Echelon Form (RREF)
**Remark**:
A matrix is in RREF if it is in REF and:
- Every pivot is 1.
- The pivot is the only nonzero entry in its column.

RREF simplifies finding general solutions.

**Example 2.7**:
Verify that:
$$
\boldsymbol{A} = \begin{bmatrix}
1 & 3 & 0 & 0 & 3 \\
0 & 0 & 1 & 0 & 9 \\
0 & 0 & 0 & 1 & -4
\end{bmatrix}
$$
is in RREF (pivots in bold). Solve $\boldsymbol{A} \boldsymbol{x} = \mathbf{0}$:
- Non-pivot columns (2 and 5) are expressed in terms of pivot columns (1, 3, 4).
- Solutions:
  $$
  \boldsymbol{x} = \lambda_1 \begin{bmatrix} 3 \\ -1 \\ 0 \\ 0 \\ 0 \end{bmatrix} + \lambda_2 \begin{bmatrix} 3 \\ 0 \\ 9 \\ -4 \\ -1 \end{bmatrix}, \quad \lambda_1, \lambda_2 \in \mathbb{R}
  $$

#### Gaussian Elimination
**Remark**:
Gaussian elimination is an algorithm using elementary transformations to bring a system into RREF, facilitating solution finding.

### 2.3.3 The Minus-1 Trick
A practical method to find solutions to $\boldsymbol{A} \boldsymbol{x} = \mathbf{0}$ when $\boldsymbol{A} \in \mathbb{R}^{k \times n}$ is in RREF (no zero rows):
- Augment $\boldsymbol{A}$ to an $n \times n$ matrix $\tilde{\boldsymbol{A}}$ by adding rows of the form $[0, \ldots, 0, -1, 0, \ldots, 0]$, placing -1s in non-pivot columns.
- Columns of $\tilde{\boldsymbol{A}}$ with -1 pivots form a basis for the solution space (kernel/null space, Section 2.7.3).

**Example 2.8**:
For $\boldsymbol{A}$ in RREF:
$$
\boldsymbol{A} = \begin{bmatrix}
1 & 3 & 0 & 0 & 3 \\
0 & 0 & 1 & 0 & 9 \\
0 & 0 & 0 & 1 & -4
\end{bmatrix}
$$
The Minus-1 Trick confirms the solutions from Example 2.7.

---

## Sections 2.4–2.6

### Vector Spaces (Section 2.4)
A vector space is the set of all vectors obtainable by adding and scaling a set of vectors, closed under these operations. Key properties include the existence of a zero vector and additive inverses.

### Linear Combinations (Section 2.5)

- **Linear Combination**: For $V$, vectors $\boldsymbol{x}_1, \ldots, \boldsymbol{x}_k \in V$, a linear combination is:
$\boldsymbol{v} = \lambda_1 \boldsymbol{x}_1 + \cdots + \lambda_k \boldsymbol{x}k = \sum{i=1}^k \lambda_i \boldsymbol{x}_i$, $\lambda_i \in \mathbb{R}$.
Trivial Case: $\mathbf{0} = \sum_{i=1}^k 0 \boldsymbol{x}_i$ always holds.

- **Linear (In)dependence**: 
  - Linearly Dependent: $\exists$ non-trivial $\lambda_i$ (not all zero) such that $\mathbf{0} = \sum_{i=1}^k \lambda_i \boldsymbol{x}_i$.
  - Linearly Independent: Only the trivial solution $\lambda_1 = \cdots = \lambda_k = 0$ satisfies $\mathbf{0} = \sum_{i=1}^k \lambda_i \boldsymbol{x}_i$.

Intuition: Independent vectors have no redundancy; removing one loses unique span.

### Basis & Rank (Section 2.6)
- **Generating Set and Span**: For vector space $V = (\mathcal{V}, +, \cdot)$, $\mathcal{A} = {\boldsymbol{x}_1, \ldots, \boldsymbol{x}_k} \subseteq \mathcal{V}$ is a generating set if every $\boldsymbol{v} \in \mathcal{V}$ is a linear combination of $\mathcal{A}$. The span is all such combinations, denoted $V = \text{span}[\mathcal{A}]$.
- **Basis**: A generating set $\mathcal{A}$ is a basis if:
  - It is minimal (no proper subset spans $V$).
  - It is linearly independent.
- **Dimension**: The number of vectors in a basis, constant for a given space.
- **Rank**: Rank of $\boldsymbol{A} \in \mathbb{R}^{m \times n}$, $\text{rk}(\boldsymbol{A})$, is the number of linearly independent columns (equals independent rows).

---

## 2.7 Linear Mappings

Linear mappings (or transformations) are functions between vector spaces preserving vector addition and scalar multiplication, represented by matrices.

### 2.7.1 Definition and Representation

**Definition**:
A mapping $\Phi: V \rightarrow W$ between vector spaces $V$ and $W$ is linear if:
- $\Phi(\boldsymbol{u} + \boldsymbol{v}) = \Phi(\boldsymbol{u}) + \Phi(\boldsymbol{v})$ for all $\boldsymbol{u}, \boldsymbol{v} \in V$.
- $\Phi(\lambda \boldsymbol{u}) = \lambda \Phi(\boldsymbol{u})$ for all $\lambda \in \mathbb{R}$, $\boldsymbol{u} \in V$.


**Definition (Injective, Surjective, Bijective)**:

1. **Injective mapping**: A function $\Phi$ is injective if different inputs map to different outputs, i.e., $\Phi(x) = \Phi(y)$ implies $x = y$.

2. **Surjective mapping**: A function $\Phi$ is surjective if every element in the target set $\mathcal{W}$ has at least one preimage in the domain $\mathcal{V}$, i.e., $\Phi(\mathcal{V}) = \mathcal{W}$.

3. **Bijective mapping**: A function $\Phi$ is bijective if it is both injective and surjective, meaning every element in the target set has a unique preimage. In this case, $\Phi$ has an inverse function $\Psi = \Phi^{-1}$ such that $\Psi \circ \Phi(x) = x$.

**Theorem 2.17** (from context):
Finite-dimensional vector spaces of the same dimension are isomorphic (there exists a bijective linear mapping between them) if and only if dim(V) = dim(W).

#### Coordinate Systems and Bases
A basis defines a coordinate system. For $\boldsymbol{x} \in \mathbb{R}^2$ with standard basis $(\boldsymbol{e}_1, \boldsymbol{e}_2)$, coordinates are $[x_1, x_2]^T$. In a different basis $(\boldsymbol{b}_1, \boldsymbol{b}_2)$, coordinates change (e.g., Figure 2.8 shows $\boldsymbol{x} = [2, 2]^T$ in standard basis becomes $[1.09, 0.72]^T$ in $(\boldsymbol{b}_1, \boldsymbol{b}_2)$).

**Example 2.20**:
For $\boldsymbol{x} = [2, 3]^T$ in standard basis $(\boldsymbol{e}_1, \boldsymbol{e}_2)$, using basis $\boldsymbol{b}_1 = [1, -1]^T, \boldsymbol{b}_2 = [1, 1]^T$, coordinates are $\frac{1}{2}[-1, 5]^T$ (Figure 2.9).

**Remark**:
For an $n$-dimensional space $V$ and an ordered basis $B = (\boldsymbol{b}_1, \ldots, \boldsymbol{b}_n)$ of $V$, the mapping $\Phi: \mathbb{R}^n \rightarrow V$, $\Phi(\boldsymbol{e}_i) = \boldsymbol{b}_i$, is linear (and because of Theorem 2.17 an isomorphism).

#### Transformation Matrix
**Definition 2.19**:
For a linear mapping $\Phi: V \rightarrow W$ with bases $B = (\boldsymbol{b}_1, \ldots, \boldsymbol{b}_n)$ of $V$ and $C = (\boldsymbol{c}_1, \ldots, \boldsymbol{c}_m)$ of $W$, the transformation matrix $\boldsymbol{A}_{\Phi} \in \mathbb{R}^{m \times n}$ has columns given by the coordinates of $\Phi(\boldsymbol{b}_j)$ in $C$:
$$
\Phi(\boldsymbol{b}_j) = \sum_{i=1}^m \alpha_{ij} \boldsymbol{c}_i, \quad \boldsymbol{A}_{\Phi}(i, j) = \alpha_{ij}
$$
If $\hat{\boldsymbol{x}}$ is the coordinate vector of $\boldsymbol{x} \in V$ w.r.t. $B$, and $\hat{\boldsymbol{y}}$ is the coordinate vector of $\boldsymbol{y} = \Phi(\boldsymbol{x}) \in W$ w.r.t. $C$, then:
$$
\hat{\boldsymbol{y}} = \boldsymbol{A}_{\Phi} \hat{\boldsymbol{x}}
$$

**Example 2.21**:
For $\Phi: V \rightarrow W$ with bases $B = (\boldsymbol{b}_1, \ldots, \boldsymbol{b}_5)$ and $C = (\boldsymbol{c}_1, \ldots, \boldsymbol{c}_4)$, given:
- $\Phi(\boldsymbol{b}_1) = \boldsymbol{c}_1 - \boldsymbol{c}_2 + 3 \boldsymbol{c}_3 - \boldsymbol{c}_4$
- $\Phi(\boldsymbol{b}_2) = 2 \boldsymbol{c}_1 + \boldsymbol{c}_2 + 7 \boldsymbol{c}_3 + 2 \boldsymbol{c}_4$
- $\Phi(\boldsymbol{b}_3) = 3 \boldsymbol{c}_2 + \boldsymbol{c}_3 + 4 \boldsymbol{c}_4$

The transformation matrix is:
$$
\boldsymbol{A}_{\Phi} = \begin{bmatrix}
1 & 2 & 0 \\
-1 & 1 & 3 \\
3 & 7 & 1 \\
-1 & 2 & 4
\end{bmatrix}
$$

**Example 2.22**:
Linear transformations of vectors in $\mathbb{R}^2$ are illustrated with specific matrix examples (referenced in Figure 2.10):
- **Rotation by 45°**: The transformation matrix is:
  $$
  \boldsymbol{A}_1 = \begin{bmatrix} \cos\left(\frac{\pi}{4}\right) & -\sin\left(\frac{\pi}{4}\right) \\ \sin\left(\frac{\pi}{4}\right) & \cos\left(\frac{\pi}{4}\right) \end{bmatrix} = \begin{bmatrix} \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix}
  $$
  This rotates a vector counterclockwise by 45° around the origin.
- **Horizontal Stretch by 2**: The transformation matrix is:
  $$
  \boldsymbol{A}_2 = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}
  $$
  This scales the horizontal component of a vector by a factor of 2 while leaving the vertical component unchanged.
- **Reflection, Rotation, and Stretch**: The document cuts off before fully specifying this matrix (denoted $\boldsymbol{A}$), but such transformations typically combine multiple effects (e.g., reflection across a line, rotation, and scaling). A common example might be a reflection across the x-axis followed by a stretch, such as:
  $$
  \boldsymbol{A} = 0.5 \begin{bmatrix} 3 & -1 \\ 1 & -1 \end{bmatrix}
  $$
  However, the exact matrix depends on the context in Figure 2.10, which is not provided here.

These examples demonstrate how matrices encode geometric transformations, a key application in computer graphics and machine learning (e.g., image processing).

---

### 2.7.2 Properties of Linear Mappings
Linear mappings preserve the structure of vector spaces, leading to several important properties:
- **Zero Vector**: $\Phi(\mathbf{0}) = \mathbf{0}$, since $\Phi(\mathbf{0}) = \Phi(0 \cdot \mathbf{0}) = 0 \cdot \Phi(\mathbf{0}) = \mathbf{0}$.
- **Additivity**: The mapping distributes over addition, ensuring that the image of a sum equals the sum of the images.
- **Homogeneity**: Scaling a vector before applying the mapping is equivalent to scaling the result.

These properties ensure that linear mappings can be fully represented by matrices in finite-dimensional spaces, as shown in Definition 2.19.

---

### 2.7.3 Kernel and Image
Two fundamental concepts associated with linear mappings are the **kernel** and **image**:
- **Kernel (Null Space)**: The kernel of $\Phi: V \rightarrow W$, denoted $\ker(\Phi)$, is the set of vectors in $V$ mapped to the zero vector in $W$:
  $$
  \ker(\Phi) = \{\boldsymbol{v} \in V \mid \Phi(\boldsymbol{v}) = \mathbf{0}\}
  $$
  - It is a subspace of $V$.
  - For $\Phi(\boldsymbol{x}) = \boldsymbol{A} \boldsymbol{x}$, the kernel is the solution set to $\boldsymbol{A} \boldsymbol{x} = \mathbf{0}$ (homogeneous system).
  - Example: For $\boldsymbol{A} = \begin{bmatrix} 1 & 3 & 0 \\ 0 & 0 & 1 \end{bmatrix}$, the kernel includes vectors like $[0, 0, 0]^T$ and others found via methods like the Minus-1 Trick (Section 2.3.3).
- **Image (Range)**: The image of $\Phi$, denoted $\text{im}(\Phi)$, is the set of all vectors in $W$ that are outputs of $\Phi$:
  $$
  \text{im}(\Phi) = \{\boldsymbol{w} \in W \mid \exists \boldsymbol{v} \in V \text{ such that } \Phi(\boldsymbol{v}) = \boldsymbol{w}\}
  $$
  - It is a subspace of $W$.
  - For $\Phi(\boldsymbol{x}) = \boldsymbol{A} \boldsymbol{x}$, the image is the span of $\boldsymbol{A}$’s columns (column space).

**Rank-Nullity Theorem**: For a linear mapping $\Phi: V \rightarrow W$ where $V$ is finite-dimensional with dimension $n$:
$$
\dim(\ker(\Phi)) + \dim(\text{im}(\Phi)) = n
$$
This theorem connects the dimensions of the kernel and image to the dimension of the domain, a critical result explored further in Chapter 4.

---

### 2.7.4 Composition of Linear Mappings
If $\Phi: U \rightarrow V$ and $\Psi: V \rightarrow W$ are linear mappings with transformation matrices $\boldsymbol{A}_{\Phi}$ and $\boldsymbol{A}_{\Psi}$ (relative to chosen bases), their composition $\Psi \circ \Phi: U \rightarrow W$ is also linear, with transformation matrix:
$$
\boldsymbol{A}_{\Psi \circ \Phi} = \boldsymbol{A}_{\Psi} \boldsymbol{A}_{\Phi}
$$
- Note the order: $\boldsymbol{A}_{\Phi}$ (applied first) is on the right, reflecting the non-commutative nature of matrix multiplication.

**Example**: If $\Phi$ rotates by 45° and $\Psi$ stretches horizontally by 2, the combined effect is computed as $\boldsymbol{A}_{\Psi} \boldsymbol{A}_{\Phi}$, first rotating then stretching.


## Other definitions

**Definition 2.21** (Equivalence). Two matrices $ A, \tilde{A} \in \mathbb{R}^{m \times n} $ are *equivalent* if there exist regular matrices $ S \in \mathbb{R}^{n \times n} $ and $ T \in \mathbb{R}^{m \times m} $, such that  
$\tilde{A} = T^{-1} A S.$

**Definition 2.22** (Similarity). Two matrices $ A, \tilde{A} \in \mathbb{R}^{n \times n} $ are *similar* if there exists a regular matrix $ S \in \mathbb{R}^{n \times n} $ with  
$\tilde{A} = S^{-1} A S.$

---

## Conclusion
This chapter lays the groundwork for linear algebra in machine learning by introducing vectors, matrices, systems of equations, and linear mappings. Key takeaways include:
- Vectors and matrices provide a unified framework for representing and solving systems of equations.
- Techniques like Gaussian elimination and the Minus-1 Trick offer practical tools for finding solutions.
- Linear mappings, represented by matrices, bridge algebraic and geometric interpretations, with applications in transformations and dimensionality reduction (e.g., PCA in Chapter 10).

The concepts build toward more advanced topics in subsequent chapters, such as determinants (Chapter 4), eigenvalues (Chapter 6), and their roles in optimization and data analysis.
