# Analytic Geometry

## Overview
Chapter 3 of *Mathematics for Machine Learning* shifts the focus from the abstract algebraic concepts of vectors, vector spaces, and linear mappings introduced in Chapter 2 to their geometric interpretations. It equips vector spaces with inner products to define lengths, distances, and angles, laying the groundwork for machine learning applications such as support vector machines (Chapter 12), principal component analysis (Chapter 10), and regression (Chapter 9). Figure 3.1 provides a mind map illustrating the interconnections between these concepts and their relevance to later chapters.

---

## 3.1 Norms
### Definition and Properties
A norm on a vector space $V$ is a function $||\cdot||: V \rightarrow \mathbb{R}$ that assigns a non-negative real number $||\boldsymbol{x}||$ (length) to each vector $\boldsymbol{x} \in V$, satisfying:
- **Absolute Homogeneity**: $||\lambda \boldsymbol{x}|| = |\lambda| ||\boldsymbol{x}||$ for all $\lambda \in \mathbb{R}$, meaning scaling a vector scales its length proportionally.
- **Triangle Inequality**: $||\boldsymbol{x} + \boldsymbol{y}|| \leq ||\boldsymbol{x}|| + ||\boldsymbol{y}||$, reflecting the geometric intuition that the length of one side of a triangle is at most the sum of the other two (see Figure 3.2).
- **Positive Definiteness**: $||\boldsymbol{x}|| \geq 0$, with $||\boldsymbol{x}|| = 0$ if and only if $\boldsymbol{x} = \mathbf{0}$, ensuring only the zero vector has zero length.

While defined generally for any vector space $V$, the book focuses on finite-dimensional spaces $\mathbb{R}^n$, where vectors $\boldsymbol{x}$ have components $x_i$.

### Examples
- **Manhattan Norm ($||\boldsymbol{x}||_1$)**:
  - Defined as $||\boldsymbol{x}||_1 := \sum_{i=1}^n |x_i|$, summing the absolute values of components.
  - Also called the $\ell_1$ norm, it measures length as the "taxicab distance" along coordinate axes.
  - In $\mathbb{R}^2$, vectors with $||\boldsymbol{x}||_1 = 1$ form a diamond shape (left panel of Figure 3.3).
- **Euclidean Norm ($||\boldsymbol{x}||_2$)**:
  - Defined as $||\boldsymbol{x}||_2 := \sqrt{\sum_{i=1}^n x_i^2} = \sqrt{\boldsymbol{x}^\top \boldsymbol{x}}$, computing the straight-line distance from the origin.
  - Known as the $\ell_2$ norm, it aligns with standard Euclidean geometry.
  - In $\mathbb{R}^2$, vectors with $||\boldsymbol{x}||_2 = 1$ form a unit circle (right panel of Figure 3.3).
  - Default norm in the book unless specified otherwise.

### Additional Insight
Norms generalize the intuitive notion of length to abstract spaces, crucial for measuring similarity or error in machine learning (e.g., $\ell_1$ for sparsity, $\ell_2$ for standard distance).

---

## 3.2 Inner Products
### Purpose
Inner products introduce geometric concepts like length, angle, and orthogonality, enhancing the utility of vector spaces beyond linear mappings.

### 3.2.1 Dot Product
- The familiar dot product in $\mathbb{R}^n$ is $\boldsymbol{x}^\top \boldsymbol{y} = \sum_{i=1}^n x_i y_i$, a specific inner product used as the default in the book.

### 3.2.2 General Inner Products
- A bilinear mapping $\Omega: V \times V \rightarrow \mathbb{R}$ is linear in each argument:
  - $\Omega(\lambda \boldsymbol{x} + \psi \boldsymbol{y}, \boldsymbol{z}) = \lambda \Omega(\boldsymbol{x}, \boldsymbol{z}) + \psi \Omega(\boldsymbol{y}, \boldsymbol{z})$
  - $\Omega(\boldsymbol{x}, \lambda \boldsymbol{y} + \psi \boldsymbol{z}) = \lambda \Omega(\boldsymbol{x}, \boldsymbol{y}) + \psi \Omega(\boldsymbol{x}, \boldsymbol{z})$
- **Definition 3.2**: $\Omega$ is:
  - **Symmetric** if $\Omega(\boldsymbol{x}, \boldsymbol{y}) = \Omega(\boldsymbol{y}, \boldsymbol{x})$.
  - **Positive Definite** if $\Omega(\boldsymbol{x}, \boldsymbol{x}) > 0$ for all $\boldsymbol{x} \neq \mathbf{0}$ and $\Omega(\mathbf{0}, \mathbf{0}) = 0$.
- **Definition 3.3**: An inner product is a symmetric, positive definite bilinear mapping, denoted $\langle \boldsymbol{x}, \boldsymbol{y} \rangle$. The pair $(V, \langle \cdot, \cdot \rangle)$ is an **inner product space**, or a **Euclidean vector space** if using the dot product.

### Example 3.3 
- In $\mathbb{R}^2$, $\langle \boldsymbol{x}, \boldsymbol{y} \rangle := x_1 y_1 - (x_1 y_2 + x_2 y_1) + 2 x_2 y_2$ is an inner product distinct from the dot product (proof left as an exercise).

### 3.2.3 Symmetric, Positive Definite Matrices
- For an $n$-dimensional vector space $V$ with basis $B = (\boldsymbol{b}_1, \ldots, \boldsymbol{b}_n)$, vectors $\boldsymbol{x} = \sum_{i=1}^n \psi_i \boldsymbol{b}_i$ and $\boldsymbol{y} = \sum_{j=1}^n \lambda_j \boldsymbol{b}_j$ have inner product:
  - $\langle \boldsymbol{x}, \boldsymbol{y} \rangle = \sum_{i=1}^n \sum_{j=1}^n \psi_i \langle \boldsymbol{b}_i, \boldsymbol{b}_j \rangle \lambda_j = \hat{\boldsymbol{x}}^\top \boldsymbol{A} \hat{\boldsymbol{y}}$, where $A_{ij} = \langle \boldsymbol{b}_i, \boldsymbol{b}_j \rangle$, $\hat{\boldsymbol{x}}, \hat{\boldsymbol{y}}$ are coordinate vectors.
- $\boldsymbol{A}$ is symmetric (due to symmetry of $\langle \cdot, \cdot \rangle$) and positive definite (since $\hat{\boldsymbol{x}}^\top \boldsymbol{A} \hat{\boldsymbol{x}} > 0$ for $\boldsymbol{x} \neq \mathbf{0}$).
- **Definition 3.4**: A matrix $\boldsymbol{A}$ is symmetric, positive definite if it is symmetric and $\boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x} > 0$ for all $\boldsymbol{x} \neq \mathbf{0}$. If only $\geq$ holds then A is called symmetric, positive semidefinite.

### Additional Insight
Inner products link algebra to geometry, with symmetric, positive definite matrices playing roles in matrix decompositions (Chapter 4) and kernel methods (Chapter 12).

---

## 3.3 Lengths and Distances
- Inner products induce norms via $||\boldsymbol{x}|| = \sqrt{\langle \boldsymbol{x}, \boldsymbol{x} \rangle}$, but not all norms (e.g., Manhattan) arise from inner products.
- **Cauchy-Schwarz Inequality**: $|\langle \boldsymbol{x}, \boldsymbol{y} \rangle| \leq ||\boldsymbol{x}|| ||\boldsymbol{y}||$, bounding the inner product by the product of norms.

### Example 3.5
- For $\boldsymbol{x} = [1, 1]^\top$:
  - Dot product: $||\boldsymbol{x}|| = \sqrt{\boldsymbol{x}^\top \boldsymbol{x}} = \sqrt{2}$.
  - Alternate inner product $\langle \boldsymbol{x}, \boldsymbol{y} \rangle = \boldsymbol{x}^\top \begin{bmatrix} 1 & -\frac{1}{2} \\ -\frac{1}{2} & 1 \end{bmatrix} \boldsymbol{y}$ yields $||\boldsymbol{x}|| = \sqrt{1} = 1$, showing dependence on the inner product.

### Definition 3.6
- **Distance**: $d(\boldsymbol{x}, \boldsymbol{y}) = ||\boldsymbol{x} - \boldsymbol{y}|| = \sqrt{\langle \boldsymbol{x} - \boldsymbol{y}, \boldsymbol{x} - \boldsymbol{y} \rangle}$, called Euclidean distance with the dot product.
- **Metric**: A mapping $d: V \times V \rightarrow \mathbb{R}$ satisfying:
  - Positive definiteness: $d(\boldsymbol{x}, \boldsymbol{y}) \geq 0$, $d(\boldsymbol{x}, \boldsymbol{y}) = 0$ if and only if $\boldsymbol{x} = \boldsymbol{y}$.
  - Symmetry: $d(\boldsymbol{x}, \boldsymbol{y}) = d(\boldsymbol{y}, \boldsymbol{x})$.
  - Triangle inequality: $d(\boldsymbol{x}, \boldsymbol{z}) \leq d(\boldsymbol{x}, \boldsymbol{y}) + d(\boldsymbol{y}, \boldsymbol{z})$.

### Remarks
- Distances can be defined with norms alone, but inner product-induced distances vary with the choice of inner product.
- Inner products and metrics differ: similar vectors yield large $\langle \boldsymbol{x}, \boldsymbol{y} \rangle$ but small $d(\boldsymbol{x}, \boldsymbol{y})$.

---

## 3.4 Angles and Orthogonality
- The Cauchy-Schwarz inequality ensures $-1 \leq \frac{\langle \boldsymbol{x}, \boldsymbol{y} \rangle}{||\boldsymbol{x}|| ||\boldsymbol{y}||} \leq 1$, allowing the angle $\omega$ between non-zero vectors $\boldsymbol{x}, \boldsymbol{y}$ to be defined as:
  - $\cos \omega = \frac{\langle \boldsymbol{x}, \boldsymbol{y} \rangle}{||\boldsymbol{x}|| ||\boldsymbol{y}||}$, where $\omega \in [0, \pi]$ (see Figure 3.4).

### Example 3.6
- For $\boldsymbol{x} = [1, 1]^\top$, $\boldsymbol{y} = [1, 2]^\top$ with dot product:
  - $\cos \omega = \frac{\boldsymbol{x}^\top \boldsymbol{y}}{\sqrt{\boldsymbol{x}^\top \boldsymbol{x} \boldsymbol{y}^\top \boldsymbol{y}}} = \frac{3}{\sqrt{10}}$, so $\omega \approx 0.32 \, \text{rad} \approx 18^\circ$ (see Figure 3.5).

### Definition 3.7
- **Orthogonality**: $\boldsymbol{x} \perp \boldsymbol{y}$ if $\langle \boldsymbol{x}, \boldsymbol{y} \rangle = 0$. If $||\boldsymbol{x}|| = 1 = ||\boldsymbol{y}||$, they are **orthonormal**.
- The zero vector is orthogonal to all vectors.

### Example 3.7
- For $\boldsymbol{x} = [1, 1]^\top$, $\boldsymbol{y} = [-1, 1]^\top$:
  - Dot product: $\boldsymbol{x}^\top \boldsymbol{y} = 0$, so $\omega = 90^\circ$, $\boldsymbol{x} \perp \boldsymbol{y}$.
  - Alternate inner product $\langle \boldsymbol{x}, \boldsymbol{y} \rangle = \boldsymbol{x}^\top \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} \boldsymbol{y}$ yields $\cos \omega = -\frac{1}{3}$, $\omega \approx 109.5^\circ$, not orthogonal (see Figure 3.6).

### Definition 3.8
- **Orthogonal Matrix**: A square matrix $\boldsymbol{A} \in \mathbb{R}^{n \times n}$ with orthonormal columns, satisfying $\boldsymbol{A} \boldsymbol{A}^\top = \boldsymbol{I} = \boldsymbol{A}^\top \boldsymbol{A}$, so $\boldsymbol{A}^{-1} = \boldsymbol{A}^\top$.
- Preserves lengths: $||\boldsymbol{A} \boldsymbol{x}||^2 = \boldsymbol{x}^\top \boldsymbol{x}$.
- Preserves angles: $\cos \omega = \frac{(\boldsymbol{A} \boldsymbol{x})^\top (\boldsymbol{A} \boldsymbol{y})}{||\boldsymbol{A} \boldsymbol{x}|| ||\boldsymbol{A} \boldsymbol{y}||} = \frac{\boldsymbol{x}^\top \boldsymbol{y}}{||\boldsymbol{x}|| ||\boldsymbol{y}||}$.
- Represents rotations (possibly with flips).

---

## 3.5 Orthonormal Basis
### Definition 3.9
- A basis $\{\boldsymbol{b}_1, \ldots, \boldsymbol{b}_n\}$ of an $n$-dimensional space $V$ is:
  - **Orthogonal** if $\langle \boldsymbol{b}_i, \boldsymbol{b}_j \rangle = 0$ for $i \neq j$.
  - **Orthonormal (ONB)** if additionally $\langle \boldsymbol{b}_i, \boldsymbol{b}_i \rangle = 1$ (i.e., $||\boldsymbol{b}_i|| = 1$).

### Construction
- Gaussian elimination (Section 2.6.1) can find a basis, but Gram-Schmidt prcoess (Section 3.8.3) orthogonalizes it.

---

## 3.6 Orthogonal Complement
- Consider a $ D $-dimensional vector space $ V $ and an $ M $-dimensional subspace $ U \subseteq V $. Then its **orthogonal complement** $ U^{\perp} $ is a $ (D - M) $-dimensional subspace of $ V $ and contains all vectors in $ V $ that are orthogonal to every vector in $ U $. Furthermore, $ U \cap U^{\perp} = \{0\} $ so that any vector $ x \in V $ can be uniquely decomposed.
- The orthogonal complement $U^\perp$ of a subspace $U \subseteq V$ is $\{ \boldsymbol{v} \in V \mid \langle \boldsymbol{v}, \boldsymbol{u} \rangle = 0 \text{ for all } \boldsymbol{u} \in U \}$.
- $V = U \oplus U^\perp$ (direct sum), and $\dim(U) + \dim(U^\perp) = \dim(V)$.

---

## 3.7 Inner Product of Functions
- For functions $u, v: \mathbb{R} \rightarrow \mathbb{R}$, an inner product is $\langle u, v \rangle = \int_a^b u(x) v(x) \, dx$, with finite $a, b$.
- Orthogonality occurs if $\langle u, v \rangle = 0$.

### Example 3.9
- For $u = \sin(x)$, $v = \cos(x)$ over $[-\pi, \pi]$, $\int_{-\pi}^\pi \sin(x) \cos(x) \, dx = 0$ (odd function, see Figure 3.8), so they are orthogonal.
- It also holds that the collection of functions $\{1, \cos(x), \cos(2x), \ldots\}$ is orthogonal if we integrate on $[-\pi, \pi]$ i.e., any pair of functions are orthogonal to each other. This collection of functions spans a large subspace of the functions that are even and periodic on $[-\pi, \pi]$, and projecting functions onto this subspace is the fundamental idea behind Fourier series.

---

## 3.8 Orthogonal Projections
### Purpose
Projections reduce dimensionality, minimizing information loss, used in PCA (Chapter 10), regression (Chapter 9), and classification (Chapter 12).

### Definition 3.10
- A linear mapping $\pi: V \rightarrow U$ (where $U \subseteq V$) is a projection if $\pi^2 = \pi$. Its matrix $\boldsymbol{P}_\pi$ satisfies $\boldsymbol{P}_\pi^2 = \boldsymbol{P}_\pi$.

### 3.8.1 Projection onto Lines
- For a line $U = \text{span}(\boldsymbol{b})$ in $\mathbb{R}^n$:
  1. **Coordinate**: $\lambda = \frac{\langle \boldsymbol{x}, \boldsymbol{b} \rangle}{||\boldsymbol{b}||^2}$ (dot product: $\lambda = \frac{\boldsymbol{b}^\top \boldsymbol{x}}{||\boldsymbol{b}||^2}$).
  2. **Projection**: $\pi_U(\boldsymbol{x}) = \lambda \boldsymbol{b} = \frac{\langle \boldsymbol{x}, \boldsymbol{b} \rangle}{||\boldsymbol{b}||^2} \boldsymbol{b}$ (dot product: $\frac{\boldsymbol{b}^\top \boldsymbol{x}}{||\boldsymbol{b}||^2} \boldsymbol{b}$).
  3. **Projection Matrix**: $\boldsymbol{P}_\pi = \frac{\boldsymbol{b} \boldsymbol{b}^\top}{||\boldsymbol{b}||^2}$ (rank-1, symmetric).

- Length: $||\pi_U(\boldsymbol{x})|| = |\lambda| ||\boldsymbol{b}|| = |\cos \omega| ||\boldsymbol{x}||$ (dot product, $\omega$ is angle between $\boldsymbol{x}$ and $\boldsymbol{b}$).

### Example 3.10
- For $\boldsymbol{b} = [1, 2, 2]^\top$:
  - $\boldsymbol{P}_\pi = \frac{1}{9} \begin{bmatrix} 1 & 2 & 2 \\ 2 & 4 & 4 \\ 2 & 4 & 4 \end{bmatrix}$.
  - For $\boldsymbol{x} = [1, 1, 1]^\top$, $\pi_U(\boldsymbol{x}) = \frac{1}{9} [5, 10, 10]^\top$, in $\text{span}(\boldsymbol{b})$.

### 3.8.2 Projection onto General Subspaces
- For $U \subseteq \mathbb{R}^n$, $\dim(U) = m$, basis $\boldsymbol{b}_1, \ldots, \boldsymbol{b}_m$, $\boldsymbol{B} = [\boldsymbol{b}_1, \ldots, \boldsymbol{b}_m]$:
  1. **Coordinates**: $\boldsymbol{\lambda} = (\boldsymbol{B}^\top \boldsymbol{B})^{-1} \boldsymbol{B}^\top \boldsymbol{x}$ (normal equation $\boldsymbol{B}^\top \boldsymbol{B} \boldsymbol{\lambda} = \boldsymbol{B}^\top \boldsymbol{x}$).
  2. **Projection**: $\pi_U(\boldsymbol{x}) = \boldsymbol{B} \boldsymbol{\lambda} = \boldsymbol{B} (\boldsymbol{B}^\top \boldsymbol{B})^{-1} \boldsymbol{B}^\top \boldsymbol{x}$.
  3. **Projection Matrix**: $\boldsymbol{P}_\pi = \boldsymbol{B} (\boldsymbol{B}^\top \boldsymbol{B})^{-1} \boldsymbol{B}^\top$.

- **Pseudo-inverse**: $(\boldsymbol{B}^\top \boldsymbol{B})^{-1} \boldsymbol{B}^\top$, valid if $\boldsymbol{B}$ is full rank.

### 3.8.3 Gram-Schmidt Orthogonalization
- Transforms basis $(\boldsymbol{b}_1, \ldots, \boldsymbol{b}_n)$ to orthogonal basis $(\boldsymbol{u}_1, \ldots, \boldsymbol{u}_n)$:
  - $\boldsymbol{u}_1 = \boldsymbol{b}_1$
  - $\boldsymbol{u}_k = \boldsymbol{b}_k - \pi_{\text{span}(\boldsymbol{u}_1, \ldots, \boldsymbol{u}_{k-1})}(\boldsymbol{b}_k)$, $k = 2, \ldots, n$.
- Normalize for ONB.

### Example 3.12
- For $\boldsymbol{b}_1 = [2, 0]^\top$, $\boldsymbol{b}_2 = [1, 1]^\top$:
  - $\boldsymbol{u}_1 = [2, 0]^\top$, $\boldsymbol{u}_2 = [0, 1]^\top$ (orthogonal, see Figure 3.12).

---

## 3.9 Rotations
### Overview
Rotations are orthogonal linear mappings preserving lengths and angles, critical in robotics and graphics (Figures 3.14, 3.15).

### 3.9.1 Rotations in $\mathbb{R}^2$
- Rotation by angle $\theta$ (counterclockwise for $\theta > 0$):
  - $\boldsymbol{R}(\theta) = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}$ (see Figure 3.16).

### 3.9.2 Rotations in $\mathbb{R}^3$
- About standard axes:
  - $e_1$-axis: $\boldsymbol{R}_1(\theta) = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos \theta & -\sin \theta \\ 0 & \sin \theta & \cos \theta \end{bmatrix}$.
  - $e_2$-axis: $\boldsymbol{R}_2(\theta) = \begin{bmatrix} \cos \theta & 0 & \sin \theta \\ 0 & 1 & 0 \\ -\sin \theta & 0 & \cos \theta \end{bmatrix}$.
  - $e_3$-axis: $\boldsymbol{R}_3(\theta) = \begin{bmatrix} \cos \theta & -\sin \theta & 0 \\ \sin \theta & \cos \theta & 0 \\ 0 & 0 & 1 \end{bmatrix}$ (see Figure 3.17).

### 3.9.3 Rotations in $n$ Dimensions
- **Givens Rotation**: $\boldsymbol{R}_{ij}(\theta)$ rotates in the $i$-$j$ plane, identity elsewhere with $r_{ii} = \cos \theta$, $r_{ij} = -\sin \theta$, $r_{ji} = \sin \theta$, $r_{jj} = \cos \theta$.

### 3.9.4 Properties
- Preserve distances and angles.
- Non-commutative in 3D+; commutative in 2D about the same point.

---