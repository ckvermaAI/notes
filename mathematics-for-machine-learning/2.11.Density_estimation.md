# Density Estimation with Gaussian Mixture Models

## Overview
This chapter, part of *Mathematics for Machine Learning* by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (Cambridge University Press, 2020), explores density estimation as a fundamental pillar of machine learning, alongside regression and dimensionality reduction (covered in Chapters 9 and 10, respectively). Density estimation aims to compactly represent data using a parametric density, such as a Gaussian or Beta distribution, rather than relying solely on raw data points. The chapter introduces Gaussian Mixture Models (GMMs) as a powerful tool for this task, leveraging the Expectation Maximization (EM) algorithm and a latent variable perspective to handle complex, multimodal data distributions.

---

## 11.1 Gaussian Mixture Model

### Introduction to Mixture Models
- **Purpose**: Mixture models enhance density estimation by representing a distribution $p(\boldsymbol{x})$ as a convex combination of $K$ simpler base distributions, offering greater expressiveness than single distributions like Gaussians.
- **Formulation**: The mixture model is defined as:
  $p(\boldsymbol{x}) = \sum_{k=1}^K \pi_k p_k(\boldsymbol{x})$,
  where $0 \leq \pi_k \leq 1$ and $\sum_{k=1}^K \pi_k = 1$. Here, $p_k(\boldsymbol{x})$ are base distributions (e.g., Gaussians), and $\pi_k$ are mixture weights.
- **Advantage**: Unlike a single Gaussian, mixture models can capture multimodal data, as illustrated by a two-dimensional dataset in Figure 11.1 that a single Gaussian fails to represent meaningfully.

### Gaussian Mixture Models (GMMs)
- **Definition**: A GMM combines $K$ Gaussian distributions:
  $p(\boldsymbol{x} \mid \boldsymbol{\theta}) = \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$,
  where $\boldsymbol{\theta} = \{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k, \pi_k : k=1, \ldots, K\}$ includes means $\boldsymbol{\mu}_k$, covariance matrices $\boldsymbol{\Sigma}_k$, and weights $\pi_k$, with constraints $0 \leq \pi_k \leq 1$ and $\sum_{k=1}^K \pi_k = 1$.
- **Flexibility**: GMMs extend beyond single Gaussians (recovered when $K=1$), as shown in Figure 11.2, where a mixture density (black) is more expressive than its weighted Gaussian components (dashed lines).
- **Example**: A sample GMM density is given by:
  $p(x \mid \boldsymbol{\theta}) = 0.5 \mathcal{N}(x \mid -2, \frac{1}{2}) + 0.2 \mathcal{N}(x \mid 1, 2) + 0.3 \mathcal{N}(x \mid 4, 1)$.

### Training GMMs
- **Objective**: For a dataset, maximize the likelihood of $\boldsymbol{\theta}$ using techniques from Chapters 5, 6, and Section 7.2.
- **Challenge**: Unlike linear regression or PCA, no closed-form maximum likelihood solution exists; instead, iterative solutions are required due to interdependent equations.

---

## 11.2 Parameter Learning via Maximum Likelihood

### Problem Setup
- **Dataset**: Consider $\mathcal{X} = \{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N\}$ drawn i.i.d. from an unknown $p(\boldsymbol{x})$. The goal is to approximate $p(\boldsymbol{x})$ with a GMM of $K$ components, parameterized by $\boldsymbol{\theta} = \{\pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k : k=1, \ldots, K\}$.
- **Example 11.1**: A one-dimensional dataset $\mathcal{X} = \{-3, -2.5, -1, 0, 2, 4, 5\}$ with $K=3$ components initialized as:
  - $p_1(x) = \mathcal{N}(x \mid -4, 1)$,
  - $p_2(x) = \mathcal{N}(x \mid 0, 0.2)$,
  - $p_3(x) = \mathcal{N}(x \mid 8, 3)$,
  with equal weights $\pi_1 = \pi_2 = \pi_3 = \frac{1}{3}$ (see Figure 11.3).

### Likelihood and Log-Likelihood
- **Likelihood**: Assuming i.i.d. data:
  $p(\mathcal{X} \mid \boldsymbol{\theta}) = \prod_{n=1}^N p(\boldsymbol{x}_n \mid \boldsymbol{\theta})$, where $p(\boldsymbol{x}_n \mid \boldsymbol{\theta}) = \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$.
- **Log-Likelihood**: 
  $\log p(\mathcal{X} \mid \boldsymbol{\theta}) = \sum_{n=1}^N \log p(\boldsymbol{x}_n \mid \boldsymbol{\theta}) = \sum_{n=1}^N \log \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$, denoted as $\mathcal{L}$.
- **Optimization**: Maximize $\mathcal{L}$ to find $\boldsymbol{\theta}_{\text{ML}}^*$. Gradient-based methods fail to yield a closed-form solution due to the log-sum structure, unlike simpler cases (e.g., single Gaussian in Chapter 8).

### Responsibilities
- **Definition**: The responsibility of the $k$-th component for the $n$-th data point is:
  $r_{n k} = \frac{\pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$.
- **Interpretation**: $r_{n k}$ measures how likely the $k$-th component generated $\boldsymbol{x}_n$, forming a probability vector $\boldsymbol{r}_n = [r_{n1}, \ldots, r_{nK}]^\top$ with $\sum_k r_{n k} = 1$.
- **Example 11.2**: For the dataset in Example 11.1, responsibilities form a matrix:
  $\begin{bmatrix} 1.0 & 0.0 & 0.0 \\ 1.0 & 0.0 & 0.0 \\ 0.057 & 0.943 & 0.0 \\ 0.001 & 0.999 & 0.0 \\ 0.0 & 0.066 & 0.934 \\ 0.0 & 0.0 & 1.0 \\ 0.0 & 0.0 & 1.0 \end{bmatrix}$,
  with total responsibilities $N_1 = 2.058$, $N_2 = 2.008$, $N_3 = 2.934$.

### Parameter Updates
- **Iterative Approach**: Update one parameter at a time (means, covariances, weights) while fixing others, using responsibilities, then recompute $r_{n k}$. This is the EM algorithm (detailed in Section 11.3).

#### 11.2.2 Updating the Means
- **Theorem 11.1**: Mean update:
  $\boldsymbol{\mu}_k^{\text{new}} = \frac{\sum_{n=1}^N r_{n k} \boldsymbol{x}_n}{\sum_{n=1}^N r_{n k}} = \frac{1}{N_k} \sum_{n=1}^N r_{n k} \boldsymbol{x}_n$, where $N_k = \sum_{n=1}^N r_{n k}$.
- **Proof**: Gradient $\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_k} = \sum_{n=1}^N r_{n k} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} = \mathbf{0}^\top$ yields the weighted average.
- **Intuition**: $\boldsymbol{\mu}_k$ is an importance-weighted mean, pulled toward data points with high $r_{n k}$ (see Figure 11.4).
- **Example 11.3**: Updates shift means from $-4 \to -2.7$, $0 \to -0.4$, $8 \to 3.7$ (Figure 11.5).

#### 11.2.3 Updating the Covariances
- **Theorem 11.2**: Covariance update:
  $\boldsymbol{\Sigma}_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N r_{n k} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^\top$.
- **Proof**: Gradient $\frac{\partial \mathcal{L}}{\partial \boldsymbol{\Sigma}_k} = -\frac{1}{2} \sum_{n=1}^N r_{n k} \left( \boldsymbol{\Sigma}_k^{-1} - \boldsymbol{\Sigma}_k^{-1} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} \right) = \mathbf{0}$ simplifies to the weighted covariance.
- **Example 11.4**: Variances update from $1 \to 0.14$, $0.2 \to 0.44$, $3 \to 1.53$ (Figure 11.6).

#### 11.2.4 Updating the Mixture Weights
- **Theorem 11.3**: Weight update:
  $\pi_k^{\text{new}} = \frac{N_k}{N}$.
- **Proof**: Using Lagrange multipliers for $\sum_k \pi_k = 1$, $\frac{\partial \mathfrak{L}}{\partial \pi_k} = \frac{N_k}{\pi_k} + \lambda = 0$ and $\sum_k \pi_k = 1$ yield $\pi_k = \frac{N_k}{N}$.
- **Example 11.5**: Weights update from $\frac{1}{3}$ to $0.29$, $0.29$, $0.42$ (Figure 11.7).
- **Outcome**: After one update cycle, log-likelihood improves from -28.3 to -14.4.

---

## 11.3 EM Algorithm

### Overview
- **Purpose**: The EM algorithm (Dempster et al., 1977) iteratively estimates GMM parameters since closed-form solutions are infeasible due to $r_{n k}$ dependencies.
- **Steps**:
  1. **Initialize**: Set initial $\boldsymbol{\mu}_k$, $\boldsymbol{\Sigma}_k$, $\pi_k$.
  2. **E-Step**: Compute responsibilities:
     $r_{n k} = \frac{\pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_j \pi_j \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$.
  3. **M-Step**: Update parameters:
     - $\boldsymbol{\mu}_k = \frac{1}{N_k} \sum_{n=1}^N r_{n k} \boldsymbol{x}_n$,
     - $\boldsymbol{\Sigma}_k = \frac{1}{N_k} \sum_{n=1}^N r_{n k} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^\top$,
     - $\pi_k = \frac{N_k}{N}$.
  4. **Repeat**: Until convergence (e.g., log-likelihood stabilizes).
- **Property**: Each iteration increases $\mathcal{L}$ (Neal and Hinton, 1999).

### Example 11.6: GMM Fit
- **Result**: After 5 iterations on Example 11.1, the GMM becomes:
  $p(x) = 0.29 \mathcal{N}(x \mid -2.75, 0.06) + 0.28 \mathcal{N}(x \mid -0.50, 0.25) + 0.43 \mathcal{N}(x \mid 3.64, 1.63)$ (Figure 11.8).
- **2D Example**: For the dataset in Figure 11.1, EM with $K=3$ converges after 62 iterations (Figures 11.9, 11.10).

---

## 11.4 Latent-Variable Perspective

### 11.4.1 Generative Process and Probabilistic Model
- **Latent Variable**: Introduce $z = [z_1, \ldots, z_K]^\top$, a one-hot vector where $z_k = 1$ if the $k$-th component generates $\boldsymbol{x}$.
- **Conditional**: $p(\boldsymbol{x} \mid z_k = 1) = \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$.
- **Prior**: $p(z) = \pi = [\pi_1, . . . , \pi_K]\top$, with $p(z_k = 1) = \pi_k$.
- **Sampling**: Sample $z^i \sim p(z)$ then sample $x^i \sim p(\mathbb{x} | z^i = 1)$. In the first step, we select a mixture component $i$ (via the one-hot encoding $z$) at random according to $p(z) = \pi$; in the second step we draw a sample from the corresponding mixture component. When we discard the samples of the latent variable so that we are left with the $x(i)$, we have valid samples from the GMM. This kind of sampling, where samples of random variables depend on samples from the variableâ€™s parents in the ancestral sampling graphical model, is called ancestral sampling.

### 11.4.2 Likelihood
- **Marginalization**: 
  $p(\boldsymbol{x} \mid \boldsymbol{\theta}) = \sum_{k=1}^K p(\boldsymbol{x} \mid \boldsymbol{\theta}, z_k = 1) p(z_k = 1 \mid \boldsymbol{\theta}) = \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$,
  matching the GMM form.

### 11.4.3 Posterior Distribution
- **Posterior**: 
  $p(z_k = 1 \mid \boldsymbol{x}) = \frac{\pi_k \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_j \pi_j \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)} = r_{n k}$,
  confirming responsibilities as posterior probabilities.

### 11.4.4 Extension to Full Dataset
- **Multiple Latent Variables**: Each $\boldsymbol{x}_n$ has a $z_n$, with shared prior $\pi$ (Figure 11.12).
- **Posterior**: $p(z_{n k} = 1 \mid \boldsymbol{x}_n) = r_{n k}$.

### 11.4.5 EM Algorithm Revisited
- **E-Step**: Compute expected log-likelihood $Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) = \mathbb{E}_{z \mid \boldsymbol{x}, \boldsymbol{\theta}^{(t)}}[\log p(\boldsymbol{x}, z \mid \boldsymbol{\theta})]$ using current posteriors.
- **M-Step**: Maximize $Q$ to update $\boldsymbol{\theta}^{(t+1)}$.
- **Caveat**: EM may converge to local optima; multiple initializations help.

---

## 11.5 Further Reading

### GMM Properties
- **Generative**: GMMs enable data generation via ancestral sampling.
- **K Selection**: Nested cross-validation (Section 8.6.1) can determine $K$.
- **Relation to K-Means**: GMMs extend K-means with soft assignments via $r_{n k}$.

### Limitations of Maximum Likelihood
- **Overfitting**: Likelihood can spike if a componentâ€™s mean aligns with a data point and $\boldsymbol{\Sigma}_k \to 0$.
- **Point Estimates**: No uncertainty quantification; Bayesian methods with priors offer posteriors but lack closed-form solutions (variational inference as an alternative).

### Alternative Density Estimation Methods
- **Histograms**: Nonparametric, bin-based counts (Pearson, 1895); bin size is critical.
- **Kernel Density Estimation (KDE)**: Nonparametric, smooth estimates via:
  $p(\boldsymbol{x}) = \frac{1}{N h} \sum_{n=1}^N k\left(\frac{\boldsymbol{x} - \boldsymbol{x}_n}{h}\right)$ (Rosenblatt, 1956; Parzen, 1962), with bandwidth $h$ and kernel $k$ (e.g., Gaussian). Figure 11.13 contrasts histograms and KDE.

---

## Additional Context
- **Applications**: GMMs are widely used in clustering, anomaly detection, and generative modeling (e.g., speech recognition, image segmentation).
- **Computational Note**: EMâ€™s iterative nature requires careful initialization (e.g., K-means++ seeding) and convergence checks (e.g., log-likelihood thresholds or parameter stability).
- **Extensions**: Beyond GMMs, mixture models can incorporate other distributions (e.g., exponential families), and advanced techniques like Dirichlet Process GMMs handle unknown $K$.