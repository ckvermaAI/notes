# Density Estimation with Gaussian Mixture Models

## Overview
This chapter, part of *Mathematics for Machine Learning* by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (Cambridge University Press, 2020), explores density estimation as a fundamental pillar of machine learning, alongside regression and dimensionality reduction (covered in Chapters 9 and 10, respectively). Density estimation aims to compactly represent data using a parametric density, such as a Gaussian or Beta distribution, rather than relying solely on raw data points. The chapter introduces Gaussian Mixture Models (GMMs) as a powerful tool for this task, leveraging the Expectation Maximization (EM) algorithm and a latent variable perspective to handle complex, multimodal data distributions.

---

## 11.1 Gaussian Mixture Model

### Introduction to Mixture Models
- **Purpose**: Mixture models enhance density estimation by representing a distribution $p(\boldsymbol{x})$ as a convex combination of $K$ simpler base distributions, offering greater expressiveness than single distributions like Gaussians.
- **Formulation**: The mixture model is defined as:
  $p(\boldsymbol{x}) = \sum_{k=1}^K \pi_k p_k(\boldsymbol{x})$,
  where $0 \leq \pi_k \leq 1$ and $\sum_{k=1}^K \pi_k = 1$. Here, $p_k(\boldsymbol{x})$ are base distributions (e.g., Gaussians), and $\pi_k$ are mixture weights.
- **Advantage**: Unlike a single Gaussian, mixture models can capture multimodal data, as illustrated by a two-dimensional dataset in Figure 11.1 that a single Gaussian fails to represent meaningfully.

### Gaussian Mixture Models (GMMs)
- **Definition**: A GMM combines $K$ Gaussian distributions:
  $p(\boldsymbol{x} \mid \boldsymbol{\theta}) = \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$,
  where $\boldsymbol{\theta} = \{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k, \pi_k : k=1, \ldots, K\}$ includes means $\boldsymbol{\mu}_k$, covariance matrices $\boldsymbol{\Sigma}_k$, and weights $\pi_k$, with constraints $0 \leq \pi_k \leq 1$ and $\sum_{k=1}^K \pi_k = 1$.
- **Flexibility**: GMMs extend beyond single Gaussians (recovered when $K=1$), as shown in Figure 11.2, where a mixture density (black) is more expressive than its weighted Gaussian components (dashed lines).
- **Example**: A sample GMM density is given by:
  $p(x \mid \boldsymbol{\theta}) = 0.5 \mathcal{N}(x \mid -2, \frac{1}{2}) + 0.2 \mathcal{N}(x \mid 1, 2) + 0.3 \mathcal{N}(x \mid 4, 1)$.

### Training GMMs
- **Objective**: For a dataset, maximize the likelihood of $\boldsymbol{\theta}$ using techniques from Chapters 5, 6, and Section 7.2.
- **Challenge**: Unlike linear regression or PCA, no closed-form maximum likelihood solution exists; instead, iterative solutions are required due to interdependent equations.

---

## 11.2 Parameter Learning via Maximum Likelihood

### Problem Setup
- **Dataset**: Consider $\mathcal{X} = \{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N\}$ drawn i.i.d. from an unknown $p(\boldsymbol{x})$. The goal is to approximate $p(\boldsymbol{x})$ with a GMM of $K$ components, parameterized by $\boldsymbol{\theta} = \{\pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k : k=1, \ldots, K\}$.
- **Example 11.1**: A one-dimensional dataset $\mathcal{X} = \{-3, -2.5, -1, 0, 2, 4, 5\}$ with $K=3$ components initialized as:
  - $p_1(x) = \mathcal{N}(x \mid -4, 1)$,
  - $p_2(x) = \mathcal{N}(x \mid 0, 0.2)$,
  - $p_3(x) = \mathcal{N}(x \mid 8, 3)$,
  with equal weights $\pi_1 = \pi_2 = \pi_3 = \frac{1}{3}$ (see Figure 11.3).

### Likelihood and Log-Likelihood
- **Likelihood**: Assuming i.i.d. data:
  $p(\mathcal{X} \mid \boldsymbol{\theta}) = \prod_{n=1}^N p(\boldsymbol{x}_n \mid \boldsymbol{\theta})$, where $p(\boldsymbol{x}_n \mid \boldsymbol{\theta}) = \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$.
- **Log-Likelihood**: 
  $\log p(\mathcal{X} \mid \boldsymbol{\theta}) = \sum_{n=1}^N \log p(\boldsymbol{x}_n \mid \boldsymbol{\theta}) = \sum_{n=1}^N \log \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$, denoted as $\mathcal{L}$.
- **Optimization**: Maximize $\mathcal{L}$ to find $\boldsymbol{\theta}_{\text{ML}}^*$. Gradient-based methods fail to yield a closed-form solution due to the log-sum structure, unlike simpler cases (e.g., single Gaussian in Chapter 8).

### Responsibilities
- **Definition**: The responsibility of the $k$-th component for the $n$-th data point is:
  $r_{n k} = \frac{\pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$.
- **Interpretation**: $r_{n k}$ measures how likely the $k$-th component generated $\boldsymbol{x}_n$, forming a probability vector $\boldsymbol{r}_n = [r_{n1}, \ldots, r_{nK}]^\top$ with $\sum_k r_{n k} = 1$.
- **Example 11.2**: For the dataset in Example 11.1, responsibilities form a matrix:
  $\begin{bmatrix} 1.0 & 0.0 & 0.0 \\ 1.0 & 0.0 & 0.0 \\ 0.057 & 0.943 & 0.0 \\ 0.001 & 0.999 & 0.0 \\ 0.0 & 0.066 & 0.934 \\ 0.0 & 0.0 & 1.0 \\ 0.0 & 0.0 & 1.0 \end{bmatrix}$,
  with total responsibilities $N_1 = 2.058$, $N_2 = 2.008$, $N_3 = 2.934$.

### Parameter Updates
- **Iterative Approach**: Update one parameter at a time (means, covariances, weights) while fixing others, using responsibilities, then recompute $r_{n k}$. This is the EM algorithm (detailed in Section 11.3).

#### 11.2.2 Updating the Means
- **Theorem 11.1**: Mean update:
  $\boldsymbol{\mu}_k^{\text{new}} = \frac{\sum_{n=1}^N r_{n k} \boldsymbol{x}_n}{\sum_{n=1}^N r_{n k}} = \frac{1}{N_k} \sum_{n=1}^N r_{n k} \boldsymbol{x}_n$, where $N_k = \sum_{n=1}^N r_{n k}$.
- **Proof**: Gradient $\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_k} = \sum_{n=1}^N r_{n k} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} = \mathbf{0}^\top$ yields the weighted average.
- **Intuition**: $\boldsymbol{\mu}_k$ is an importance-weighted mean, pulled toward data points with high $r_{n k}$ (see Figure 11.4).
- **Example 11.3**: Updates shift means from $-4 \to -2.7$, $0 \to -0.4$, $8 \to 3.7$ (Figure 11.5).

#### 11.2.3 Updating the Covariances
- **Theorem 11.2**: Covariance update:
  $\boldsymbol{\Sigma}_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N r_{n k} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^\top$.
- **Proof**: Gradient $\frac{\partial \mathcal{L}}{\partial \boldsymbol{\Sigma}_k} = -\frac{1}{2} \sum_{n=1}^N r_{n k} \left( \boldsymbol{\Sigma}_k^{-1} - \boldsymbol{\Sigma}_k^{-1} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} \right) = \mathbf{0}$ simplifies to the weighted covariance.
- **Example 11.4**: Variances update from $1 \to 0.14$, $0.2 \to 0.44$, $3 \to 1.53$ (Figure 11.6).

#### 11.2.4 Updating the Mixture Weights
- **Theorem 11.3**: Weight update:
  $\pi_k^{\text{new}} = \frac{N_k}{N}$.
- **Proof**: Using Lagrange multipliers for $\sum_k \pi_k = 1$, $\frac{\partial \mathfrak{L}}{\partial \pi_k} = \frac{N_k}{\pi_k} + \lambda = 0$ and $\sum_k \pi_k = 1$ yield $\pi_k = \frac{N_k}{N}$.
- **Example 11.5**: Weights update from $\frac{1}{3}$ to $0.29$, $0.29$, $0.42$ (Figure 11.7).
- **Outcome**: After one update cycle, log-likelihood improves from -28.3 to -14.4.

---

## 11.3 EM Algorithm

### Overview
- **Purpose**: The EM algorithm (Dempster et al., 1977) iteratively estimates GMM parameters since closed-form solutions are infeasible due to $r_{n k}$ dependencies.
- **Steps**:
  1. **Initialize**: Set initial $\boldsymbol{\mu}_k$, $\boldsymbol{\Sigma}_k$, $\pi_k$.
  2. **E-Step**: Compute responsibilities:
     $r_{n k} = \frac{\pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_j \pi_j \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$.
  3. **M-Step**: Update parameters:
     - $\boldsymbol{\mu}_k = \frac{1}{N_k} \sum_{n=1}^N r_{n k} \boldsymbol{x}_n$,
     - $\boldsymbol{\Sigma}_k = \frac{1}{N_k} \sum_{n=1}^N r_{n k} (\boldsymbol{x}_n - \boldsymbol{\mu}_k)(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^\top$,
     - $\pi_k = \frac{N_k}{N}$.
  4. **Repeat**: Until convergence (e.g., log-likelihood stabilizes).
- **Property**: Each iteration increases $\mathcal{L}$ (Neal and Hinton, 1999).

### Example 11.6: GMM Fit
- **Result**: After 5 iterations on Example 11.1, the GMM becomes:
  $p(x) = 0.29 \mathcal{N}(x \mid -2.75, 0.06) + 0.28 \mathcal{N}(x \mid -0.50, 0.25) + 0.43 \mathcal{N}(x \mid 3.64, 1.63)$ (Figure 11.8).
- **2D Example**: For the dataset in Figure 11.1, EM with $K=3$ converges after 62 iterations (Figures 11.9, 11.10).

---

## 11.4 Latent-Variable Perspective

### 11.4.1 Generative Process and Probabilistic Model
- **Latent Variable**: Introduce $z = [z_1, \ldots, z_K]^\top$, a one-hot vector where $z_k = 1$ if the $k$-th component generates $\boldsymbol{x}$.
- **Conditional**: $p(\boldsymbol{x} \mid z_k = 1) = \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$.
- **Prior**: $p(z) = \pi = [\pi_1, . . . , \pi_K]\top$, with $p(z_k = 1) = \pi_k$.
- **Sampling**: Sample $z^i \sim p(z)$ then sample $x^i \sim p(\mathbb{x} | z^i = 1)$. In the first step, we select a mixture component $i$ (via the one-hot encoding $z$) at random according to $p(z) = \pi$; in the second step we draw a sample from the corresponding mixture component. When we discard the samples of the latent variable so that we are left with the $x(i)$, we have valid samples from the GMM. This kind of sampling, where samples of random variables depend on samples from the variable’s parents in the ancestral sampling graphical model, is called ancestral sampling.

### 11.4.2 Likelihood
- **Marginalization**: 
  $p(\boldsymbol{x} \mid \boldsymbol{\theta}) = \sum_{k=1}^K p(\boldsymbol{x} \mid \boldsymbol{\theta}, z_k = 1) p(z_k = 1 \mid \boldsymbol{\theta}) = \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$,
  matching the GMM form.

### 11.4.3 Posterior Distribution
- **Posterior**: 
  $p(z_k = 1 \mid \boldsymbol{x}) = \frac{\pi_k \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_j \pi_j \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)} = r_{n k}$,
  confirming responsibilities as posterior probabilities.

### 11.4.4 Extension to Full Dataset
- **Multiple Latent Variables**: Each $\boldsymbol{x}_n$ has a $z_n$, with shared prior $\pi$ (Figure 11.12).
- **Posterior**: $p(z_{n k} = 1 \mid \boldsymbol{x}_n) = r_{n k}$.

### 11.4.5 EM Algorithm Revisited
- **E-Step**: Compute expected log-likelihood $Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) = \mathbb{E}_{z \mid \boldsymbol{x}, \boldsymbol{\theta}^{(t)}}[\log p(\boldsymbol{x}, z \mid \boldsymbol{\theta})]$ using current posteriors.
- **M-Step**: Maximize $Q$ to update $\boldsymbol{\theta}^{(t+1)}$.
- **Caveat**: EM may converge to local optima; multiple initializations help.

---

## 11.5 Further Reading

### GMM Properties
- **Generative**: GMMs enable data generation via ancestral sampling.
- **K Selection**: Nested cross-validation (Section 8.6.1) can determine $K$.
- **Relation to K-Means**: GMMs extend K-means with soft assignments via $r_{n k}$.

### Limitations of Maximum Likelihood
- **Overfitting**: Likelihood can spike if a component’s mean aligns with a data point and $\boldsymbol{\Sigma}_k \to 0$.
- **Point Estimates**: No uncertainty quantification; Bayesian methods with priors offer posteriors but lack closed-form solutions (variational inference as an alternative).

### Alternative Density Estimation Methods
- **Histograms**: Nonparametric, bin-based counts (Pearson, 1895); bin size is critical.
- **Kernel Density Estimation (KDE)**: Nonparametric, smooth estimates via:
  $p(\boldsymbol{x}) = \frac{1}{N h} \sum_{n=1}^N k\left(\frac{\boldsymbol{x} - \boldsymbol{x}_n}{h}\right)$ (Rosenblatt, 1956; Parzen, 1962), with bandwidth $h$ and kernel $k$ (e.g., Gaussian). Figure 11.13 contrasts histograms and KDE.

---

## Additional Context
- **Applications**: GMMs are widely used in clustering, anomaly detection, and generative modeling (e.g., speech recognition, image segmentation).
- **Computational Note**: EM’s iterative nature requires careful initialization (e.g., K-means++ seeding) and convergence checks (e.g., log-likelihood thresholds or parameter stability).
- **Extensions**: Beyond GMMs, mixture models can incorporate other distributions (e.g., exponential families), and advanced techniques like Dirichlet Process GMMs handle unknown $K$.