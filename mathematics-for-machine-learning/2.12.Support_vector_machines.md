# Classification with Support Vector Machines

## Introduction to Binary Classification

The chapter focuses on binary classification, a supervised machine learning task where the goal is to predict one of two discrete outcomes, such as labeling emails as "personal" or "junk," or classifying celestial objects as "galaxy" or "star." Unlike regression (covered in Chapter 9), which predicts continuous values, binary classification deals with outputs that are categorical and binary. The chapter denotes these outcomes as ${+1}$ and ${-1}$, representing positive and negative classes, respectively. These labels are arbitrary and do not imply inherent positivity or negativity (e.g., in cancer detection, ${+1}$ might represent a cancerous case).

The predictor function is defined as ${f: \mathbb{R}^D \rightarrow \{+1, -1\}}$, where ${\mathbb{R}^D}$ is the feature space of ${D}$ real-valued features representing each data point ${x_n}$. The training dataset consists of ${N}$ example-label pairs ${\{(x_1, y_1), \ldots, (x_N, y_N)\}}$, where ${y_n \in \{+1, -1\}}$ is the corresponding label. The chapter introduces Support Vector Machines (SVMs) as a powerful method for binary classification, offering strong theoretical guarantees and practical effectiveness (Steinwart and Christmann, 2008).

SVMs are chosen to illustrate binary classification due to two key advantages:
1. **Geometric Intuition**: Unlike probabilistic models (e.g., maximum likelihood estimation in Chapter 9), SVMs provide a geometric perspective, leveraging concepts like inner products and projections from Chapter 3.
2. **Optimization Complexity**: Unlike regression, SVM optimization lacks an analytical solution, requiring numerical optimization techniques from Chapter 7, making it a rich example for learning.

---

## 12.1 Separating Hyperplanes

SVMs classify data by finding a hyperplane that separates the two classes in the feature space. A hyperplane in ${\mathbb{R}^D}$ is an affine subspace of dimension ${D-1}$, defined by the equation ${\langle w, x \rangle + b = 0}$, where:
- ${w \in \mathbb{R}^D}$ is the normal vector to the hyperplane.
- ${b \in \mathbb{R}}$ is the intercept, determining the offset from the origin.
- ${\langle w, x \rangle}$ is the inner product between ${w}$ and an example ${x}$.

The classification function is ${f(x) = \langle w, x \rangle + b}$, where:
- If ${f(x) \geq 0}$, the example is classified as ${+1}$ (positive side).
- If ${f(x) < 0}$, it is classified as ${-1}$ (negative side).

Geometrically, ${w}$ is orthogonal to the hyperplane, as proven by showing that for any two points ${x_a}$ and ${x_b}$ on the hyperplane, ${\langle w, x_a - x_b \rangle = 0}$. The training objective ensures:
- Positive examples (${y_n = +1}$) satisfy ${\langle w, x_n \rangle + b \geq 0}$.
- Negative examples (${y_n = -1}$) satisfy ${\langle w, x_n \rangle + b < 0}$.

These conditions are combined into a single inequality: ${y_n (\langle w, x_n \rangle + b) \geq 0}$. Figure 12.2 illustrates this setup, with ${w}$ as the normal vector and the hyperplane dividing the space into positive and negative regions.

---

## 12.2 Primal Support Vector Machine

### 12.2.1 Concept of the Margin

For linearly separable data, many hyperplanes can separate the classes (see Figure 12.3). SVMs select the hyperplane that maximizes the **margin**, defined as the distance from the hyperplane to the nearest example. A larger margin improves generalization (Steinwart and Christmann, 2008).

To compute the margin, consider an example ${x_a}$ on the positive side (${\langle w, x_a \rangle + b > 0}$) and its orthogonal projection ${x_a'}$ onto the hyperplane (${\langle w, x_a' \rangle + b = 0}$). The distance ${r}$ is along the direction of ${w}$, scaled by its norm:
${x_a = x_a' + r \frac{w}{\|w\|}}$.

The margin condition requires all examples to be at least ${r}$ away from the hyperplane:
${y_n (\langle w, x_n \rangle + b) \geq r}$, with ${\|w\| = 1}$ (unit norm assumption for simplicity). The optimization problem becomes:
${\max_{w, b, r} r \text{ subject to } y_n (\langle w, x_n \rangle + b) \geq r, \|w\| = 1, r > 0}$.

### 12.2.2 Traditional Derivation of the Margin

An alternative derivation scales the data such that the closest example lies at ${\langle w, x_a \rangle + b = 1}$. For ${x_a'}$ on the hyperplane:
${\langle w, x_a' \rangle + b = 0}$.

Using ${x_a = x_a' + r \frac{w}{\|w\|}}$, substitute into the hyperplane equation:
${\langle w, x_a - r \frac{w}{\|w\|} \rangle + b = 0}$,
${\langle w, x_a \rangle + b - r \frac{\langle w, w \rangle}{\|w\|} = 0}$,
${1 - r \|w\| = 0}$,
${r = \frac{1}{\|w\|}}$.

The margin is thus ${\frac{1}{\|w\|}}$, and the condition becomes ${y_n (\langle w, x_n \rangle + b) \geq 1}$. The optimization problem is:
${\max_{w, b} \frac{1}{\|w\|} \text{ subject to } y_n (\langle w, x_n \rangle + b) \geq 1}$.

To simplify, minimize ${\frac{1}{2} \|w\|^2}$ (since maximizing ${\frac{1}{\|w\|}}$ is equivalent):
${\min_{w, b} \frac{1}{2} \|w\|^2 \text{ subject to } y_n (\langle w, x_n \rangle + b) \geq 1}$.

This is the **hard margin SVM**, which assumes perfect separability and does not allow violations.

### 12.2.3 Why We Can Set the Margin to 1

Theorem 12.1 proves the equivalence of the two formulations:
1. ${\max_{w, b, r} r \text{ subject to } y_n (\langle w, x_n \rangle + b) \geq r, \|w\| = 1, r > 0}$.
2. ${\min_{w, b} \frac{1}{2} \|w\|^2 \text{ subject to } y_n (\langle w, x_n \rangle + b) \geq 1}$.

Starting with the first, reparametrize ${w = \frac{w'}{\|w'\|}}$, adjust constraints, and show that maximizing ${r^2}$ with ${\|w\| = \frac{1}{r}}$ aligns with minimizing ${\frac{1}{2} \|w\|^2}$. This equivalence allows flexibility in formulation.

### 12.2.4 Soft Margin SVM: Geometric View

For non-linearly separable data (Figure 12.6), the **soft margin SVM** introduces slack variables ${\xi_n \geq 0}$ to allow some examples to fall within the margin or on the wrong side (Figure 12.7). The optimization becomes:
${\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{n=1}^N \xi_n \text{ subject to } y_n (\langle w, x_n \rangle + b) \geq 1 - \xi_n, \xi_n \geq 0}$.

- ${C > 0}$ is the regularization parameter, balancing margin size and classification errors.
- ${\xi_n}$ measures the violation distance, penalizing misclassifications or margin intrusions.
- Large ${C}$ prioritizes low error over a large margin; small ${C}$ emphasizes margin maximization.

The term ${\frac{1}{2} \|w\|^2}$ acts as a regularizer, while ${b}$ remains unregularized, impacting theoretical and computational aspects.

### 12.2.5 Soft Margin SVM: Loss Function View

From an empirical risk minimization perspective (Section 8.2), the SVM uses the **hinge loss**:
${\ell(t) = \max\{0, 1 - t\}}$, where ${t = y f(x) = y (\langle w, x \rangle + b)}$.

- If ${t \geq 1}$ (correct side, beyond margin), loss is 0.
- If ${0 < t < 1}$ (correct side, within margin), loss is positive.
- If ${t < 0}$ (wrong side), loss increases linearly.

The unconstrained problem is:
${\min_{w, b} \frac{1}{2} \|w\|^2 + C \sum_{n=1}^N \max\{0, 1 - y_n (\langle w, x_n \rangle + b)\}}$.

This is equivalent to the constrained form (12.26a) by replacing ${\max\{0, 1 - t\}}$ with ${\min_{\xi} \xi \text{ subject to } \xi \geq 0, \xi \geq 1 - t}$. The hard margin SVM uses an infinite penalty for violations (${\ell(t) = \infty}$ if ${t < 1}$).

---

## 12.3 Dual Support Vector Machine

### 12.3.1 Convex Duality via Lagrange Multipliers

The **primal SVM** optimizes over ${w, b, \xi}$, scaling with feature dimension ${D}$. The **dual SVM** shifts to optimizing over Lagrange multipliers ${\alpha_n}$, scaling with the number of examples ${N}$, which is advantageous when ${D > N}$. The Lagrangian for the soft margin SVM is:
${\mathfrak{L}(w, b, \xi, \alpha, \gamma) = \frac{1}{2} \|w\|^2 + C \sum_{n=1}^N \xi_n - \sum_{n=1}^N \alpha_n (y_n (\langle w, x_n \rangle + b) - 1 + \xi_n) - \sum_{n=1}^N \gamma_n \xi_n}$,
with ${\alpha_n \geq 0}$ and ${\gamma_n \geq 0}$.

Differentiating and setting to zero:
- ${\frac{\partial \mathfrak{L}}{\partial w} = 0 \implies w = \sum_{n=1}^N \alpha_n y_n x_n}$ (representer theorem).
- ${\frac{\partial \mathfrak{L}}{\partial b} = 0 \implies \sum_{n=1}^N \alpha_n y_n = 0}$.
- ${\frac{\partial \mathfrak{L}}{\partial \xi_n} = 0 \implies C - \alpha_n - \gamma_n = 0 \implies \alpha_n \leq C}$ (since ${\gamma_n \geq 0}$).

Substituting into the Lagrangian and simplifying yields the dual problem:
${\min_{\alpha} \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j \alpha_i \alpha_j \langle x_i, x_j \rangle - \sum_{i=1}^N \alpha_i \text{ subject to } \sum_{i=1}^N y_i \alpha_i = 0, 0 \leq \alpha_i \leq C}$.

- Examples with ${\alpha_n > 0}$ are **support vectors**, defining the hyperplane.
- ${b^*}$ is computed from support vectors on the margin: ${b^* = y_n - \langle w^*, x_n \rangle}$.

### 12.3.2 Dual SVM: Convex Hull View

Geometrically, the dual SVM finds the hyperplane bisecting the closest points between the convex hulls of positive and negative examples (Figure 12.9). The convex hull of a set ${X}$ is:
${\operatorname{conv}(X) = \{\sum_{n=1}^N \alpha_n x_n \mid \sum_{n=1}^N \alpha_n = 1, \alpha_n \geq 0\}}$.

For positive (${y_n = +1}$) and negative (${y_n = -1}$) hulls:
- ${c = \sum_{n: y_n = +1} \alpha_n^{+} x_n}$, ${\sum_{n: y_n = +1} \alpha_n^{+} = 1}$.
- ${d = \sum_{n: y_n = -1} \alpha_n^{-} x_n}$, ${\sum_{n: y_n = -1} \alpha_n^{-} = 1}$.
- ${w = c - d}$.

Minimize ${\frac{1}{2} \|w\|^2}$, leading to the dual hard margin SVM. The soft margin extends this with a reduced hull, bounding ${\alpha_n \leq C}$.

---

## 12.4 Kernels

The dual SVM’s reliance on inner products ${\langle x_i, x_j \rangle}$ allows the use of **kernels**, enabling non-linear classification. A kernel ${k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle_{\mathcal{H}}}$ implicitly maps data into a higher-dimensional space via ${\phi}$, without computing it explicitly (the **kernel trick**). Kernels must be symmetric and positive semidefinite, forming a Gram matrix ${K}$.

Examples include:
- **Linear Kernel**: ${k(x_i, x_j) = \langle x_i, x_j \rangle}$.
- **Polynomial Kernel**: Efficient for high-degree polynomials.
- **RBF Kernel**: Infinite-dimensional, Gaussian-based.

Figure 12.10 shows how kernels transform decision boundaries, maintaining linear separation in the transformed space.

---

## 12.5 Numerical Solution

The SVM can be solved as:
1. **Unconstrained Optimization**: Using the hinge loss subgradient ${g(t)}$ (non-differentiable at ${t = 1}$).
2. **Quadratic Programming**:
   - Primal: ${\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum \xi_n}$ with matrix constraints.
   - Dual: ${\min_{\alpha} \frac{1}{2} \alpha^T Y K Y \alpha - 1^T \alpha}$ with box constraints.

Practical solvers like LIBSVM (Chang and Lin, 2011) optimize these efficiently.

---

## 12.6 Further Reading

SVMs are one of many binary classification methods (e.g., logistic regression, random forests). They excel in empirical risk minimization and kernel methods, with extensive literature (Vapnik, 2000; Schölkopf and Smola, 2002). Extensions include probabilistic outputs via calibration (Platt, 2000) and Bayesian approaches like Gaussian process classification.
