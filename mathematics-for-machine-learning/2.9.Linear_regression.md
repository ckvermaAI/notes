# Linear Regression

## Overview
Chapter 9 applies the mathematical foundations from Chapters 2 (Linear Algebra), 5 (Vector Calculus), 6 (Probability and Distributions), and 7 (Optimization) to linear regression, a core machine learning problem. Linear regression involves finding a function ${f}$ that maps ${D}$-dimensional inputs ${\boldsymbol{x} \in \mathbb{R}^D}$ to scalar outputs ${f(\boldsymbol{x}) \in \mathbb{R}}$, given noisy observations ${y_n = f(\boldsymbol{x}_n) + \epsilon}$, where ${\epsilon}$ is i.i.d. Gaussian noise with zero mean and variance ${\sigma^2}$. The goal is to infer ${f}$ such that it fits the training data and generalizes to unseen data (Chapter 8). The chapter focuses on parametric models linear in their parameters, exploring maximum likelihood estimation (MLE), maximum a posteriori (MAP) estimation, and Bayesian linear regression, while addressing overfitting, uncertainty, and geometric interpretations.

---

## 9.1 Problem Formulation
Linear regression models the relationship between inputs ${\boldsymbol{x} \in \mathbb{R}^D}$ and noisy targets ${y \in \mathbb{R}}$ probabilistically, assuming Gaussian noise:
- **Likelihood**: ${p(y \mid \boldsymbol{x}) = \mathcal{N}(y \mid f(\boldsymbol{x}), \sigma^2)}$, where ${f(\boldsymbol{x})}$ is the underlying function, and ${y = f(\boldsymbol{x}) + \epsilon}$, with ${\epsilon \sim \mathcal{N}(0, \sigma^2)}$.
- **Objective**: Find ${f}$ that approximates the true function and generalizes well.
- **Parametric Model**: The chapter uses ${f(\boldsymbol{x}) = \boldsymbol{x}^{\top} \boldsymbol{\theta}}$, where ${\boldsymbol{\theta} \in \mathbb{R}^D}$ are parameters, yielding:
  - ${p(y \mid \boldsymbol{x}, \boldsymbol{\theta}) = \mathcal{N}(y \mid \boldsymbol{x}^{\top} \boldsymbol{\theta}, \sigma^2)}$,
  - ${y = \boldsymbol{x}^{\top} \boldsymbol{\theta} + \epsilon}$.
- **Linearity**: "Linear regression" refers to linearity in ${\boldsymbol{\theta}}$, not necessarily ${\boldsymbol{x}}$. For example, ${f(\boldsymbol{x}) = \phi^{\top}(\boldsymbol{x}) \boldsymbol{\theta}}$ with nonlinear features ${\phi(\boldsymbol{x})}$ remains linear in ${\boldsymbol{\theta}}$.

### Example 9.1
For scalar ${x, \theta \in \mathbb{R}}$, ${y = x \theta + \epsilon}$ represents lines through the origin, with ${\theta}$ as the slope (Figure 9.2(a)).

---

## 9.2 Parameter Estimation
Given a training set ${\mathcal{D} = \{(\boldsymbol{x}_1, y_1), \ldots, (\boldsymbol{x}_N, y_N)\}}$, where ${\boldsymbol{x}_n \in \mathbb{R}^D}$ and ${y_n \in \mathbb{R}}$, the likelihood factorizes due to i.i.d. noise:
- ${p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) = \prod_{n=1}^N \mathcal{N}(y_n \mid \boldsymbol{x}_n^{\top} \boldsymbol{\theta}, \sigma^2)}$, where ${\mathcal{X} = \{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N\}}$ and ${\mathcal{Y} = \{y_1, \ldots, y_N\}}$.
- **Prediction**: Optimal parameters ${\boldsymbol{\theta}^*}$ yield ${p(y_* \mid \boldsymbol{x}_*, \boldsymbol{\theta}^*) = \mathcal{N}(y_* \mid \boldsymbol{x}_*^{\top} \boldsymbol{\theta}^*, \sigma^2)}$ for test input ${\boldsymbol{x}_*}$.

### 9.2.1 Maximum Likelihood Estimation (MLE)
MLE finds ${\boldsymbol{\theta}_{\text{ML}}}$ maximizing ${p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta})}$:
- ${\boldsymbol{\theta}_{\text{ML}} \in \arg \max_{\boldsymbol{\theta}} p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta})}$.
- **Negative Log-Likelihood**: Minimize ${-\log p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) = -\sum_{n=1}^N \log \mathcal{N}(y_n \mid \boldsymbol{x}_n^{\top} \boldsymbol{\theta}, \sigma^2)}$:
  - ${\log \mathcal{N}(y_n \mid \boldsymbol{x}_n^{\top} \boldsymbol{\theta}, \sigma^2) = -\frac{1}{2\sigma^2} (y_n - \boldsymbol{x}_n^{\top} \boldsymbol{\theta})^2 + \text{const}}$,
  - ${\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \boldsymbol{x}_n^{\top} \boldsymbol{\theta})^2}$.
- **Design Matrix**: Define ${\boldsymbol{X} = [\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N]^{\top} \in \mathbb{R}^{N \times D}}$ and ${\boldsymbol{y} = [y_1, \ldots, y_N]^{\top} \in \mathbb{R}^N}$, so ${\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2\sigma^2} \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\theta}\|^2}$.
- **Gradient**: ${\frac{d\mathcal{L}}{d\boldsymbol{\theta}} = \frac{1}{\sigma^2} (-\boldsymbol{y}^{\top} \boldsymbol{X} + \boldsymbol{\theta}^{\top} \boldsymbol{X}^{\top} \boldsymbol{X})}$.
- **Solution**: Set ${\frac{d\mathcal{L}}{d\boldsymbol{\theta}} = \mathbf{0}^{\top}}$, yielding ${\boldsymbol{\theta}_{\text{ML}} = (\boldsymbol{X}^{\top} \boldsymbol{X})^{-1} \boldsymbol{X}^{\top} \boldsymbol{y}}$, assuming ${\boldsymbol{X}^{\top} \boldsymbol{X}}$ is invertible (requires ${\text{rk}(\boldsymbol{X}) = D}$).
- **Remarks**:
  - The Hessian ${\nabla_{\boldsymbol{\theta}}^2 \mathcal{L} = \boldsymbol{X}^{\top} \boldsymbol{X}}$ is positive definite, ensuring a unique global minimum.
  - Solves ${\boldsymbol{A} \boldsymbol{\theta} = \boldsymbol{b}}$, where ${\boldsymbol{A} = \boldsymbol{X}^{\top} \boldsymbol{X}}$, ${\boldsymbol{b} = \boldsymbol{X}^{\top} \boldsymbol{y}}$.

#### Example 9.2 (Fitting Lines)
For ${f(x) = \theta x}$, MLE fits a line to data (Figure 9.2(b)), with ${\theta_{\text{ML}}}$ from ${(9.12c)}$ visualized in Figure 9.2(c).

#### MLE with Features
For nonlinear relationships, use features ${\phi: \mathbb{R}^D \rightarrow \mathbb{R}^K}$:
- ${p(y \mid \boldsymbol{x}, \boldsymbol{\theta}) = \mathcal{N}(y \mid \phi^{\top}(\boldsymbol{x}) \boldsymbol{\theta}, \sigma^2)}$,
- ${y = \phi^{\top}(\boldsymbol{x}) \boldsymbol{\theta} + \epsilon}$.
- **Feature Matrix**: ${\boldsymbol{\Phi} = [\phi^{\top}(\boldsymbol{x}_1), \ldots, \phi^{\top}(\boldsymbol{x}_N)]^{\top} \in \mathbb{R}^{N \times K}}$.
- **Negative Log-Likelihood**: ${-\log p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) = \frac{1}{2\sigma^2} \|\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\theta}\|^2 + \text{const}}$.
- **Solution**: ${\boldsymbol{\theta}_{\text{ML}} = (\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\top} \boldsymbol{y}}$, requiring ${\text{rk}(\boldsymbol{\Phi}) = K}$.

#### Example 9.3 (Polynomial Regression)
For ${x \in \mathbb{R}}$, define ${\phi(x) = [1, x, x^2, \ldots, x^{K-1}]^{\top} \in \mathbb{R}^K}$, enabling polynomials of degree ${\leq K-1}$:
- ${f(x) = \sum_{k=0}^{K-1} \theta_k x^k = \phi^{\top}(x) \boldsymbol{\theta}}$.

#### Example 9.4 (Feature Matrix for Second-Order Polynomials)
For ${K=3}$, ${\boldsymbol{\Phi} = [1, x_n, x_n^2]_{n=1}^N \in \mathbb{R}^{N \times 3}}$.

#### Example 9.5 (Maximum Likelihood Polynomial Fit)
Fit a degree-4 polynomial to ${N=10}$ points ${y_n = -\sin(x_n/5) + \cos(x_n) + \epsilon}$, ${\epsilon \sim \mathcal{N}(0, 0.2^2)}$ (Figure 9.4).

#### Estimating Noise Variance
If ${\sigma^2}$ is unknown, maximize ${\log p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}, \sigma^2)}$:
- ${\log p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}, \sigma^2) = -\frac{N}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \phi^{\top}(\boldsymbol{x}_n) \boldsymbol{\theta})^2 + \text{const}}$,
- ${\frac{\partial}{\partial \sigma^2} = -\frac{N}{2\sigma^2} + \frac{s}{2\sigma^4} = 0}$, where ${s = \sum_{n=1}^N (y_n - \phi^{\top}(\boldsymbol{x}_n) \boldsymbol{\theta})^2}$,
- ${\sigma_{\text{ML}}^2 = \frac{s}{N}}$.

### 9.2.2 Overfitting in Linear Regression
- **Evaluation**: Use root mean square error (RMSE): ${\sqrt{\frac{1}{N} \|\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\theta}\|^2}}$, which is unit-consistent and normalized.
- **Model Selection**: Test polynomial degrees ${0 \leq M \leq N-1}$. For ${M \geq N}$, ${\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi}}$ is not invertible, yielding infinite solutions.
- **Example**: For ${N=10}$ (Figure 9.5):
  - Low ${M}$ (0, 1) underfits,
  - ${M=3}$ to 6 fits well,
  - High ${M}$ (e.g., 9) overfits, passing through all points but oscillating wildly.
- **Generalization**: Test RMSE on 200 unseen points (Figure 9.6) shows optimal ${M=4}$, with test error increasing for ${M \geq 6}$.

### 9.2.3 Maximum A Posteriori (MAP) Estimation
To mitigate overfitting, use a prior ${p(\boldsymbol{\theta})}$:
- **Posterior**: ${p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y}) = \frac{p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathcal{Y} \mid \mathcal{X})}}$,
- **MAP**: ${\boldsymbol{\theta}_{\text{MAP}} \in \arg \min_{\boldsymbol{\theta}} \{-\log p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) - \log p(\boldsymbol{\theta})\}}$.
- **Gaussian Prior**: ${p(\boldsymbol{\theta}) = \mathcal{N}(\mathbf{0}, b^2 \boldsymbol{I})}$:
  - ${-\log p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y}) = \frac{1}{2\sigma^2} \|\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\theta}\|^2 + \frac{1}{2b^2} \boldsymbol{\theta}^{\top} \boldsymbol{\theta} + \text{const}}$,
  - Gradient: ${\frac{1}{\sigma^2} (\boldsymbol{\theta}^{\top} \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} - \boldsymbol{y}^{\top} \boldsymbol{\Phi}) + \frac{1}{b^2} \boldsymbol{\theta}^{\top} = \mathbf{0}^{\top}}$,
  - ${\boldsymbol{\theta}_{\text{MAP}} = (\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} + \frac{\sigma^2}{b^2} \boldsymbol{I})^{-1} \boldsymbol{\Phi}^{\top} \boldsymbol{y}}$.
- **Comparison**: Adds ${\frac{\sigma^2}{b^2} \boldsymbol{I}}$ to MLE, ensuring invertibility and regularization.

#### Example 9.6 (MAP Polynomial Regression)
For degree-6 and 8 polynomials with ${p(\boldsymbol{\theta}) = \mathcal{N}(\mathbf{0}, \boldsymbol{I})}$, MAP smooths high-degree fits compared to MLE (Figure 9.7).

### 9.2.4 MAP Estimation as Regularization
- **Regularized Least Squares**: Minimize ${\|\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\theta}\|^2 + \lambda \|\boldsymbol{\theta}\|_2^2}$:
  - Solution: ${\boldsymbol{\theta}_{\text{RLS}} = (\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} + \lambda \boldsymbol{I})^{-1} \boldsymbol{\Phi}^{\top} \boldsymbol{y}}$,
  - Matches MAP with ${\lambda = \frac{\sigma^2}{b^2}}$.
- **Variants**: LASSO uses ${\|\boldsymbol{\theta}\|_1}$ for sparsity (Tibshirani, 1996).

---

## 9.3 Bayesian Linear Regression
Bayesian linear regression avoids point estimates, using a full posterior over ${\boldsymbol{\theta}}$.

### 9.3.1 Model
- **Prior**: ${p(\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{m}_0, \boldsymbol{S}_0)}$, Gaussian prior on $\theta$ which turns the parameter vector into a random variable.
- **Likelihood**: ${p(y \mid \boldsymbol{x}, \boldsymbol{\theta}) = \mathcal{N}(y \mid \phi^{\top}(\boldsymbol{x}) \boldsymbol{\theta}, \sigma^2)}$,
- **Joint** distribution of observed ($y$) and unobserved ($\theta$) random variables: ${p(y, \boldsymbol{\theta} \mid \boldsymbol{x}) = p(y \mid \boldsymbol{x}, \boldsymbol{\theta}) p(\boldsymbol{\theta})}$.

### 9.3.2 Prior Predictions
- In practice, we are usually not so much interested in the parameter values $\theta$ themselves. Instead, our focus often lies in the predictions we make with those parameter values. In a Bayesian setting, we take the parameter distribution and average over all plausible parameter settings when we make predictions.
- **Predictive Distribution**: ${p(y_* \mid \boldsymbol{x}_*) = \int p(y_* \mid \boldsymbol{x_*}, \boldsymbol{\theta}) p(\boldsymbol{\theta}) d\boldsymbol{\theta} = \int \mathcal{N}(y_* \mid \phi^{\top}(\boldsymbol{x}_*) \boldsymbol{\theta}, \sigma^2) \mathcal{N}(\boldsymbol{m}_0, \boldsymbol{S}_0) d\boldsymbol{\theta} = \mathcal{N}(\phi^{\top}(\boldsymbol{x}_*) \boldsymbol{m}_0, \phi^{\top}(\boldsymbol{x}_*) \boldsymbol{S}_0 \phi(\boldsymbol{x}_*) + \sigma^2)}$,
- **Noise-Free**: ${p(f(\boldsymbol{x}_*)) = \mathcal{N}(\phi^{\top}(\boldsymbol{x}_*) \boldsymbol{m}_0, \phi^{\top}(\boldsymbol{x}_*) \boldsymbol{S}_0 \phi(\boldsymbol{x_*}))}$.

#### Example 9.7 (Prior over Functions)
For degree-5 polynomials with ${p(\boldsymbol{\theta}) = \mathcal{N}(\mathbf{0}, \frac{1}{4} \boldsymbol{I})}$, Figure 9.9 shows prior function distribution and samples.

### 9.3.3 Posterior Distribution
- **Posterior**: ${p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y}) = \frac{\mathcal{N}(\boldsymbol{y} \mid \boldsymbol{\Phi} \boldsymbol{\theta}, \sigma^2 \boldsymbol{I}) \mathcal{N}(\boldsymbol{m}_0, \boldsymbol{S}_0)}{p(\mathcal{Y} \mid \mathcal{X})}}$,
- **Closed Form**: ${p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y}) = \mathcal{N}(\boldsymbol{m}_N, \boldsymbol{S}_N)}$,
  - ${\boldsymbol{S}_N = (\boldsymbol{S}_0^{-1} + \sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi})^{-1}}$,
  - ${\boldsymbol{m}_N = \boldsymbol{S}_N (\boldsymbol{S}_0^{-1} \boldsymbol{m}_0 + \sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{y})}$.
- **Proof**: Completing the squares on ${-\frac{1}{2} (\sigma^{-2} (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\theta})^{\top} (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\theta}) + (\boldsymbol{\theta} - \boldsymbol{m}_0)^{\top} \boldsymbol{S}_0^{-1} (\boldsymbol{\theta} - \boldsymbol{m}_0))}$.

### 9.3.4 Posterior Predictions
- **Predictive Distribution**: ${p(y_* \mid \mathcal{X}, \mathcal{Y}, \boldsymbol{x}_*) = \mathcal{N}(y_* \mid \phi^{\top}(\boldsymbol{x}_*) \boldsymbol{m}_N, \phi^{\top}(\boldsymbol{x}_*) \boldsymbol{S}_N \phi(\boldsymbol{x}_*) + \sigma^2)}$,
- **Noise-Free**: ${\mathbb{E}[f(\boldsymbol{x}_*) \mid \mathcal{X}, \mathcal{Y}] = \phi^{\top}(\boldsymbol{x}_*) \boldsymbol{m}_N}$, ${\mathbb{V}[f(\boldsymbol{x}_*) \mid \mathcal{X}, \mathcal{Y}] = \phi^{\top}(\boldsymbol{x}_*) \boldsymbol{S}_N \phi(\boldsymbol{x_*})}$.

#### Example 9.8 (Posterior over Functions)
For degree-5 polynomials, Figure 9.10 shows training data, posterior distribution, and samples. Figure 9.11 compares ${M=3, 5, 7}$, highlighting increased uncertainty with higher ${M}$.

### 9.3.5 Computing the Marginal Likelihood
- **Marginal Likelihood**: ${p(\mathcal{Y} \mid \mathcal{X}) = \int p(y \mid \boldsymbol{x}, \boldsymbol{\theta}) p(\boldsymbol{\theta}) d\boldsymbol{\theta} = \mathcal{N}(\boldsymbol{y} \mid \boldsymbol{X} \boldsymbol{m}_0, \boldsymbol{X} \boldsymbol{S}_0 \boldsymbol{X}^{\top} + \sigma^2 \boldsymbol{I})}$,
- **Derivation**: Uses Gaussian properties and affine transformations.

---

## 9.4 Maximum Likelihood as Orthogonal Projection
- Geometric interpretation of maximum likelihood estimation. Let us consider a simple linear regression setting.
- **Simple Case**: ${y = x \theta + \epsilon}$, ${\theta_{\text{ML}} = \frac{\boldsymbol{X}^{\top} \boldsymbol{y}}{\boldsymbol{X}^{\top} \boldsymbol{X}}}$,
- **Projection**: ${\boldsymbol{X} \theta_{\text{ML}} = \frac{\boldsymbol{X} \boldsymbol{X}^{\top}}{\boldsymbol{X}^{\top} \boldsymbol{X}} \boldsymbol{y}}$ projects ${\boldsymbol{y}}$ onto the subspace spanned by ${\boldsymbol{X}}$ (Figure 9.12).
- **General Case**: ${\boldsymbol{\Phi} \theta_{\text{ML}} = \boldsymbol{\Phi} (\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^{\top} \boldsymbol{y}}$ projects onto the ${K}$-dimensional subspace spanned by ${\boldsymbol{\Phi}}$â€™s columns.
- **Orthonormal Features**: If ${\boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} = \boldsymbol{I}}$, ${\boldsymbol{\Phi} \boldsymbol{\Phi}^{\top} \boldsymbol{y} = \sum_{k=1}^K \phi_k \phi_k^{\top} \boldsymbol{y}}$.

---

## 9.5 Further Reading
- **Generalized Linear Models (GLMs)**: Extend linear regression with non-Gaussian likelihoods (e.g., Bernoulli for classification, Poisson for counts) via ${y = \sigma(f(\boldsymbol{x}))}$, foundational to neural networks.
- **Gaussian Processes**: Model distributions over functions directly, related to Bayesian linear regression with infinite features.
- **Non-Gaussian Priors**: Laplace priors (LASSO) enforce sparsity, enhancing interpretability in high-dimensional settings.
