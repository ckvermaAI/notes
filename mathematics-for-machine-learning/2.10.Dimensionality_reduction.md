# Dimensionality Reduction with Principal Component Analysis

## Introduction to Dimensionality Reduction

High-dimensional data, such as images, poses challenges: it is difficult to analyze, interpret, visualize, and store efficiently. However, such data often exhibits exploitable properties, including redundancy (overcompleteness) and correlation among dimensions, suggesting an intrinsic lower-dimensional structure. Dimensionality reduction leverages these properties to create a compact representation, ideally preserving essential information, akin to compression techniques like JPEG or MP3. This chapter focuses on **Principal Component Analysis (PCA)**, a linear dimensionality reduction method introduced by Pearson (1901) and Hotelling (1933). Over a century old, PCA remains widely used for data compression, visualization, and identifying patterns, latent factors, and structures in high-dimensional data. In signal processing, PCA is known as the **Karhunen-Loève transform**. The chapter derives PCA from first principles, drawing on concepts like basis changes, projections, eigenvalues, Gaussian distributions, and constrained optimization.

For example, a $640 \times 480$ pixel color image exists in a million-dimensional space ($640 \times 480 \times 3$ for RGB channels), but PCA can reduce this to a manageable subspace. Figure 10.1 illustrates this: a 2D dataset with little variation in the $x_2$ direction can be compressed to a 1D representation along $x_1$ with minimal loss.

---

## 10.1 Problem Setting

PCA aims to find projections $\tilde{x}_n$ of data points $x_n$ that are as similar as possible to the originals while reducing intrinsic dimensionality. Given an i.i.d. dataset $\mathcal{X} = \{x_1, \ldots, x_N\}$, where $x_n \in \mathbb{R}^D$ and the mean is $0$, the **data covariance matrix** is defined as:

$S = \frac{1}{N} \sum_{n=1}^N x_n x_n^\top$

PCA seeks a low-dimensional representation (code) $z_n = B^\top x_n \in \mathbb{R}^M$, where $B = [b_1, \ldots, b_M] \in \mathbb{R}^{D \times M}$ is a projection matrix with orthonormal columns ($b_i^\top b_j = 0$ if $i \neq j$, and $b_i^\top b_i = 1$). The goal is to project data onto an $M$-dimensional subspace $U \subseteq \mathbb{R}^D$ (where $M < D$), with projected points $\tilde{x}_n = B z_n$, minimizing compression loss. For instance, in $\mathbb{R}^2$ with basis $e_1 = [1,0]^\top$ and $e_2 = [0,1]^\top$, a vector like $[5, 3]^\top$ can be approximated as $z e_2$ if variation is mostly along $e_2$, reducing storage to a single coordinate.

---

## 10.2 Maximum Variance Perspective

PCA can be derived by maximizing the variance of the projected data to retain maximum information. Variance measures data spread (Section 6.4.1), and PCA identifies a subspace capturing the most variance.

### 10.2.1 Direction with Maximal Variance

For a single direction, PCA seeks a unit vector $b_1 \in \mathbb{R}^D$ maximizing the variance of the first coordinate $z_{1n} = b_1^\top x_n$:

$V_1 = \mathbb{V}[z_1] = \frac{1}{N} \sum_{n=1}^N z_{1n}^2 = b_1^\top S b_1$

Since scaling $b_1$ artificially inflates $V_1$, the constraint $||b_1||^2 = 1$ is imposed, forming the optimization problem:

$\max_{b_1} b_1^\top S b_1 \quad \text{subject to} \quad ||b_1||^2 = 1$

Using the Lagrangian $\mathfrak{L}(b_1, \lambda) = b_1^\top S b_1 + \lambda_1 (1 - b_1^\top b_1)$, setting partial derivatives to zero yields:

$S b_1 = \lambda_1 b_1, \quad b_1^\top b_1 = 1$

Thus, $b_1$ is an eigenvector of $S$, and $\lambda_1$ is its eigenvalue. The variance becomes $V_1 = \lambda_1$, so $b_1$ is chosen as the eigenvector with the largest eigenvalue (the **first principal component**). The projected data in the original space is $\tilde{x}_n = b_1 b_1^\top x_n$.

### 10.2.2 M-dimensional Subspace with Maximal Variance

For $M$ dimensions, PCA sequentially finds orthonormal eigenvectors $b_1, \ldots, b_M$ corresponding to the $M$ largest eigenvalues. After finding the first $m-1$ components, the $m$-th is derived by subtracting their effect:

$\hat{X} = X - \sum_{i=1}^{m-1} b_i b_i^\top X$

where $X = [x_1, \ldots, x_N] \in \mathbb{R}^{D \times N}$. The variance along $b_m$ is:

$V_m = b_m^\top \hat{S} b_m, \quad \text{subject to} \quad ||b_m||^2 = 1$

where $\hat{S}$ is the covariance of $\hat{X}$. Solving shows $b_m$ is an eigenvector of both $S$ and $\hat{S}$, with $V_m = \lambda_m$ (the $m$-th largest eigenvalue of $S$). Total variance captured by $M$ components is:

$V_M = \sum_{m=1}^M \lambda_m$

Variance lost is $J_M = V_D - V_M = \sum_{j=M+1}^D \lambda_j$. Relative variance captured is $V_M / V_D$. For MNIST "8" digits, Figure 10.5 shows most variance is captured by a few components.

---

## 10.3 Projection Perspective

Alternatively, PCA minimizes the average squared reconstruction error:

$J_M = \frac{1}{N} \sum_{n=1}^N ||x_n - \tilde{x}_n||^2$

### 10.3.1 Setting and Objective

Given an orthonormal basis $B = (b_1, \ldots, b_D)$ of $\mathbb{R}^D$, any $x \in \mathbb{R}^D$ is $x = \sum_{d=1}^D \zeta_d b_d$. PCA seeks $\tilde{x} = \sum_{m=1}^M z_m b_m \in U$ (dim $M$) to minimize $||x - \tilde{x}||^2$ for a centered dataset.

### 10.3.2 Finding Optimal Coordinates

For fixed $b_1, \ldots, b_M$, optimize $z_{mn}$ in $\tilde{x}_n = B z_n$. The partial derivative of $J_M$ with respect to $z_{in}$ yields $z_{in} = b_i^\top x_n$, showing $\tilde{x}_n$ is the orthogonal projection onto $U$.

### 10.3.3 Finding the Basis of the Principal Subspace

Substituting $\tilde{x}_n = B B^\top x_n$, the error becomes:

$J_M = \frac{1}{N} \sum_{n=1}^N ||(I - B B^\top) x_n||^2 = \sum_{j=M+1}^D b_j^\top S b_j = \sum_{j=M+1}^D \lambda_j$

Minimizing $J_M$ is equivalent to maximizing $\sum_{m=1}^M \lambda_m$, aligning with the variance perspective. $B B^\top$ is a rank-$M$ approximation of the identity.

---

## 10.4 Eigenvector Computation and Low-Rank Approximations

The covariance $S = \frac{1}{N} X X^\top$ can be decomposed via eigendecomposition or SVD of $X = U \Sigma V^\top$, where $S = \frac{1}{N} U \Sigma \Sigma^\top U^\top$, and eigenvalues are $\lambda_d = \sigma_d^2 / N$. PCA uses the top $M$ eigenvectors of $S$.

### 10.4.1 PCA Using Low-Rank Matrix Approximations

The Eckart-Young theorem provides the best rank-$M$ approximation $\hat{X}_M = U_M \Sigma_M V_M^\top$, aligning with PCA’s projection matrix $B = U_M$.

### 10.4.2 Practical Aspects

For large matrices, iterative methods (e.g., power iteration: $x_{k+1} = S x_k / ||S x_k||$) efficiently compute dominant eigenvectors, as used in Google’s PageRank.

---

## 10.5 PCA in High Dimensions

For $N \ll D$, computing $S \in \mathbb{R}^{D \times D}$ is costly. Instead, use $X^\top X \in \mathbb{R}^{N \times N}$, where $S b_m = \lambda_m b_m$ transforms to $\frac{1}{N} X^\top X c_m = \lambda_m c_m$ ($c_m = X^\top b_m$). Eigenvectors of $S$ are recovered as $X c_m$ (normalized).

---

## 10.6 Key Steps of PCA in Practice

1. **Mean Subtraction**: Center data by subtracting $\mu$.
2. **Standardization**: Divide by standard deviation $\sigma_d$ per dimension.
3. **Eigendecomposition**: Compute $S$’s eigenvectors and eigenvalues.
4. **Projection**: Project standardized data onto $B$’s top eigenvectors, adjust back to original space.

Example: MNIST "8" reconstruction improves with more PCs (Figure 10.12), with error $\sum_{i=M+1}^D \lambda_i$ dropping rapidly (Figure 10.13).

---

## 10.7 Latent Variable Perspective

**Probabilistic PCA (PPCA)** (Tipping and Bishop, 1999) models $x = B z + \mu + \epsilon$, with $z \sim \mathcal{N}(0, I)$ and $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$. The likelihood is $p(x | B, \mu, \sigma^2) = \mathcal{N}(x | \mu, B B^\top + \sigma^2 I)$, enabling generative sampling and Bayesian inference. Posterior $p(z | x)$ is Gaussian, useful for outlier detection.

---

## 10.8 Further Reading

PCA is a linear auto-encoder ($z = B^\top x$, $\tilde{x} = B z$) or maximizes mutual information. PPCA’s maximum likelihood estimates are $\mu_{ML} = \bar{x}$, $B_{ML} = T (\Lambda - \sigma^2 I)^{1/2} R$, $\sigma_{ML}^2 = \frac{1}{D-M} \sum_{j=M+1}^D \lambda_j$. Extensions include Bayesian PCA, factor analysis (FA), ICA, kernel PCA, deep auto-encoders, and GP-LVM.