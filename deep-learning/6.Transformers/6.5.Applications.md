# Applications of Transformers

## Introduction and Recap of Transformer Basics
- **Context**: The lecture builds on prior discussions of the Transformer architecture, a neural network model widely used in natural language processing (NLP) and beyond.
- **Sentiment Analysis Example**: 
  - **Process**: A sentence is augmented with a special classification token (e.g., `[CLS]` in BERT-style models), fed through a Transformer, and the output at this token’s position is used to classify sentiment (positive or negative).
  - **Mechanism**: The classification token attends to all other tokens in the sentence at each layer, aggregating information via attention to build a representation of the entire input. This output is then used for classification.
  - **Insight**: This demonstrates how Transformers can summarize sequence data into a single vector for downstream tasks.

## Application to Machine Translation
- **Task**: Transformers are applied to translate a sentence from one language to another (e.g., "I love deep learning" in English to "我爱深度学习" (Wǒ ài shēndù xuéxí) in Chinese).
- **Challenges**:
  - **Variable Length**: Input and output sentences differ in length and structure (e.g., English and Chinese have distinct grammatical rules), lacking a one-to-one token correspondence.
  - **Coherent Output**: Generating a full, coherent sentence simultaneously is difficult, unlike human language production, which is sequential (one word at a time).

## Autoregressive Prediction for Translation
- **Solution**: Machine translation is framed as an autoregressive task:
  - **Process**: The Transformer predicts one token at a time. Given an input sentence, it outputs the first word (e.g., "I"), then takes the input plus this output to predict the next word (e.g., "love"), continuing iteratively.
  - **End of Sentence (EOS) Token**: An EOS token signals completion of the output sequence.
- **Formulation**: For a sequence $X = [x_1, x_2, ..., x_n]$ (input) and $Y = [y_1, y_2, ..., y_m]$ (output), the Transformer models $P(y_t | X, y_1, ..., y_{t-1})$, predicting each $y_t$ conditioned on prior outputs and the input.

## Masked Attention Mechanism
- **Need**: Vanilla Transformers use full attention, attending to all tokens, which allows "cheating" during training by looking at future tokens in the target sequence.
- **Solution**: Masked attention restricts attention to previous tokens only:
  - **Implementation**: A mask matrix is applied to the attention operator, setting future token scores to zero before the softmax. For a sequence of length $T$, the attention score matrix $A$ is modified such that $A_{i,j} = 0$ for $j > i$.
  - **Mathematical Form**: $Attention(Q, K, V) = softmax(\frac{QK^T + M}{\sqrt{d_k}})V$, where $M$ is the mask with $M_{i,j} = -\infty$ if $j > i$ (effectively zeroing post-softmax contributions).
- **Benefits**:
  - **Training Efficiency**: Allows parallel prediction of all next tokens during training, conditioned on prior ground-truth tokens.
  - **Inference Efficiency**: Enables caching of prior computations, speeding up sequential generation at test time.

## Training with Teacher Forcing
- **Problem**: Sequential prediction during training (one word at a time) is computationally infeasible for large models.
- **Solution**: Teacher forcing:
  - **Process**: The Transformer is conditioned on ground-truth tokens from the target sequence, predicting only the next token at each step. E.g., given "I love" as ground truth, it predicts "deep," then given "I love deep," it predicts "learning."
  - **Formulation**: For training, $P(y_t | X, y_1, ..., y_{t-1})$ uses true $y_1, ..., y_{t-1}$ rather than model-generated outputs.
- **Mismatch**: Training uses ground truth, while testing samples autoregressively, but this works well in practice with minor sampling adjustments (e.g., beam search or temperature scaling).
- **EOS in Training**: The model is trained with EOS tokens to learn when to stop generating.

## Translation Workflow
- **Execution**: 
  - Start with a start-of-sentence (SOS) token.
  - Predict the first word using the input sentence, then feed back ground truth (during training) or predicted tokens (during testing) until EOS is produced.
  - Example: Input "我爱深度学习" → SOS → "I" → "love" → "deep" → "learning" → EOS.

## Transformer Architectures
- **Decoder-Only**:
  - **Description**: Used for translation and text generation, as described above. The model takes an input and continues generating an output sequence.
  - **Popularity**: Most common due to flexibility and ease of training (e.g., GPT family).
  - **Training**: Predicts the next token in a sequence, leveraging masked attention and teacher forcing.
- **Encoder-Decoder**:
  - **Description**: An encoder processes the input into a fixed representation, which a decoder uses to generate the output (e.g., T5, popularized by Google).
  - **Use Case**: Similar to sentiment analysis encoding, but paired with a generative decoder.
- **Encoder-Only**:
  - **Description**: Encodes text into fixed vectors without generation (e.g., BERT).
  - **Use Case**: Text understanding, search, or feature extraction, not suited for sequence generation.

## Versatility Beyond Text
- **Token Expansion**: Transformers originally used character or subword tokens (e.g., WordPiece, BPE), but now handle:
  - **Visual Inputs**: Images are tokenized into patches (e.g., ViT splits images into $16 \times 16$ patches), fed as continuous (raw pixel values) or discrete (compressed codebook representations) tokens.
  - **Audio**: Modeled autoregressively as sequences of sound features or tokens.
- **Applications**: 
  - **Generative Modeling**: Autoregressive prediction extends to images (e.g., DALL-E) and audio (e.g., WaveNet-style models).
  - **Unified Architecture**: Transformers dominate diverse fields due to their flexibility in handling sequential data.

## Efficiency and Practicality
- **Training**: Teacher forcing and masked attention enable parallel training of large models, predicting all next tokens simultaneously.
- **Inference**: Caching in masked attention speeds up autoregressive generation on devices like laptops or phones.
- **Scalability**: These optimizations allow Transformers to scale to massive datasets and models (e.g., GPT-3, LLaMA).

## Conclusion
- **Summary**: Transformers are versatile, handling tasks like sentiment analysis, translation, and multimodal generation via autoregressive prediction, masked attention, and teacher forcing.
- **Architectures**: Decoder-only is the most popular, followed by encoder-decoder and encoder-only, each suited to specific tasks.
- **Impact**: Their ability to process diverse token types and train efficiently positions Transformers as a dominant architecture across NLP, vision, and beyond.

---

## Additional Notes
- **Historical Context**: Introduced in "Attention is All You Need" (Vaswani et al., 2017), Transformers replaced RNNs due to parallelization and long-range dependency handling.
- **Real-World Example**: Models like Google Translate (encoder-decoder) and ChatGPT (decoder-only) exemplify these concepts in practice.
