# Positional Embeddings

## Introduction to Positional Embeddings and Permutation Invariance

The lecture begins by discussing the fundamental issue with the basic form of attention mechanisms in transformers: they are **permutation invariant**. This means that if the inputs (queries, keys, and values) to an attention mechanism are permuted in the same way, the output remains unchanged. Mathematically, this arises because the permutations in the keys and values cancel out due to the softmax operation in attention:

- For a set of inputs, permuting the keys and values arbitrarily (while keeping the queries the same) does not affect the attention output.
- This property poses a challenge for tasks where the order or spatial arrangement of inputs matters, such as in natural language processing (NLP), speech, or image processing.

### Why Permutation Invariance is Problematic
- In NLP, the order of words in a sentence is crucial. For example, "My kid likes the movie" is not the same as "The movie likes my kid"—one is meaningful, while the other is not.
- In speech or sound patterns, the temporal structure matters.
- In images, spatial arrangement is key. The lecture references "visual anagrams" (from a CVPR paper), where reshuffling patches of an image (with minimal post-processing like color warping) changes its visual meaning entirely.

### Cases Where Permutation Invariance is Not an Issue
However, there are scenarios where permutation invariance is not a problem:
- Example: Predicting power outages in counties in Texas based on past data. If the counties are treated as a set (without spatial relationships), their exact locations may not matter. A transformer can learn patterns across counties without needing positional information, as long as the data itself does not rely on spatial or sequential ordering.

### Inputs Where Order Matters
For most applications, order is critical:
- **Natural Language**: Sentences are heavily ordered.
- **Speech/Sound**: Temporal correlations are significant.
- **Images**: Spatial structure is essential.
- **Point Clouds**: An interesting edge case. Point clouds (sets of 3D coordinates, often from a range finder like a 3D camera) can be treated as a set. Since the positional information is already encoded in the 3D coordinates of the points, a transformer can treat them as a permutation-invariant set and still interpret the scene effectively.

### Breaking Permutation Invariance
To address this limitation, **positional embeddings** are introduced. They add positional information to the inputs, breaking the permutation invariance of attention:
- If positional information is encoded in the input $x$, permuting the keys and values also permutes the positions, which changes the attention output.
- This allows the model to reason about relationships between nearby elements (e.g., words in a sentence, points in a point cloud, or patches in an image).

## What Are Positional Embeddings?
Positional embeddings are additional information added to the input embeddings to encode the position of each input:
- For a sentence, the embedding of a word is combined with an embedding of its position.
- This combined embedding ensures that attention is no longer a set operation—it can now reason about positional relationships.

### Simple Positional Embedding: Enumeration
The most straightforward approach to positional embedding is to enumerate the inputs:
- For a sequence of length $n$, assign positions from $1$ to $n$.
- In images, enumerate in 2D (e.g., $(x, y)$ coordinates).
- These positions can be attached directly or passed through a linear layer to create an embedding.

However, this method is ineffective because:
- The network struggles to interpret what these numbers mean (e.g., position $0$ vs. $1$ vs. $2$).
- Concatenating or adding these numbers to the input embeddings does not provide meaningful positional information for the network to learn from.

## Sinusoidal Positional Embeddings
A more effective approach is **sinusoidal positional embeddings**, which use sine and cosine functions to encode positions:
- For a position $n$ in the sequence, the embedding is computed across a $c$-dimensional space, where $c$ is the number of channels (or frequencies).
- The position $n$ is divided by a base frequency that increases exponentially (often using a base like $10000$).
- The formula for sinusoidal positional embeddings (as introduced in the "Attention is All You Need" paper) is:
  $PE(n, 2i) = \sin\left(\frac{n}{10000^{2i/c}}\right), \quad PE(n, 2i+1) = \cos\left(\frac{n}{10000^{2i/c}}\right)$
  where:
  - $n$ is the position in the sequence.
  - $i$ is the dimension index ($0 \leq i < c/2$).
  - $c$ is the total dimensionality of the embedding.

### Characteristics of Sinusoidal Embeddings
- **High to Low Frequencies**: The embedding starts with high-frequency components (small denominators, e.g., $10000^{0}$) and progresses to low-frequency components (large denominators, e.g., $10000^{(c-2)/c}$).
- **Localization**: The network can use these sines and cosines to localize elements efficiently:
  - Low-frequency components (large $i$) help identify broad regions in the sequence.
  - High-frequency components (small $i$) allow fine-grained localization within those regions.
- **Large Sequence Handling**: The exponential increase in frequency denominators allows the embedding to handle very large sequences by splitting positions into fine-grained representations.

## Learnable Positional Embeddings
Another approach is to use **learnable positional embeddings**:
- Instead of using a fixed function (like sines and cosines), assign a learnable weight to each position.
- These weights are optimized during training, allowing the network to focus on each input individually.

### Limitations of Learnable Embeddings
- Works well if the training sequence length is always greater than or equal to the test sequence length.
- Fails if the test sequence is longer than the training sequence, as the model has not learned embeddings for those positions.
- Best used when there is no obvious frequency information or fixed ordering in the inputs, and the structure of the positional relationships is unknown.

### Example: Vision Transformers (ViT)
- In Vision Transformers, images are split into patches, and each patch is assigned a learnable positional embedding.
- This embedding helps the transformer understand the spatial arrangement of patches (e.g., top-left, center, bottom-right).
- ViT has been successful in computer vision tasks by using these learnable embeddings to reason about the arrangement of image patches.

## Relative Positional Embeddings
Instead of encoding absolute positions, **relative positional embeddings** focus on the relationships between elements in the sequence:
- Example: The T5 language model uses relative positional embeddings by learning a bias inside the softmax of the attention mechanism.
- For each attention head in multi-head attention, a bias is learned based on the difference in positions between two elements ($m - n$).

### Advantages of Relative Positional Embeddings
- **Generalization**: They generalize better to new sequence lengths because they focus on relative positions rather than absolute ones.
- **Shift Invariance**: They capture the shift invariance seen in convolutional neural networks (CNNs), bringing this property into transformers.

### Variants of Relative Positional Embeddings
1. **ALiBi (Attention with Linear Biases)**:
   - Instead of learning a parameter for every pair of positional differences, ALiBi uses a scaled version of the difference between two positions.
   - This simplifies the learning process while maintaining the benefits of relative positional embeddings.
2. **Pairwise Encoding**:
   - Encodes the relative position directly into the keys and values (rather than as a bias in the attention mechanism).
   - The encoding ensures that the attention mechanism is sensitive to the relative positions of elements.

## Rotary Positional Embeddings (RoPE)
The lecture highlights **Rotary Positional Embeddings (RoPE)** as a powerful approach that combines the benefits of absolute and relative positional embeddings. RoPE is becoming the "one positional embedding to rule them all" in modern transformer architectures.

### Key Idea of RoPE
RoPE encodes absolute positions but results in relative positional embeddings during the attention computation:
- Instead of adding positional embeddings to the keys and values, RoPE multiplies the keys and queries with a rotation matrix derived from sine and cosine functions.
- This multiplication ensures that the attention mechanism captures the relative positional differences between elements.

### Mathematical Formulation of RoPE
The lecture provides the following details (supported by the first image):
- The goal is to find a kernel function $h(q_m, k_n)$ that depends on the relative position $m - n$:
  $
  h(q_m, k_n) = q_m^{\top} k_n = g(q_m, k_n, m - n)
  $
- The queries $q_m$ and keys $k_n$ are computed by multiplying the input embeddings $x_m$ and $x_n$ with a rotation matrix $R_m$:
  $q_m = R_m W^Q x_m, \quad k_n = R_n W^K x_n$

- The rotation matrix $R_m$ is a block-diagonal matrix composed of 2D rotation matrices: \\
  $
  R_m =
  \begin{bmatrix}
  \cos(m\theta_1) & -\sin(m\theta_1) & 0 & 0 & \cdots & 0 & 0 \\
  \sin(m\theta_1) & \cos(m\theta_1) & 0 & 0 & \cdots & 0 & 0 \\
  0 & 0 & \cos(m\theta_2) & -\sin(m\theta_2) & \cdots & 0 & 0 \\
  0 & 0 & \sin(m\theta_2) & \cos(m\theta_2) & \cdots & 0 & 0 \\
  \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
  0 & 0 & 0 & 0 & \cdots & \cos(m\theta_{c/2}) & -\sin(m\theta_{c/2}) \\
  0 & 0 & 0 & 0 & \cdots & \sin(m\theta_{c/2}) & \cos(m\theta_{c/2})
  \end{bmatrix}
  $
  where $\theta_i$ are the rotation angles, typically defined as $\theta_i = 10000^{-2i/c}$ (similar to sinusoidal embeddings).

### How RoPE Works

- When the attention mechanism computes $q_m^{\top} k_n$, the rotation matrices interact:
    $q_m^{\top} k_n = (R_m W^Q x_m)^{\top} (R_n W^K x_n) = (x_m^{\top} W^{Q\top} R_m^{\top}) (R_n W^K x_n)$

- The key insight is that $R_m^{\top} R_n = R_{m-n}$, because the product of two rotation matrices results in a rotation by the difference of their angles:
    $
    \begin{bmatrix}
    \cos(m\theta_i) & \sin(m\theta_i) \\
    -\sin(m\theta_i) & \cos(m\theta_i)
    \end{bmatrix}^{\top}
    $
    $
    \begin{bmatrix}
    \cos(n\theta_i) & -\sin(n\theta_i) \\
    \sin(n\theta_i) & \cos(n\theta_i)
    \end{bmatrix}
    $
    $=
    \begin{bmatrix}
    \cos((m-n)\theta_i) & -\sin((m-n)\theta_i) \\
    \sin((m-n)\theta_i) & \cos((m-n)\theta_i)
    \end{bmatrix}
    $
- Thus, the attention computation becomes:
    $
    q_m^{\top} k_n = x_m^{\top} W^{Q\top} R_{m-n} W^K x_n
    $
- The resulting embedding depends on $m - n$, making it a relative positional embedding, even though $R_m$ and $R_n$ are based on absolute positions.

### Why "Rotary"?
- The embeddings are called "rotary" because they represent rotations in 2D space (via the 2x2 rotation matrices).
- While extensions to higher dimensions have been explored, 2D rotations are often sufficient.

### Advantages of RoPE
The second image highlights RoPE's strengths:
- **Great Extrapolation Capability**:
  - RoPE can extrapolate to sequences longer than those seen during training.
  - The graph in the image shows that RoPE (labeled "LongRoPE") maintains stable performance in the "unseen range" (positions 4096 to 8192), while other methods like position interpolation degrade.
  - This is because the sine and cosine functions are cyclical, allowing RoPE to generalize to longer sequences.
- **Widely Adopted in LLMs**:
  - RoPE is used in large language models (LLMs) like LLaMA (as noted in the lecture).
  - The cyclical nature of sine and cosine embeddings allows RoPE to model relative interactions effectively without the need to learn additional parameters for relative positions.

### Practical Usage
- RoPE is implemented in state-of-the-art architectures to quickly obtain relative positional embeddings without the complexity of learning pairwise interactions.
- It provides a balance between absolute positional encoding (easy to implement) and relative positional encoding (better generalization).

## Applications of Positional Embeddings
### 1. Transformers and Large Language Models
- Positional embeddings are combined with input embeddings and fed into the transformer:
  - Often, positional embeddings are added not only at the input but also later in the attention mechanism.
- RoPE and its variants are increasingly used in modern LLMs (e.g., LLaMA family) for their ability to handle long sequences and model relative positions effectively.

### 2. Implicit Functions
- Positional embeddings are also used in **implicit functions**, which map coordinates to outputs (e.g., color values in an image or occupancy in 3D space):
  - Example: In Neural Radiance Fields (NeRF), positional embeddings are used to encode 3D coordinates $(x, y, z)$ to determine if a point in space is occupied and its color.
  - Sinusoidal positional embeddings (or their variants, like Fourier Features) are commonly used in implicit functions because they allow the network to capture high-frequency details.
- The lecture mentions a paper on "Fourier Features" that explores what can be encoded using positional embeddings in implicit functions.

### Duality Between Applications
- Positional embeddings developed for implicit functions (e.g., sinusoidal embeddings) can be applied to transformers, and vice versa, showing a nice duality between the two domains.

## Summary of Positional Embeddings
- **Purpose**: Positional embeddings break the permutation invariance of attention, allowing transformers to reason about the relationships between inputs based on their positions.
- **Types**:
  - **Absolute**: Sinusoidal embeddings (sines and cosines) or simple enumeration.
  - **Learnable**: Weights assigned to each position, optimized during training.
  - **Relative**: Focus on the difference between positions (e.g., T5, ALiBi, pairwise encoding).
  - **Rotary (RoPE)**: Combines absolute and relative embeddings, widely used in modern LLMs.
- **Applications**:
  - In transformers (e.g., NLP, vision models like ViT).
  - In implicit functions (e.g., NeRF for 3D modeling).
- **Recommendation**:
  - For transformer-based architectures, RoPE is the go-to choice due to its balance of absolute and relative positional encoding.
  - For implicit functions, sinusoidal or Fourier-based embeddings are preferred to capture high-frequency details.

This comprehensive overview covers the evolution of positional embeddings, their mathematical foundations (especially for RoPE), and their practical applications in modern machine learning architectures.