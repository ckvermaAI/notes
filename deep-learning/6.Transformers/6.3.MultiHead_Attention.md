# Multi-Head Attention

## Introduction to Multi-Head Attention
- **Definition and Context**: Multi-head attention is presented as an extension of the basic attention operator, commonly referred to simply as "attention" in practice. The lecturer emphasizes that when people mention "attention" in neural networks, they typically mean multi-head attention, as the basic attention operator is rarely used in its simplest form.
- **Purpose**: Multi-head attention enhances the capability of attention mechanisms, enabling more flexible and expressive computations over sets of data, such as sequences or images.

## Recap of Basic Attention Operator
- **Components**: The basic attention operator involves three key elements:
  - **Keys (K)**: Represent the elements being attended to.
  - **Values (V)**: Represent the information associated with the keys.
  - **Queries (Q)**: Represent the input seeking to attend to specific keys.
- **Mechanism**: It applies a softmax function to compute attention scores between queries and keys, then uses these scores to weight and average the values for each query. Mathematically, this can be expressed as:
  - $Attention(Q, K, V) = softmax(QK^T)V$
  - Here, $QK^T$ computes the similarity between queries and keys, and $softmax$ normalizes these scores.
- **Self-Attention**: The lecture focuses on self-attention, where $Q = K = V$, meaning the input itself serves as queries, keys, and values.

## Limitations of Basic Self-Attention
- **Cosine Distance Constraint**: The lecturer highlights a fundamental limitation in basic self-attention using cosine distances:
  - For any pair of elements $x$ and $y$, and a third element $z$, the pairwise distance between $x$ and $y$ must be less than half the distance from $x$ to itself or $y$ to itself.
  - Mathematically, this implies: $d(x, y) < \frac{1}{2}(d(x, x) + d(y, y))$, where $d(x, x) = 0$ in cosine distance (since an element is identical to itself).
- **Consequence**: This constraint forces the attention mechanism to always assign the highest attention score to the element itself (the diagonal of the attention matrix dominates). Thus, an element cannot attend more to another element than to itself, limiting the flexibility of the operator.
- **Issue**: This self-dominance restricts the ability of basic self-attention to perform arbitrary reasoning over sets, as it cannot prioritize external elements over the input element.

## Solution: Adding Linear Layers
- **Approach**: To overcome this limitation, the input $X$ is transformed using linear layers before applying attention:
  - Three weight matrices are introduced: $W_Q$ (for queries), $W_K$ (for keys), and $W_V$ (for values).
  - Transformed inputs: $Q = XW_Q$, $K = XW_K$, $V = XW_V$.
- **Effect**: These linear transformations allow queries, keys, and values to differ arbitrarily from the original input $X$, breaking the self-dominance constraint. Now, attention can focus more on other elements than the element itself.
- **Enhanced Flexibility**: This enables attention to attend to arbitrary locations (e.g., in an image or sequence) rather than being biased toward the input element.
- **Additional Benefit**: The inclusion of weight matrices makes attention resemble a linear layer:
  - If the attention mechanism (softmax and key-query interaction) were an identity function, the remaining $W_V$ acts as a linear transformation, akin to a $1 \times 1$ convolution in convolutional neural networks (CNNs).
  - This equivalence adds expressiveness comparable to a convolution with a $1 \times 1$ kernel.

## Limitation of Single-Head Attention
- **Single Focus**: Even with linear layers, a single attention mechanism uses one shared attention matrix, restricting it to focus on one location or average information from multiple locations without reasoning about them independently.
- **Lack of Disjoint Reasoning**: It cannot process information from multiple distinct regions separately, limiting its representational power.

## Multi-Head Attention: The Solution
- **Concept**: Multi-head attention addresses this by running multiple attention mechanisms (heads) in parallel:
  - Each head has its own set of weight matrices: $W_Q^i$, $W_K^i$, $W_V^i$ for head $i$.
  - For input $X$, each head computes: $head_i = Attention(XW_Q^i, XW_K^i, XW_V^i)$.
- **Concatenation**: The outputs of all heads are concatenated:
  - $MultiHead(X) = Concat(head_1, head_2, ..., head_h)$, where $h$ is the number of heads.
- **Output Transformation**: A final linear layer with weight matrix $W_O$ is applied:
  - $MultiHead(X) = Concat(head_1, head_2, ..., head_h)W_O$.
- **Purpose of $W_O$**: This layer projects the concatenated output back to a desired dimensionality, enhancing computational efficiency:
  - Initial linear layers ($W_Q^i$, $W_K^i$, $W_V^i$) can reduce dimensionality per head.
  - Concatenation increases dimensionality (proportional to $h$).
  - $W_O$ adjusts the final output size, avoiding excessive computational cost.

## Why Two Linear Layers?
- **Theoretical Note**: Attention is linear in the values ($V$), so theoretically, $W_V^i$ and $W_O$ could cancel out. However, in practice:
  - **Efficiency**: Pre-attention linear layers project inputs to lower dimensions per head, reducing computation in the attention operation. Post-attention $W_O$ combines and adjusts the output.
  - **Flexibility**: This structure allows the model to learn richer representations by separating transformations before and after attention.

## Expressiveness and Relation to Convolution
- **Key Insight**: Multi-head attention is mathematically as expressive as convolution:
  - With $h$ heads, it can match the expressiveness of a 2D convolution with kernel size $\sqrt{h} \times \sqrt{h}$.
- **Intuition**: Each head can specialize to attend to a specific position in a convolutional kernel:
  - E.g., one head attends to the element above, another to the right, etc., mimicking a kernel’s spatial coverage.
  - The linear transformations ($W_V^i$ and $W_O$) replicate the linear projection of a convolutional filter.
- **Proof Sketch**: The lecturer references a diagram (not provided) showing how attention heads can be arranged to cover a kernel’s receptive field, with linear layers handling the weighting.
- **Comparison**:
  - **Efficiency**: Convolution is more efficient for tasks where spatial locality is key (e.g., image processing), as it directly encodes this structure.
  - **Data and Computation**: Multi-head attention requires more data and computation to learn equivalent operations, as it lacks convolution’s inductive bias.

## Recap and Conclusion
- **Definition**: Multi-head attention is the standard "attention" in neural networks, involving:
  - Linear layers transforming inputs into multiple parallel attention operations.
  - Concatenation of head outputs, followed by an output linear layer.
- **Expressiveness**: With $h$ heads, it matches a $\sqrt{h} \times \sqrt{h}$ convolution; with $h=1$, it equates to a $1 \times 1$ convolution.
- **Ubiquity**: When networks mention "attention," they imply multi-head attention due to its superior flexibility and power over basic attention.

---

## Additional Notes
- **Contextual Relevance**: Multi-head attention is a cornerstone of Transformer models (e.g., BERT, GPT), introduced in the paper "Attention is All You Need" (Vaswani et al., 2017). This lecture aligns with that framework.
- **Practical Insight**: The number of heads ($h$) is a hyperparameter (e.g., 8 or 12 in Transformers), balancing expressiveness and computational cost.

This summary captures every point from the transcript, enriched with clarifications and context, making it a comprehensive resource on multi-head attention.