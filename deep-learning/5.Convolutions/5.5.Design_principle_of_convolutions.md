# Design Principles of Convolutional Networks

## Recap of Convolution and CNN Basics
- **Convolution Overview**: 
  - Convolution is a **memory-efficient** and **fast operation** for processing images, preserving their spatial structure, which makes it ideal for image-related tasks.
- **CNN Components**: Previously covered topics include:
  - **Building Blocks**: Combining convolution, non-linearities (e.g., ReLU), and normalization into larger structures like **residual blocks**.
  - **Striding**: Adjusting resolution and channel numbers through strided convolutions.
- **Parameters**:
  - **Kernel Size**: Small sizes (e.g., $3 \times 3$) are preferred.
  - **Input/Output Channels**: Typically kept equal across layers unless striding occurs.
  - **Padding**: Often set to half the kernel size (e.g., 1 for a $3 \times 3$ kernel) to preserve spatial dimensions.

---

## Key Design Question: When and How to Stride?
- **Central Issue**: While many CNN parameters (e.g., kernel size, padding) are relatively fixed, deciding **when to stride** and how to structure the network remains a critical choice.
- **Striding Mechanics**:
  - **Stride Factor**: Typically 2 (skipping every other output), though other factors are rare.
  - **Effect**: Reduces spatial dimensions (width and height) by half ($W/2 \times H/2$) while doubling the number of channels ($2C$) to maintain computational capacity.
  - **Activation Impact**: The total number of activations shrinks by half with each stride. For an input of size $C \times H \times W$, after striding it becomes $2C \times H/2 \times W/2$, reducing activations from $C \cdot H \cdot W$ to $C \cdot H \cdot W / 2$.
- **Trade-Off**: 
  - **Benefit**: Reduces computational load and memory usage.
  - **Drawback**: Information loss occurs with each stride, as spatial resolution decreases.

---

## Special Role of the First Layer
- **Purpose**: The first layer is unique and designed to mitigate information loss while setting up the network for subsequent processing.
- **Channel Expansion**: 
  - Takes an input with few channels (e.g., 3 for RGB images) and "blows it up" to 60–90 channels.
  - This projects the input into a higher-dimensional space for richer feature extraction.
- **Kernel Size**: 
  - Larger than typical layers, ranging from $7 \times 7$ to $16 \times 16$, to capture broader context around each pixel.
  - A small kernel (e.g., $3 \times 3$) would limit the ability to meaningfully expand from a single pixel's value.
- **Striding**: 
  - Applies a stride of 2 or more to immediately reduce spatial dimensions (e.g., from $1000 \times 1000$ to $500 \times 500$).
  - Prevents an explosion of activations (e.g., $1000 \times 1000 \times 3 = 3M$ pixels could become $1000 \times 1000 \times 90 = 90M$ without striding).
- **Example**: For a $1000 \times 1000 \times 3$ image, the first layer might use a $7 \times 7$ kernel, stride 2, and output 64 channels, yielding a $500 \times 500 \times 64$ feature map.

---

## General Network Design Principles
- **Post-First Layer Strategy**:
  - After the initial channel expansion and resolution reduction, the network applies **regular convolutions**.
  - Spatial dimensions are progressively shrunk via striding, while channels grow (e.g., 64 → 128 → 256).
- **Kernel Size Preference**:
  - **Small Kernels**: Use $3 \times 3$ or $1 \times 1$ convolutions in most layers.
  - **Avoid Large Kernels**: Sizes like $5 \times 5$ or $11 \times 11$ are computationally wasteful.
  - **Efficiency**: A single $11 \times 11$ kernel (121 parameters) can be approximated by three $3 \times 3$ kernels (27 parameters total) applied sequentially, with non-linearities (e.g., ReLU) between them enhancing expressiveness.
- **Exception**: The first layer can (and often should) use larger kernels due to its unique role.

---

## Repeating Blocks
- **Pattern Development**: CNNs rely on repeating **blocks** of operations rather than designing each layer independently.
- **Example Block**: 
  - Convolution ($3 \times 3$) → ReLU → Convolution ($1 \times 1$) → ReLU → Normalization (e.g., batch norm) → Skip Connection (residual link).
- **Striding Within Blocks**: 
  - One layer in the block (e.g., the first or last convolution) may stride to reduce spatial dimensions and increase channels.
  - Exact placement of the stride is flexible.
- **Advantages**:
  - **Time-Saving**: Reusing a tested block simplifies design and tuning.
  - **Debugging**: Fixing a bug in one block applies network-wide.
  - **Tuning**: Consistent structure eases hyperparameter optimization.

---

## All-Convolutional Architecture
- **Avoid Linear Layers**: Fully connected (linear) layers have too many parameters (e.g., $C \cdot H \cdot W$ inputs to $K$ outputs yields $C \cdot H \cdot W \cdot K$ weights) and are prone to overfitting.
- **End-to-End Convolution**: Use only convolutional layers throughout the network.
- **Final Stage**:
  - Apply a classifier (e.g., $1 \times 1$ convolution) to predict per-spatial-location outputs (e.g., class scores across the feature map).
  - Use **global average pooling** to collapse spatial dimensions ($C \times H \times W \to C$), yielding a final prediction.
- **Benefits**:
  - **Interpretability**: Classifying before averaging allows inspection of spatial predictions (e.g., “this region thinks it’s a cat, that region thinks it’s a dog”).
  - **Training Efficiency**: Convolutional layers share weights across spatial locations, providing more training signal and better generalization compared to linear layers.

---

## Summary of Design Principles
- **First Layer**: Special handling with large kernels (e.g., $7 \times 7$), significant channel expansion (e.g., 3 → 64), and striding (e.g., 2) to reduce input size and set up subsequent layers.
- **Subsequent Layers**: Use small kernels ($3 \times 3$ or $1 \times 1$), repeat modular blocks, and stride periodically to shrink spatial dimensions while growing channels.
- **All-Convolutional**: Avoid linear layers, relying on convolutions until the end, where averaging produces the final output.
- **Core Strategy**: Increase channels, decrease spatial dimensions via striding, keep kernels small, and repeat patterns for efficiency and effectiveness.

---

## Additional Insights
- **Historical Context**: These principles align with modern CNN architectures like ResNet, which popularized small kernels, residual blocks, and all-convolutional designs.
- **Practical Note**: The emphasis on $3 \times 3$ kernels stems from their balance of receptive field size and parameter efficiency, a finding solidified by networks like VGG and ResNet.
