# Deep Representations

This lecture explores the concept of **deep representations** in convolutional neural networks (CNNs), focusing on what these networks learn as they process images. It builds on prior discussions about image structure and convolution’s suitability for capturing spatial patterns. The lecture examines two key approaches to understanding CNNs: analyzing **activations** within the network and visualizing **decision-making regions** in the input image. 

---

## Recap: Images and Convolution
- **Image Structure**: Images consist of patterns where nearby objects form continuous pixel regions, though discontinuities (e.g., occlusions) can occur. CNNs are well-suited to this structure due to their local, sliding-window operations.
- **Convolution Fit**: Convolutions exploit spatial proximity, making them effective for image processing by capturing relationships between neighboring pixels.

---

## Research Question: What Do CNNs Learn?
- **Objective**: Understand how CNNs represent images internally (via activations) and how they make decisions (via attention to image regions).
- **Two Approaches**:
  1. **Activations**: Study what individual units (neurons) in the network respond to.
  2. **Decision Focus**: Identify which parts of the image influence the network’s final prediction.

---

## Approach 1: Visualizing Activations
- **Method**: Correlate activations in various layers with specific classes or labels from a dataset (e.g., “cat,” “dog,” “sky”).
- **Study Highlight**: The lecture references **Network Dissection**, a technique that:
  - Records activations across all layers.
  - Identifies the top-$p$ activations (highest responses) for each unit.
  - Correlates these peak responses with ground-truth labels to determine what each unit detects.
- **Layer-by-Layer Findings**:
  - **Early Layers**: 
    - Units respond to basic features like **colors** (e.g., red, yellow, blue).
    - Example: Some units specialize in detecting sky-like objects based on blue color and low-level patterns.
  - **Next Layers**: 
    - Units assemble colors into **textures** (e.g., woven patterns, banded textures, grids).
  - **Middle Layers**: 
    - Textures combine into **primitive objects** (e.g., sky, fruits with unique textures like oranges).
  - **Higher Layers (e.g., Conv4)**: 
    - Note: “Conv4” refers to the fourth convolutional block, not necessarily the fourth layer.
    - Units detect **object parts** (e.g., faces, swirly textures, dog features—due to abundant dog images in datasets).
  - **Highest Layers**: 
    - Units respond to **entire objects** (e.g., cats, human legs, wheels), reflecting complex feature hierarchies.
- **Hierarchy Insight**: CNNs build representations progressively:
  - Low-level: Colors ($R, G, B$).
  - Mid-level: Textures and simple objects.
  - High-level: Object parts and full objects.
- **Limitation**: 
  - Channels are not independent; activations are reused across classes in a complex, interdependent manner, making this a partial view of the network’s behavior.

---

## Approach 2: Understanding Decision Focus
- **Objective**: Determine which image regions the network “looks at” to make its final prediction.
- **Two Methods**: 
  1. **Class Activation Maps (CAM)** for fully convolutional networks.
  2. **Grad-CAM** for more complex architectures.

### Class Activation Maps (CAM)
- **Concept**: 
  - In a fully convolutional network (FCNN), the final classification layer produces a spatial map of predictions (e.g., $C \times H \times W$, where $C$ is the number of classes).
  - The receptive field slides over the image, assigning confidence scores to different regions.
- **Examples**:
  - **Brushing Teeth**: The network is most confident when centered on the toothbrush.
  - **Cutting Trees**: Confidence peaks around the person, tree, or chainsaw.
- **Implementation**:
  - If the network is fully convolutional, the last layer naturally provides a spatial heatmap of predictions.
  - **Modification for Non-FCNNs**: 
    - If average pooling and a linear classifier follow the convolutional layers, replace them with a $1 \times 1$ convolution followed by average pooling to maintain spatial information.
  - **Limitation**: Early networks with multiple linear layers, dropout, and ReLUs cannot use CAM directly due to their complexity.

### Grad-CAM (Gradient-weighted Class Activation Mapping)
- **Concept**: 
  - Extends CAM to handle complex networks by using gradients.
  - Computes the gradient of a specific class output with respect to intermediate activations (e.g., from a convolutional layer).
  - Visualizes the **magnitude of these gradients** as a heatmap, highlighting regions that most influence the prediction.
- **Process**:
  1. Forward pass: Obtain the network’s class scores.
  2. Average the spatial dimensions of the final layer to get a single class score.
  3. Backward pass: Compute gradients of this score with respect to an earlier layer’s activations.
  4. Visualize the gradient magnitudes as a heatmap.
- **Insight**: 
  - High-gradient regions indicate where changes in the input most affect the output (e.g., “this area drives the ‘cat’ prediction”).
- **Advantage**: Works with any network architecture, unlike CAM, which requires a fully convolutional design.

---

## Holistic Understanding
- **Activations**: Reveal the hierarchical buildup of features (colors → textures → objects), but miss the interdependence of channels.
- **Decision Focus**: 
  - CAM shows where the network looks in a fully convolutional setup.
  - Grad-CAM generalizes this to complex networks via gradient analysis.
- **Complementary Views**:
  - Activations dissect internal representations.
  - CAM/Grad-CAM highlight external attention, answering “what matters for the decision?”

---

## Additional Context
- **Network Dissection**: Introduced by Bau et al. (2017), this method quantifies interpretability by aligning activations with human-understandable concepts, often using datasets like Broden.
- **CAM**: Proposed by Zhou et al. (2016), it relies on global average pooling before classification to generate spatial heatmaps.
- **Grad-CAM**: Developed by Selvaraju et al. (2017), it improves on CAM by leveraging gradients, making it widely applicable in modern CNN analysis (e.g., ResNet, VGG).
- **Practical Use**: These techniques are critical for debugging, interpreting model behavior, and improving trust in AI systems (e.g., in medical imaging).

---

## Summary
- **What CNNs Learn**: 
  - Early layers capture low-level features (colors, edges).
  - Middle layers detect textures and simple objects.
  - Later layers recognize object parts and full objects.
- **Analysis Methods**:
  - **Activations (Network Dissection)**: Show the progression of learned patterns but oversimplify channel interactions.
  - **CAM**: Maps decision confidence spatially in FCNNs, requiring architectural adjustments for non-FCNNs.
  - **Grad-CAM**: Uses gradients for a universal, gradient-based heatmap of decision focus.
- **Takeaway**: CNNs construct deep representations by layering simple features into complex objects, and visualization tools like CAM and Grad-CAM reveal both internal learning and external focus, providing a dual perspective on network behavior.
