# Fully Connected Networks

## Introduction
This lecture explores **Fully Connected Networks (FCNs)**, their fundamental properties, and their limitations. FCNs, also known as **dense networks**, are essential components in deep learning models but come with computational and data efficiency challenges.

---

## Fully Connected Deep Networks

### **Definition**
Fully connected networks are neural networks where:
- Every **input node** is connected to every **output node**.
- This property extends to every layer, meaning each neuron in one layer influences all neurons in the next.

### **Mathematical Representation**
A fully connected layer performs a **linear transformation**: $y = Wx + b$
where:
- $W$ is a **weight matrix** (size: number of inputs × number of outputs),
- $x$ is the **input vector**,
- $b$ is a **bias term**.

### **Key Components**
- **Non-linearities**: Activation functions like ReLU, ELU, GELU, etc.
- **Normalization & Residual Connections**: Help stabilize training and improve gradient flow.
- **Learned Parameters**: The weights and biases are learned via backpropagation.

### **Operations**
- **Matrix Multiplication**: Performed in each fully connected layer.
- **Fully Connected Layers**: These layers map input features to outputs, with each neuron affecting the entire feature space.

---

## **Limitations of Fully Connected Networks**
Despite their expressiveness, fully connected networks suffer from three major drawbacks:

### **1. Computational Inefficiency**
- Every neuron connects to every other neuron in the next layer, leading to **large weight matrices**.
- As a result, FCNs require high **computational power** for training and inference.

### **2. Memory Inefficiency**
- Due to the fully connected nature, the number of **trainable parameters** grows rapidly.
- Storing large matrices consumes **significant memory**, often exceeding GPU capabilities.

### **3. Data Inefficiency**
- FCNs require **a large amount of training data** to generalize well.
- The high number of parameters makes them prone to **overfitting**, meaning they perform well on training data but fail on unseen data.

---

## **Towards More Efficient Architectures**
Since FCNs are inefficient, researchers have explored **better parameterization strategies**, leading to architectures like:
1. **Convolutional Neural Networks (CNNs)** – Introduce spatial locality and weight sharing.
2. **Transformers** – Use self-attention mechanisms for sequential data.
3. **Sparse Neural Networks** – Reduce unnecessary connections.

### **Comparison: FCNs vs. Convolutions**
- **FCNs**: Large weight matrices, fully connected structure.
- **Convolutions**: Efficient weight sharing, fewer parameters, and localized operations.

Convolutions allow us to **capture data structure efficiently**, leading to major improvements in modern deep learning models.

---

## **Conclusion**
While fully connected networks are fundamental to deep learning, their limitations in **efficiency, memory usage, and data requirements** make them suboptimal for large-scale tasks. More advanced architectures like **CNNs and Transformers** address these inefficiencies, providing better performance across various applications.

This lecture sets the stage for understanding **convolutions**, which will be discussed in the next segment.
