# Structure of Convolutions in Deep Learning

## Introduction to Convolutional Networks
The lecture begins by building on the previous discussion of convolutions as a spatially anchored, memory-efficient linear operator that preserves image structure through a sliding window mechanism. It explores how convolutions can form the backbone of deep neural networks by replacing traditional linear layers. A basic convolutional network alternates between convolutions and non-linearities (e.g., ReLU), potentially incorporating normalization (e.g., BatchNorm) and residual connections for deeper architectures. The output might be a smaller image or a single value (e.g., via averaging), depending on the task.

## Issue 1: Shrinking Inputs with Vanilla Convolution
### Problem Description
- **Shrinkage**: Vanilla convolution reduces the spatial dimensions of the input with each layer. For a kernel of size $k \times k$ applied to an input of size $H \times W$, the output size becomes $(H - k + 1) \times (W - k + 1)$.
- **Example**: With 4 input pixels in width and a kernel size of 2, the kernel fits 3 times, reducing the output width to 3. Each layer further shrinks the image, which is undesirable as it complicates network design and limits depth.

### Solution: Padding
- **Concept**: Padding artificially enlarges the input by adding border pixels, typically with a value of 0, before applying convolution.
- **Initial Attempt**: Padding on one side only (e.g., adding 1 pixel to one end for a kernel size of 2) maintains the original size (e.g., 4 to 4), but shifts the image content (e.g., toward the top-left corner).
- **Improved Solution**: Pad all sides equally to avoid shifting.
  - **Formula**: For input size $H \times W$, kernel size $k_h \times k_w$, and padding $p_h$ (height) and $p_w$ (width) on all sides:
    $H_{out} = H + 2p_h - k_h + 1$, $W_{out} = W + 2p_w - k_w + 1$
  - **Goal**: Set $p_h$ and $p_w$ so $H_{out} = H$ and $W_{out} = W$.
  - **Rule**: $2p_h + 1 = k_h$, $2p_w + 1 = k_w$, or $p_h = (k_h - 1)/2$, $p_w = (k_w - 1)/2$.
- **Kernel Size Constraint**: Kernel sizes must be odd (e.g., 1, 3, 5) to ensure integer padding values. Even kernel sizes (e.g., 2, 4) lack a padding that preserves size symmetrically.
- **Examples**:
  - Kernel size 3 → $p = (3-1)/2 = 1$
  - Kernel size 5 → $p = (5-1)/2 = 2$
  - Kernel size 1 → $p = 0$
- **Recommendation**: Always use padding with odd kernel sizes to maintain input dimensions in convolutional layers.

## Building Convolutional Networks
With padding, convolutional networks avoid unwanted shrinkage, allowing replacement of linear layers without worrying about diminishing spatial dimensions. However, additional considerations arise as networks deepen.

## Issue 2: Computational Cost and Channel Growth
### Channel Increase
- **Rationale**: Deep networks pack spatial information into channels, increasing the number of output channels ($C_{out}$) across layers to capture complex patterns and relationships.
- **Problem**: Vanilla convolution’s computational cost scales with:
  $H \times W \times k_h \times k_w \times C_{in} \times C_{out}$
  As $C_{out}$ grows while $H$ and $W$ remain constant (due to padding), computation becomes prohibitively expensive.

### Solution: Strided Convolution
- **Concept**: Striding skips positions during the sliding window, downsampling the output spatially.
- **Mechanism**: Instead of moving the kernel one pixel at a time, it moves by a stride $s_h$ (height) and $s_w$ (width).
- **Output Size**: For input $H \times W$, kernel $k_h \times k_w$, padding $p_h, p_w$, and stride $s_h, s_w$:
  $H_{out} = \lfloor (H + 2p_h - k_h) / s_h \rfloor + 1$, $W_{out} = \lfloor (W + 2p_w - k_w) / s_w \rfloor + 1$
- **Example**: Stride of 2 halves the output dimensions (e.g., 4 to 2 with no padding and kernel size 1).
- **Advantages**:
  - **Efficiency**: Reduces $H$ and $W$, lowering the cost of subsequent layers by a factor of $s_h \times s_w$.
  - **Receptive Field**: Increases the receptive field of later layers by the stride factor, enabling broader context awareness (e.g., a stride-2 layer doubles the effective area seen by subsequent kernels).
- **Rounding Issue**: If $(H + 2p_h - k_h)$ isn’t divisible by $s_h$, the output is rounded down, cropping edge pixels. This is minor in practice due to large receptive fields in deep networks.

## Further Optimization: Group and Depthwise Convolutions
### Motivation
Even with striding, convolutional networks may remain too slow for resource-constrained environments (e.g., mobile devices).

### Group Convolution
- **Concept**: Split input channels into $G$ groups, applying a separate convolution to each group without inter-group communication.
- **Cost Reduction**: Reduces computation by a factor of $G$, as each group processes $C_{in}/G$ to $C_{out}/G$ channels.
- **Specialization**: Encourages channel groups to focus on distinct features.

### Depthwise Convolution
- **Special Case**: $G = C_{in} = C_{out}$, where each input channel is processed independently with its own spatial filter.
- **Computation**: $H \times W \times k_h \times k_w \times C_{in}$, significantly less than standard convolution.
- **Limitation**: No channel mixing, problematic for RGB inputs (e.g., red, green, blue processed separately).
- **Solution**: Pair with a 1x1 convolution (pointwise convolution):
  - Depthwise: $H \times W \times k_h \times k_w \times C_{in}$
  - 1x1: $H_{out} \times W_{out} \times C_{in} \times C_{out}$
  - Total cost reduced while enabling channel interaction.
- **Efficiency**: Widely used in lightweight models (e.g., MobileNet).

## Designing Convolutional Networks
### Hyperparameters
Convolution introduces several parameters: output channels ($C_{out}$), kernel size ($k_h, k_w$), padding ($p_h, p_w$), stride ($s_h, s_w$), and groups ($G$). This complexity seems daunting compared to linear layers, but practical choices simplify design.

### Recommended Choices
- **Stride**:
  - Binary decision: $s = 1$ (no downsampling) or $s = 2$ (downsample by 2).
  - Higher strides (e.g., 3) are rarely justified.
  - Apply uniformly ($s_h = s_w$).
- **Kernel Size**:
  - Odd values only (1 or 3 for most layers; 5 occasionally for the first layer).
  - Larger kernels increase computation without proportional benefit.
- **Padding**: Determined by kernel size: $p = (k-1)/2$ to preserve input size when not striding.
- **Output Channels**:
  - Start with a base value (e.g., 32 or 64).
  - Increase by the stride factor (e.g., double when $s = 2$) to maintain computational balance.
  - Rationale: $H$ and $W$ shrink by $s$, while $C_{in}$ and $C_{out}$ grow by $s$, keeping $H \times W \times C_{in} \times C_{out}$ roughly constant.
- **Groups**: Typically ignored in this class; use depthwise + 1x1 convolution only if optimization is critical.

### Network Structure
- **Pattern**: Alternate convolution, non-linearity (e.g., ReLU), optional normalization, and residuals.
- **Strategic Striding**: Decide where to stride to:
  - Reduce spatial dimensions ($H, W$).
  - Pack information into channels ($C_{out}$).
  - Enlarge receptive fields for global context.
- **Simplified Design**: Focus on stride (1 or 2) and kernel size (1 or 3), letting padding and channels follow logically.

## Conclusion
- **Padding**: Prevents shrinkage, controlled by kernel size (e.g., $p = (k-1)/2$ for odd $k$).
- **Striding**: Reduces spatial size purposefully, boosts efficiency, and expands receptive fields (e.g., $s = 2$ halves dimensions).
- **Groups/Depthwise**: Cuts computation cost (e.g., depthwise + 1x1 for channel mixing).
- **Takeaway**: With padding and striding, convolutional networks manage input size and computational load effectively, enabling deep, practical architectures.
