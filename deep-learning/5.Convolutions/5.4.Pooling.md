# Pooling

## Convolution Recap
- **Definition**: Convolution is an operator that involves sliding a window (called a **kernel**) over an image or image-like input.
- **Linear Transformation**: At each position, a linear transformation is applied to the patch of the image covered by the kernel. This is typically represented as $wx + b$, where $w$ is the weight (kernel values), $x$ is the input patch, and $b$ is a bias term.
- **Parameters/Hyperparameters**:
  - **Stride**: The step size with which the kernel moves across the image.
  - **Kernel Size**: The dimensions of the window (e.g., $H \times W$).
  - **Output Channels**: The number of feature maps produced after convolution.
  - **Groups**: A parameter that divides input channels into groups for separate processing (specific to linear operations).
  - **Padding**: Extra pixels (usually zeros) added to the image borders to control output size, often tied to stride and kernel size.
- **Mathematical View**: Convolution extracts a patch of size $H \times W$, flattens it into a vector, and applies a linear transformation to produce an output that retains spatial structure.

---

## Transition to Pooling
- **Generalization**: While convolution relies on linear transformations, there’s no inherent reason to restrict operations on patches to linearity. Any arbitrary function $f$ could theoretically be applied to these patches.
- **Process**: 
  1. Extract a patch from the image.
  2. Flatten it into a vector.
  3. Apply a function $f$ (not necessarily linear).
  4. Place the output back into a spatial location in the resulting feature map.
- **Pooling Definition**: Pooling extends convolution by replacing the linear transformation with arbitrary functions, enabling non-linear operations on image patches.

---

## Pooling Basics
- **Parameters**:
  - **Stride**: Similar to convolution, controls how the window moves.
  - **Kernel Size**: Defines the patch size (e.g., $2 \times 2$ or $3 \times 3$).
  - **Padding**: Can be applied to handle border effects.
  - **Removed Parameters**: Unlike convolution, pooling does not involve **output channels** or **groups**, as these are tied to linear transformations.
- **Key Idea**: Pooling operates on patches but uses functions other than linear transformations, making it a flexible operator.

---

## Types of Pooling
### 1. Max Pooling
- **Function**: The function $f$ computes the maximum value within each patch, applied channel-wise (i.e., independently for each input channel).
- **Example**:
  - Kernel size $2 \times 2$: Takes the maximum value from a $2 \times 2$ patch.
  - Kernel size $3 \times 3$: Takes the maximum from a $3 \times 3$ patch.
- **Padding**:
  - Can pad with zeros, but this may affect the max if zero is the largest value.
  - Alternatively, padding with a value close to negative infinity ensures the padding is never selected as the maximum.
- **Non-Linearity**: Max pooling is a **non-linear operator**, which introduces non-linearity into a network. This property allows it to be combined with convolutions to build deep networks without relying on traditional non-linear activation functions (e.g., ReLU).
- **Historical Use**: Early CNNs used max pooling with stride to reduce spatial dimensions, rather than strided convolutions.

### 2. Average Pooling
- **Function**: The function $f$ computes the mean (average) of all pixel values within a patch.
- **Linear Nature**: This can be expressed as a convolution where the kernel weights are $1/(H \times W)$, normalizing the sum of the patch values. Thus, it is a **linear operation**.
- **Implementation**: Despite its linearity, average pooling is often implemented as a distinct operation for simplicity and clarity in network design.

---

## Global Pooling
- **Definition**: A variant of pooling where the entire input (e.g., the full image) is flattened, and a single function $f$ is applied to produce a non-spatial output.
- **Relation to Regular Pooling**: Equivalent to regular pooling with a kernel size equal to the entire input dimensions (e.g., the full $H \times W$ of the image).
- **Parameters**: No stride, kernel size, or padding—just a single global operation.
- **Output**: Compresses spatial inputs (e.g., images) into a single value or vector, removing all spatial dimensions.
- **Examples**:
  - **Global Average Pooling**: Averages all values across the spatial dimensions, often used at the end of a CNN to produce a single prediction (e.g., for image classification).
  - **Global Max Pooling**: Takes the maximum value across the input, useful in tasks like point cloud processing where inputs have variable sizes but need to be condensed into a fixed-size representation.

---

## Historical Context and Evolution
- **Past Popularity**: Pooling, especially max pooling, was widely used in early CNNs to:
  - Reduce spatial dimensions (e.g., halving width and height with a $2 \times 2$ kernel and stride 2).
  - Introduce non-linearity (via max pooling).
- **Experimentation**: Research showed that deep networks could be built using only convolutions and max pooling, achieving performance close to the state-of-the-art around 2017 (eight years prior to 2025). These networks alternated between convolution and max pooling layers without pointwise non-linearities like ReLU.
- **Decline in Use**:
  - **Regular Pooling**: Rarely used today. Modern CNNs prefer strided convolutions with pointwise non-linearities (e.g., ReLU) to reduce dimensions and introduce non-linearity.
  - **Reason**: Network architectures have simplified, and alternative structures (e.g., residual connections, transformers) have emerged, reducing the need for pooling.
- **Global Pooling’s Persistence**: Still used in specific cases:
  - At the end of CNNs for classification (global average pooling).
  - In point cloud processing (global max pooling) to handle variable-size inputs.

---

## Summary
- **Pooling Overview**: Pooling is a convolution-like operator that slides a window over an image and applies an arbitrary function $f$ to each patch. It generalizes convolution by allowing non-linear operations.
- **Types**: Max pooling (non-linear), average pooling (linear), and global pooling (spatial compression).
- **Current Trends**: While pooling was historically significant, its use has diminished. Regular pooling is largely replaced by strided convolutions, but global pooling retains niche applications.
- **Takeaway**: Convolution with linear operators remains dominant, but pooling’s flexibility with arbitrary functions offers insights into the evolution of neural network design.
