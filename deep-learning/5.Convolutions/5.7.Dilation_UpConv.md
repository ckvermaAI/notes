# Dilation and Up-Convolution

## Recap: Receptive Field in Convolutional Networks
- **Context**: The lecture assumes familiarity with convolutions and striding, previously covered topics aimed at manipulating the receptive field—the region of the input an output neuron "sees."
- **Basic Growth with Standard Convolution**:
  - **Kernel Size**: A $3 \times 3$ kernel applied to an input yields a receptive field of $3 \times 3$.
  - **Layer Stacking**: Adding layers linearly increases the receptive field:
    - 1 layer: Receptive field = $3 \times 3$.
    - 2 layers: Receptive field = $5 \times 5$ (each layer adds 1 pixel on each side, growing by 2 total).
    - $n$ layers: Receptive field = $2n + 1$ (for $3 \times 3$ kernels, stride 1, no padding).
  - **Mechanism**: Each $3 \times 3$ kernel extends the receptive field by 1 pixel per side beyond the previous layer’s field.
  - **Limitation**: Growth is slow (linear, +2 per layer), requiring many layers for a large receptive field, increasing computational cost.

### Striding for Faster Growth
- **Striding Recap**: Striding skips input positions, downsampling the output and accelerating receptive field growth.
- **Example**:
  - 1 layer, $3 \times 3$ kernel, stride 1: Receptive field = $3 \times 3$.
  - 2 layers, first with stride 2, second with stride 1: Receptive field = $7 \times 7$ (not $5 \times 5$ as with stride 1).
  - 3 layers, two with stride 2, one with stride 1: Receptive field = $15 \times 15$.
- **Exponential Increase**: Each strided layer doubles the effective step size of subsequent layers’ receptive field growth:
  - Stride 2 in layer 1: Output downsampled by 2, next $3 \times 3$ kernel covers a $6 \times 6$ input region (in original input terms), adding 4 to the base $3 \times 3$, yielding $7 \times 7$.
  - Additional stride 2: Further doubles the step, leading to rapid growth (e.g., $15 \times 15$).
- **Trade-Off**: Larger receptive fields come at the cost of reduced output resolution (e.g., a $32 \times 32$ input with stride 2 becomes $16 \times 16$).

---

## Problem: Balancing Receptive Field and Resolution
- **Challenge**: Striding achieves large receptive fields efficiently but sacrifices spatial detail, critical for tasks requiring precise positional information (e.g., segmentation).
- **Trade-Off**:
  - **Large Receptive Field**: Captures global context but loses fine details due to downsampling.
  - **Small Receptive Field**: Retains local details but lacks broader context.
- **Solution**: Introduce techniques to grow the receptive field without excessive resolution loss.

---

## Dilated Convolution (Atrous Convolution)
- **Concept**: Dilated convolution spreads out the kernel by inserting gaps (zeros) between its elements, increasing the receptive field without changing the number of parameters or downsampling the output.
- **Mechanism**:
  - **Standard Kernel**: A $2 \times 2$ kernel with stride 1 has a $2 \times 2$ receptive field.
  - **Dilation Factor 2**: Gaps of 1 pixel are inserted, expanding it to a $3 \times 3$ effective kernel (e.g., original elements at corners, zeros in between), receptive field becomes $3 \times 3$.
  - **Dilation Factor 3**: Gaps of 2 pixels, yielding a $4 \times 4$ kernel, receptive field = $4 \times 4$.
  - **For $3 \times 3$ Kernel**: Dilation factor 2 expands it to $5 \times 5$ (gaps of 1 between elements).
- **Parameters**: The number of weights remains constant (e.g., 4 for a $2 \times 2$ kernel, 9 for a $3 \times 3$), regardless of dilation, as only the spacing changes.
- **Output Resolution**: Unlike striding, dilation preserves the output size because it samples the input sparsely without skipping outputs.
- **Comparison to Striding**:
  - **Striding**: Skips outputs, reducing resolution (e.g., stride 2 halves output size).
  - **Dilation**: Skips input elements within the kernel, maintaining resolution while expanding the receptive field.
- **Receptive Field Growth**: Similar to striding, dilation increases the receptive field exponentially when stacked:
  - 1 layer, $3 \times 3$, dilation 1: $3 \times 3$.
  - 2 layers, dilation 2 each: Much larger field (e.g., $7 \times 7$ or more, depending on overlap).
- **Drawback**: 
  - **Computational Cost**: Known as atrous convolution, it’s slower than striding because it processes all outputs at full resolution, not skipping any.
  - **Efficiency**: Fully dilated networks are less efficient than combining striding with upsampling.

---

## Up-Convolution (Transposed Convolution)
- **Purpose**: Up-convolution reverses striding’s downsampling, increasing the spatial resolution of activations to recover lost detail.
- **Naming**:
  - **Preferred**: "Up-convolution" reflects its role in upsampling.
  - **Common**: "Transposed convolution" (due to implementation as the transpose of a convolution matrix in frameworks like PyTorch).
  - **Misnomer**: "Deconvolution" is discouraged—it has a distinct signal processing meaning (inverting convolution), unlike this operation.
  - **Alternative**: "Fractional striding" (e.g., stride < 1 conceptually upsamples).
- **Mechanism**:
  - **Conceptual View**: 
    - Takes a small input (e.g., $2 \times 2$).
    - Inserts zeros between elements (e.g., upsamples to $4 \times 4$ with zeros).
    - Applies a standard convolution (e.g., $3 \times 3$ kernel) to the inflated input, producing a larger output.
  - **Mathematical View**: Equivalent to a transposed convolution matrix operation, where one input element contributes to multiple outputs (unlike convolution’s many-to-one mapping), with overlapping outputs averaged.
- **Example**:
  - Input: $2 \times 2$.
  - Upsample with stride 2: Insert zeros to make $4 \times 4$ (e.g., $[a, b; c, d] \to [a, 0, b, 0; 0, 0, 0, 0; c, 0, d, 0; 0, 0, 0, 0]$).
  - Convolve with $3 \times 3$ kernel: Output = $4 \times 4$ (exact size depends on padding/kernel).
- **Role**: Restores resolution lost to striding, often used in later network layers after downsampling.

### Practical Usage
- **Workflow**:
  - **Early Layers**: Strided convolutions reduce resolution (e.g., $32 \times 32 \to 16 \times 16 \to 8 \times 8$), growing the receptive field efficiently.
  - **Later Layers**: Up-convolutions increase resolution (e.g., $8 \times 8 \to 16 \times 16 \to 32 \times 32$), recovering spatial detail.
- **Implementation Note**: Striding rounds down output sizes (e.g., $5 \times 5$ with stride 2 → $2 \times 2$), so up-convolution must account for this (e.g., ensuring the last output is included via padding or stride settings).

---

## Trade-Offs and Comparisons
- **Striding**:
  - **Effect**: Increases receptive field, decreases output size.
  - **Analogy**: Skips outputs, effectively "removing" parts of the input’s spatial representation.
- **Dilated Convolution**:
  - **Effect**: Increases receptive field, preserves output size.
  - **Analogy**: Adds zeros in the kernel, making it sparse but larger.
- **Up-Convolution**:
  - **Effect**: Increases output size, can maintain or adjust receptive field depending on subsequent layers.
  - **Analogy**: Adds zeros in the input, inflating it before convolution.
- **Unified Perspective**: All three manipulate zeros in the convolution process:
  - Striding: Implicitly discards outputs.
  - Dilation: Explicitly spaces out kernel elements.
  - Up-Convolution: Explicitly spaces out input elements.

---

## Practical Implications
- **Network Design**:
  - **Inefficient**: Fully dilated networks (dilation throughout) are computationally expensive due to high resolution at every layer.
  - **Efficient**: Combine striding (downsampling) in early layers with up-convolution (upsampling) in later layers, optionally using dilation for intermediate large receptive fields without resolution loss.
- **Applications**:
  - **Segmentation**: Requires high resolution outputs (up-convolution) and global context (striding/dilation), as in U-Net.
  - **Generative Models**: Up-convolution upsamples latent features to full-size images (e.g., GANs).

---

## Conclusion
- **Summary**:
  - **Receptive Field Growth**: Standard convolution grows it linearly (+2 per layer), striding accelerates this exponentially but reduces resolution.
  - **Dilation**: Expands the receptive field without downsampling, at higher computational cost.
  - **Up-Convolution**: Restores resolution post-striding, acting as an inverse operation.
- **Key Takeaways**:
  - Striding: $ \text{Receptive Field} \uparrow, \text{Resolution} \downarrow $.
  - Dilation: $ \text{Receptive Field} \uparrow, \text{Resolution} = $.
  - Up-Convolution: $ \text{Receptive Field} \text{adjustable}, \text{Resolution} \uparrow $.
- **Final Note**: These techniques offer flexible trade-offs between context, resolution, and computation, tailored to specific tasks.

---

## Additional Context
- **Receptive Field Formula**: For a $k \times k$ kernel, stride $s$, dilation $d$, and $L$ layers:
  - Receptive field = $ 1 + \sum_{i=1}^L (k_i - 1) \cdot \prod_{j=1}^{i-1} s_j \cdot d_i $, where $s_j$ and $d_i$ are stride and dilation at layer $i$.
  - Example: 2 layers, $3 \times 3$ kernel, stride 2 then 1, dilation 1: $1 + (3-1) \cdot 1 + (3-1) \cdot 2 = 7$.
- **Historical Note**: Dilated convolutions were popularized by Yu and Koltun (2015) for segmentation, while transposed convolutions are staples in architectures like U-Net (Ronneberger et al., 2015).
- **PyTorch Implementation**:
  - Dilation: `torch.nn.Conv2d(..., dilation=2)`.
  - Up-Conv: `torch.nn.ConvTranspose2d(..., stride=2)`.

