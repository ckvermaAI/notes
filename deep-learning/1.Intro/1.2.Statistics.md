# Basic Statistics for Deep Learning

## Introduction to Statistics in Machine Learning
- **Connection to Deep Learning**: Statistics provides foundational concepts and notation for machine learning (ML) and deep learning (DL). While ML originated from statistics, it often uses notation more flexibly.
- **Goal of the Lecture**: Standardize notation and introduce core statistical concepts relevant to ML/DL, rather than a full statistics course.

---

## Foundational Concepts
### **Random Variables and Events**
- **Random Variable (e.g., `X`)**: Represents uncertain outcomes (e.g., a coin toss).
- **Event**: A specific outcome (e.g., landing on *heads*).
- **Probability (`P(x)`)**: Likelihood of an event. For a fair coin, theoretical probability is ~50/50, but physical factors (e.g., initial coin position) introduce slight bias.

### **Discrete vs. Continuous Events**
| **Aspect**          | **Discrete (e.g., coin toss)**           | **Continuous (e.g., coin landing angle)** |
|----------------------|------------------------------------------|--------------------------------------------|
| **Probability Type** | Probability mass function (PMF)          | Probability density function (PDF)         |
| **Example**          | `P(heads) = 0.5`                         | Probability of angle ‚àà [40¬∞, 50¬∞]          |
| **Key Property**     | Sum of probabilities = 1                 | Integral over range = 1                    |

- **Probability Density**: For continuous variables, the probability of an exact value (e.g., 45¬∞) is *zero*. Instead, use cumulative distribution functions (CDFs) to measure intervals.

---

## Key Statistical Measures
### **Expectation (ùîº)**
- **Definition**: Weighted average of a function‚Äôs outputs under a distribution.
  - **Discrete**: ùîº[f(x)] = Œ£ P(x)‚ãÖf(x)
  - **Continuous**: ùîº[f(x)] = ‚à´ P(x)‚ãÖf(x) dx
- **Linearity**: ùîº[f + g] = ùîº[f] + ùîº[g]; constants can be factored out.

### **Mean and Variance**
- **Mean (Œº)**: Average value of a distribution (e.g., center of a Gaussian).
- **Variance (œÉ¬≤)**: Measure of spread. Key differences:
  - **Discrete**: Always finite if outcomes are bounded.
  - **Continuous**: Can be infinite (e.g., heavy-tailed distributions).
- **Caution**: Probability densities (PDFs) can exceed 1 for continuous variables.

---

## Sampling and Its Challenges
- **Sampling**: Process of drawing observations from a distribution (e.g., rolling a die, taking photos).
- **Bias in Sampling**: 
  - Finite samples ‚â† true distribution (e.g., flipping a coin 10x rarely yields exactly 5 heads).
  - Requires infinite samples to perfectly represent the distribution.
- **Examples**:
  - Weather data: Empirical samples vs. data-generating distribution (climate).
  - Typing: Random errors mimic sampling from a "key-press distribution."

---

## Statistical Models
### **Types of Models**
1. **Regression**:
   - Maps real-valued inputs to real-valued outputs (e.g., linear regression).
   - Example: Predicting temperature from date.
2. **Classification**:
   - Maps inputs to probabilities over discrete classes (e.g., dog vs. cat in images).
   - Outputs constrained to sum to 1 (probabilistic).

### **Model Goals**
- Learn parameters (Œ∏) to map inputs (x) to outputs (y) using data.
- Data is sampled from a **data-generating distribution** (e.g., real-world processes like weather).

---

## Data Types
- **Labeled Data**: Includes explicit targets (e.g., tagged photos, temperature with timestamps).
- **Unlabeled Data**: Raw observations without annotations (e.g., internet text/images).
- **Focus in ML**: Primarily labeled data for supervised learning (predicting labels from inputs).

---

## Key Takeaways
1. **Probability Distributions**:
   - **Data-generating**: Ground-truth process (e.g., climate).
   - **Empirical**: Observed samples (e.g., temperature measurements).
   - **Model**: Learned approximation (e.g., regression line).
2. **Statistical Models** are functions (e.g., `f(x; Œ∏)`) that map inputs to outputs/predictions.
3. **Expectations and Variances** are critical for understanding model behavior and data spread.
4. **Sampling Bias** necessitates caution when interpreting finite datasets.

*TL;DR*: Statistics underpins ML/DL with concepts like distributions, expectations, and models. Understanding discrete vs. continuous probabilities, sampling biases, and model types is essential for leveraging data effectively.
