## view vs. reshape

In PyTorch, both `torch.view` and `torch.reshape` are used to reshape tensors, but they differ in how they handle memory and data sharing. Here's a breakdown of their key differences:

---

### **1. `torch.view`: Strict Contiguity Requirement**
- **Shares Data**: `torch.view` returns a **view** of the original tensor. This means the reshaped tensor shares the **same underlying data** as the original tensor. Changes to one will affect the other.
- **Contiguous Tensors Only**: It requires the input tensor to be **contiguous** (i.e., stored in a single, uninterrupted block of memory). 
  - Non-contiguous tensors (e.g., after operations like `transpose()` or slicing) will throw an error if used with `view()`.
  - You can check contiguity with `tensor.is_contiguous()`.
  - To force contiguity, use `tensor.contiguous()` before calling `view()`.

#### Example:
```python
x = torch.arange(6)        # [0, 1, 2, 3, 4, 5], contiguous
y = x.view(2, 3)           # Reshape to (2, 3) ‚Äî shares data with x
y[0, 0] = 10               # Modifies x as well: x[0] = 10
```

---

### **2. `torch.reshape`: Flexible Handling**
- **May Share Data or Copy**: 
  - If the input tensor is contiguous, `reshape()` returns a **view** (like `view()`), sharing data with the original.
  - If the input is non-contiguous, `reshape()` **copies the data** to create a contiguous tensor internally, then returns a view of the copy. This avoids errors but may incur performance costs due to copying.
- **No Contiguity Requirement**: Works with both contiguous and non-contiguous tensors.

#### Example:
```python
x = torch.arange(6).transpose(0, 1)  # Non-contiguous (shape adjusted)
y = x.reshape(6)                     # Works ‚Äî may create a copy
y[0] = 10                            # May or may not modify x, depending on contiguity
```

---

### **Key Differences**
| Feature                | `torch.view`                          | `torch.reshape`                     |
|------------------------|---------------------------------------|--------------------------------------|
| **Data Sharing**       | Always shares data (view).            | Shares data if possible; copies otherwise. |
| **Contiguity**         | Requires contiguous input.            | Handles non-contiguous input.        |
| **Use Case**           | Use when you know the tensor is contiguous and want to avoid copies. | Use when unsure about contiguity.    |

---

### **When to Use Which**
- Use **`view()`** when:
  - You **know the tensor is contiguous**.
  - You want to avoid unnecessary data copies.
- Use **`reshape()`** when:
  - You are unsure about the tensor‚Äôs contiguity.
  - You prioritize code flexibility over potential hidden copies.

---

### **Why Contiguity Matters**
- Tensors become non-contiguous after operations like `transpose()`, `permute()`, or slicing. Their memory layout no longer matches their logical shape.
- Contiguous tensors have a predictable memory layout, making it safe for `view()` to reinterpret the data without copying. Non-contiguous tensors require restructuring, which `view()` cannot safely do.

---

### **Summary**
- **`view()`**: Efficient (no copy), but strict about contiguity.
- **`reshape()`**: Flexible (handles non-contiguous tensors), but may involve hidden copies. Prefer this when contiguity is uncertain.



## Does PyTorch uses pointers to create non-contiguous tensors ?

Yes, PyTorch **does use pointers** (or references to memory locations) to manage **non-contiguous tensors**.

Here‚Äôs what‚Äôs going on under the hood:

---

### üîç Contiguous vs Non-contiguous Tensors in PyTorch

In PyTorch, a **tensor is contiguous** if its data is stored in a single, unbroken block of memory and elements are laid out sequentially in memory according to its strides.

A **non-contiguous tensor** arises when:

- You do a `.transpose()`, `.permute()`, `.select()`, slicing with a step, etc.
- These operations change the **view** of the data without actually copying it.

Rather than moving data around (which is expensive), PyTorch simply **creates a new view** using metadata (like shape and stride) that describes how to index into the **original data buffer**.

---

### ‚úÖ So Does PyTorch Use Pointers?

Yes, in a way:

- PyTorch stores a pointer to the **original data buffer** (usually in C++/CUDA).
- The **non-contiguous tensor** just changes how that data is interpreted via strides.
- So multiple tensors can **share the same underlying storage** but have different shapes and strides.

Here‚Äôs a quick demo:

```python
import torch

a = torch.randn(2, 3)
b = a.t()  # transpose, now non-contiguous

print(b.is_contiguous())  # False
print(b.storage().data_ptr() == a.storage().data_ptr())  # True
```

This shows that `a` and `b` **point to the same storage**, even though `b` is non-contiguous.

---

### TL;DR

- PyTorch uses **shared storage** and **strides** to support non-contiguous tensors efficiently.
- It uses **pointers under the hood** to avoid copying data unless necessary (e.g., when calling `.contiguous()`).

If you‚Äôre curious about the C++ side or want to dive into how PyTorch‚Äôs `TensorImpl` and `Storage` classes handle this, I can link you to the relevant source code or docs.