# Regression and Classification: Lecture Summary

This lecture introduces fundamental tasks in deep learning—regression and classification—and shows how even simple models (linear models) can be used to approximate these tasks. The lecture uses weather forecasting examples and PyTorch code snippets to illustrate the concepts.

---

## 1. Regression

### Overview
- **Definition:** Regression involves mapping an $n$-dimensional input to a $d$-dimensional output (often focusing on the one-dimensional case for simplicity).
- **Example:** Predicting temperature from a day or time measurement. The x-axis represents time (e.g., day of the year) while the y-axis shows the measured temperature.

### Linear Regression Model
- **Model Structure:** 
  - A simple linear model consists of a weight matrix $ W $ and a bias vector $ b $.
  - It computes the output using a linear transformation:  $\text{output} = W \times \text{input} + b$
- **Interpretation:**  
  - **Weight $W$:** Determines the slope or rate of change.
  - **Bias $b$:** Shifts the line up or down.
- **PyTorch Implementation:**  
  - Use `torch.nn.Linear` to define the model.
  - The lecture demonstrates inspecting the model’s weights and bias, and how slight randomness in initialization affects the outputs.

### Limitations
- Linear models can only capture linear relationships.
- They are not suited to modeling cyclical, quadratic, or more complex non-linear patterns (e.g., seasonal trends in weather data).

---

## 2. Binary Classification

### Overview
- **Objective:** Instead of predicting a continuous value, the goal is to assign a sample to one of two classes (e.g., rain vs. no rain).
- **Transformation:**  
  - Convert the output of the linear model into a probability between 0 and 1.
  - This is achieved by passing the output through the **sigmoid function**.

### Sigmoid Function
- $\sigma(x) = \frac{1}{1 + \exp(-x)}$
- **Role:** Squashes any real-valued input to a value in the range (0, 1), making it interpretable as a probability.
- **Mathematical Insight:**  
  - The function takes an input, applies a negative exponent to ensure positivity, adds one, and then takes the reciprocal—this ensures the output is bounded.

### Model Interpretation
- **Weights:**  
  - Determine the orientation (rotation) of the decision boundary (the line that separates the two classes).
- **Bias:**  
  - Adjusts the position of the decision boundary (how much the line shifts up or down).
- **Decision Boundary:**  
  - Occurs where the output of the linear function is zero (before the sigmoid), corresponding to a probability of 0.5.

### Practical Consideration
- **Implementation Tip:**  
  - Although a sigmoid is used to illustrate the concept, in practice it is often applied outside the model for numerical stability during training.

---

## 3. Multi-Class Classification

### Overview
- **Scenario:** When there are more than two classes, the model must output a probability distribution over $ C $ classes.
- **Key Constraint:**  
  - The $ C $ output probabilities must be non-negative and sum to one.

### Softmax Function
- $\sigma(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^{C} \exp(x_j)}  $
- **Role:**  
  - An extension of the sigmoid function to multiple classes.
  - Exponentiates each output value and then normalizes them so that they sum to one.
- **Properties:**  
  - **Order Preservation:** The order of the input values is maintained. The highest input value corresponds to the highest probability.
  - **Interpretability:** Provides a clear probabilistic interpretation for each class.

### Model Structure
- **Modification from Binary:**  
  - Instead of a single output, the linear model produces $ C $ outputs.
  - The softmax function is applied externally (i.e., not embedded directly in the model) for numerical stability.
- **PyTorch Implementation:**  
  - Use a linear layer with an output size equal to the number of classes.
  - Follow it with a softmax function along the appropriate dimension to obtain class probabilities.

### Comparison: Softmax vs. Multiple Sigmoids
- **Softmax for Multi-Class:**  
  - Ensures mutual exclusivity among classes (each input is assigned to one unique class).
  - Automatically treats examples from other classes as negative examples.
- **Multiple Binary Classifiers (Sigmoids):**  
  - Can be used for multi-label classification where the classes are not mutually exclusive (an example can belong to more than one class).
  - However, outputs from multiple sigmoids are not directly comparable since they do not sum to one.

---

## 4. Key Takeaways

- **Linear Models:**  
  - Consist of a weight matrix and a bias.
  - Provide a simple yet foundational approach to both regression and classification.
- **Model Limitations:**  
  - While linear models are effective for simple tasks, they cannot capture non-linear relationships or complex decision boundaries.
- **Binary vs. Multi-Class:**  
  - For binary classification, a linear model plus a sigmoid function works well.
  - For multi-class tasks, a linear model plus a softmax function is used to produce a probability distribution over classes.
- **Practical Implementation in PyTorch:**  
  - The lecture includes practical code examples demonstrating how to create these models, inspect parameters, and run forward passes.
- **Numerical Stability:**  
  - A crucial practical note is to avoid embedding the sigmoid or softmax directly within the model definition when training, due to numerical stability concerns.
