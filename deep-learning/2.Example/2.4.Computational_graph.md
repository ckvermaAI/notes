# Computational Graphs

## 1. Introduction

- **Computational graphs** are a key tool in deep learning, allowing efficient and automatic computation of gradients.
- **Purpose:** They enable backpropagation, which is crucial for training deep learning models.
- **Historical Context:** Over the past 10â€“20 years, computational graphs have significantly improved gradient computation efficiency.

---

## 2. The Challenge of Computing Gradients

- In previous lectures, we computed gradients manually for simple models.
- However, real-world deep learning models (e.g., multi-class logistic regression) involve complex functions that make manual differentiation difficult.
- **Computational graphs provide an automated way to track and compute gradients efficiently.**

---

## 3. Constructing a Computational Graph

### 3.1 Example: Softmax Cross-Entropy Loss

- Consider the function: $L(\theta) = -\log(\text{softmax}(Wx + b))_y$
- This can be broken down into elementary functions:
  - **Matrix multiplication**: $Z = Wx + b$
  - **Softmax activation**: $S = \text{softmax}(Z)$
  - **Log function**: $L = -\log(S[y])$
- Instead of viewing this as a single complex function, we **express it as a graph**:
  - Inputs: $X, W, b$.
  - Operations: Matrix multiplication, softmax, logarithm, and indexing.

### 3.2 Graph Representation

- **Nodes** represent operations (e.g., addition, multiplication, softmax).
- **Edges** represent dependencies (i.e., input-output relationships between operations).

For the softmax cross-entropy function, the computational graph looks like:

```
X --> MatMul --> Add --> Softmax --> Log --> Index --> Loss
W -->/        b -->/                     y -->/
```

- This structure captures how data flows through a model during forward computation.

---

## 4. Backpropagation Using Computational Graphs

### 4.1 Chain Rule in Graph Form

- To compute gradients, we apply the **chain rule**:
  - Compute the gradient of each operation **in reverse order**.
  - Multiply each gradient term along the path back to the inputs.
- Example: $\frac{dL}{dW} = \frac{dL}{dS} \cdot \frac{dS}{dZ} \cdot \frac{dZ}{dW}$
- Each partial derivative corresponds to a **Jacobian matrix**.

### 4.2 Reverse-Mode Accumulation

- Instead of computing gradients from the inputs forward (which is inefficient), **we propagate gradients backward**:
  - Start from the final loss node.
  - Compute gradients for each function in reverse order.
  - Store accumulated gradients at each step.
- This process is called **backpropagation**.

### 4.3 Computational Efficiency

- Matrix multiplications can be **costly**, so the choice of computation order matters.
- By computing gradients **from the loss backward to the inputs**, we reduce redundant calculations.
- The time complexity is significantly reduced compared to forward-mode differentiation.

---

## 5. Practical Implementation in PyTorch

### 5.1 Automatic Gradient Computation

- In PyTorch, computational graphs are built **dynamically** during execution.
- To enable automatic differentiation:
  ```python
  import torch
  x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
  y = x ** 2
  y.sum().backward()  # Computes dy/dx
  print(x.grad)  # Outputs the gradient
  ```

### 5.2 `.backward()` for Gradient Propagation

- The `backward()` function:
  - Triggers **backpropagation** by computing gradients for all parameters with `requires_grad=True`.
  - Stores gradients in `.grad` attributes of tensors.

### 5.3 Limitations

- PyTorch only supports **scalar** loss functions for backpropagation.
- If an output is not a scalar, **Jacobian-vector products** must be used instead.

---

## 6. Summary and Key Takeawayss

- **Computational graphs** allow us to express deep learning models as directed graphs of functions.
- **Backpropagation** computes gradients by **traversing the graph in reverse**, applying the chain rule efficiently.
- **Automatic differentiation (Autograd)** in PyTorch enables users to compute gradients **without manually deriving them**.
- **Efficiency:** This approach minimizes redundant computations, making deep learning models scalable.

### The Power of Computational Graphs:
"You only need to define the function. Call `.backward()`, and PyTorch computes the gradients for youâ€”no complex math required."



---

# Computation Graphs in PyTorch: Lecture Summary

## 1. Introduction

- **Computational graphs** are fundamental to PyTorch's automatic differentiation system.  
- PyTorch dynamically constructs a computation graph as operations are performed on tensors.
- The graph allows efficient gradient computation using **backpropagation**.

---

## 2. Constructing Computation Graphs in PyTorch

### 2.1 Creating Tensors with `requires_grad`
- By default, tensors in PyTorch do not track gradients.
- To enable automatic differentiation, set `requires_grad=True`:
  ```python
  import torch
  
  x = torch.randn(10, requires_grad=True)  # A tensor that tracks gradients
  y = torch.randn(10)  # A tensor without gradient tracking
  ```
- Any computation performed on `x` results in **a new tensor that tracks gradients**.

### 2.2 Checking the Computation Graph
- Applying operations to a tensor with `requires_grad=True` **builds a computation graph in the background**.
- Example:
  ```python
  z = x ** 2  # Squaring x
  p = z.mean()  # Taking the mean
  print(p)  # p has a computation graph attached
  ```
- The tensor `p` now has a `.grad_fn` attribute, indicating that it was created by a tracked operation.

---

## 3. Computing Gradients with `.backward()`

### 3.1 Backpropagation with a Scalar Output
- The `.backward()` function computes gradients for **all tensors with `requires_grad=True`**.
- Example:
  ```python
  p.backward()  # Computes gradients
  print(x.grad)  # Prints gradient of x
  ```
- This backpropagation step:
  - Propagates gradients **backward through the computation graph**.
  - Populates the `.grad` attribute of `x` with its gradient.

### 3.2 Understanding the Gradient Computation
- The mean function's derivative is **1/n**.
- The square function's derivative is **2x**.
- The final gradient of `x` is computed as: $\text{grad}(x) = \frac{2x}{10}$
- PyTorch **automatically computes these gradients** using the computation graph.

### 3.3 Calling `.backward()` Multiple Times
- If `.backward()` is called multiple times without clearing `.grad`, **gradients accumulate**:
  ```python
  p.backward()
  p.backward()  # Second call accumulates gradients
  print(x.grad)  # Contains twice the expected value
  ```
- **Solution:** Before each step, clear gradients manually using:
  ```python
  x.grad.zero_()
  ```

### 3.4 Why Gradients Accumulate
- In deep learning, parameters (e.g., weights) can be used multiple times.
- Accumulating gradients ensures that **all contributions are considered**.
- This behavior is essential when performing **mini-batch gradient updates**.

---

## 4. Memory Usage in Computation Graphs

### 4.1 Memory Cost of Computation Graphs
- Each tensor in PyTorch **consumes memory**.
- The computation graph **stores intermediate values** required for backpropagation.

### 4.2 GPU Memory Example
- Creating a tensor with **1 million elements** on a GPU:
  ```python
  x = torch.randn(1_000_000, requires_grad=True, device="cuda")
  ```
  - This allocates **4MB** of GPU memory (since each float is 4 bytes).

- Applying a **ReLU function** increases memory usage:
  ```python
  p = torch.relu(x)
  ```
  - This adds another **4MB** (since an additional node is created in the computation graph).

- If **100** operations are performed:
  ```python
  for _ in range(100):
      p = torch.relu(p)
  ```
  - This accumulates **400MB** of memory, as each operation stores intermediate results.

### 4.3 Memory Cleanup with `.backward()`
- Calling `.backward()` **collapses the computation graph**, freeing up memory:
  ```python
  p.sum().backward()
  ```
- After backpropagation, only:
  - The **original tensor**,
  - The **final output**,
  - And the **computed gradient** remain in memory.

---

## 5. Practical Considerations in PyTorch

### 5.1 `.backward()` Can Only Be Called Once
- Once `.backward()` is called, **the computation graph is freed**.
- Attempting to call `.backward()` a second time **raises an error**.

### 5.2 Avoiding Unnecessary Graph Retention
- To retain the graph (e.g., for RNNs where multiple backward passes are needed), use:
  ```python
  p.backward(retain_graph=True)
  ```
- However, excessive retention **increases memory usage**.

### 5.3 Computing Gradients for Non-Scalars
- `.backward()` **only works with scalar outputs**.
- To compute gradients for non-scalar outputs, a **vector-Jacobian product** must be used:
  ```python
  v = torch.ones_like(p)  # A vector
  p.backward(v)  # Computes Jacobian-vector product
  ```

---

## 6. Summary and Key Takeaways

### ðŸ”¹ What are Computation Graphs?
- **Directed acyclic graphs (DAGs)** that track operations for **automatic differentiation**.

### ðŸ”¹ How Does PyTorch Use Them?
- Operations on `requires_grad=True` tensors **build the graph dynamically**.
- Calling `.backward()` **propagates gradients back through the graph**.

### ðŸ”¹ Memory Considerations
- Each operation **stores intermediate values**, consuming memory.
- `.backward()` **frees memory** by deallocating the graph.

### ðŸ”¹ Key PyTorch Features
| Feature | Description |
|---------|-------------|
| `requires_grad=True` | Enables gradient tracking. |
| `.grad_fn` | Shows the function that created a tensor. |
| `.backward()` | Computes gradients for all `requires_grad=True` tensors. |
| `.grad.zero_()` | Resets gradients before next step. |
| `retain_graph=True` | Prevents graph deallocation after `.backward()`. |

### ðŸš€ The Power of Computational Graphs:
"Just perform computations as usual. Call `.backward()`, and PyTorch will handle all gradient calculations automatically."