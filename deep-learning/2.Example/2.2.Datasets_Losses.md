# Datasets and Losses: Lecture Summary

This lecture explains how we connect our models to real-world data and measure their performance. It focuses on two critical components in machine learning: **datasets** and **loss functions**.

---

## 1. Overview

- **Datasets and Losses:** To express what our models should do, we need:
  - **Datasets:** Collections of samples (inputs) paired with their true labels (outputs).
  - **Loss Functions:** Mathematical functions that quantify how well the model's predictions match the true labels.

---

## 2. Datasets

- **Definition:**  
  Datasets are samples drawn from a data-generating distribution (e.g., large image collections, temperature measurements, etc.) where each sample comes with an associated label.
  
- **Purpose:**  
  They serve as the foundation for training the model by providing examples of the relationship between inputs and outputs.
  
- **Role in Training:**  
  The dataset ties the model parameters to the loss function, meaning the performance (or loss) of the model is directly linked to how well it can predict labels from the given data.

---

## 3. Loss Functions

Loss functions measure the discrepancy between the model's predictions and the ground truth. They are crucial for guiding the training process.

### a. Individual (Element-wise) Loss

- **Notation:**  
  Represented as $ l $ (lowercase).
  
- **Purpose:**  
  Measures the performance of the model for a single data point $(x_i, y_i)$.
  
- **Expected Loss:**  
  The overall loss for the dataset is the average of the individual losses, often referred to as the expected loss.

### b. Loss Functions for Specific Tasks

#### Regression Losses

- **Objective:**  
  For tasks where the model predicts continuous, real-valued outputs.
  
- **Common Losses:**
  - **L1 Loss:**  
    Measures the absolute difference (Manhattan distance) between the predicted value and the ground truth.
  - **L2 Loss:**  
    Measures the squared difference (Euclidean distance) between the prediction and the true value.
  
- **Interpretation:**  
  These losses quantify the distance between the prediction and the actual value, with lower values indicating a better fit.

#### Binary Classification Loss

- **Objective:**  
  For tasks where the model distinguishes between two classes (e.g., rain vs. no rain) by outputting a probability using a sigmoid function.
  
- **Binary Cross Entropy Loss:**
  - $L = -(y * \log(\hat{y}) + (1 - y) * \log(1 - \hat{y}))$, where $y$ is true label (0 or 1) and $\hat{y}$ is predicted probability. 
  - **Mechanism:**  
    - If the true label is 1, the loss focuses on maximizing $\log(\hat{y})$.
    - If the true label is 0, it maximizes $\log(1 - \hat{y})$.
  - **Interpretation:**  
    The loss is the negative log likelihood of the correct class, pushing the model to assign high probabilities to the true labels.

#### Multi-Class Classification Loss

- **Objective:**  
  For tasks where the model must classify inputs into one of $ C $ classes.
  
- **Softmax and Cross Entropy Loss:**
  - **Softmax Function:**  
    Converts the model’s raw output (logits) into a probability distribution over $ C $ classes.
  - **Cross Entropy Loss:**  
    Measures the negative log likelihood of the correct class. The loss function extracts the probability assigned to the true class and computes its negative log.
  - **Key Property:**  
    The softmax function preserves the order of the logits while ensuring that all output probabilities are positive and sum to one.

---

## 4. Model Optimization

- **Goal:**  
  Minimize the expected loss over the entire dataset by adjusting the model parameters.
  
- **Optimization Process:**
  - Compute the gradient of the loss with respect to model parameters.
  - Use methods like gradient descent to update parameters iteratively until the loss is minimized.
  
- **Connection to Datasets:**  
  The loss function is conditioned on the dataset, meaning that minimizing loss directly corresponds to improving the model’s performance on the provided data.

---

## 5. Recap and Key Takeaways

- **Datasets:**  
  Provide the examples and ground truth labels that guide the learning process.
  
- **Loss Functions:**  
  - For **regression**, losses (L1/L2) measure the distance between predictions and true values.
  - For **binary classification**, binary cross entropy loss (with a sigmoid) quantifies prediction quality.
  - For **multi-class classification**, cross entropy loss (with softmax) assesses the model’s ability to correctly classify inputs.
  
- **Training Objective:**  
  The overall aim is to adjust model parameters to minimize the expected loss, thereby achieving a better fit between the model’s predictions and the actual data.

- **Next Steps:**  
  With the loss function defined, the next phase is to compute gradients and optimize the model parameters using techniques such as gradient descent.
