# Normalization in Deep Learning

## Introduction to Gradient Problems
- **Vanishing and Exploding Gradients**: The lecture begins by revisiting the challenges of vanishing and exploding gradients in deep neural networks.
  - **Exploding Gradients**: These occur when gradients grow excessively large, destabilizing training. They are manageable by reducing the learning rate.
  - **Vanishing Gradients**: A more severe issue, where gradients become too small to propagate effectively through deep networks (beyond 10-12 layers). This prevents input activations from reaching the output and gradients from updating earlier layers, halting learning.

## Normalization as a Solution
- **Purpose**: Normalization addresses vanishing gradients by adjusting layer activations to maintain stable magnitudes throughout the network.
- **Mechanism**: For a layer’s activations:
  - Subtract the mean: Shifts activations to zero mean.
  - Divide by the standard deviation: Scales activations to unit variance.
  - Result: $\hat{x} = \frac{x - \mu}{\sigma + \epsilon}$, where $\epsilon$ is a small constant for numerical stability.
- **Effect**:
  - **Vanishing Activations**: If activations shrink, normalization scales them up by dividing by a small standard deviation.
  - **Exploding Activations**: If activations grow large, normalization scales them down by dividing by a larger standard deviation.
- **Limitation**: While normalization mitigates exploding activations, it does not fully address exploding gradients, which still require learning rate adjustments.

## Batch Normalization (BatchNorm)
- **Concept**: BatchNorm estimates the mean and standard deviation using the current mini-batch during training, rather than the entire dataset, for computational efficiency.
- **Computation**:
  - For a batch of size $B$, with data (e.g., images) of shape $C \times H \times W$ (channels, height, width):
    - Mean: $ \mu_c = \frac{1}{BHW} \sum_{b=1}^{B} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{bchw} $
    - Standard Deviation: $ \sigma_c = \sqrt{ \frac{1}{BHW} \sum_{b=1}^{B} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{bchw} - \mu_c)^2 } $
    - Normalized Output: $ \hat{x}_{bchw} = \frac{x_{bchw} - \mu_c}{\sigma_c + \epsilon} $
  - Applied per channel across all spatial positions and batch elements.
- **Advantages**:
  - Stabilizes training by normalizing activations dynamically.
  - Tolerates poor weight initialization and supports higher learning rates.
- **Challenges**:
  - **Gradient Mixing**: Gradients from one sample influence others via shared batch statistics, potentially problematic with outliers. Larger batch sizes mitigate this by stabilizing estimates.
  - **Test Time**: At inference, no batch is available. Solution: Maintain running averages of mean and standard deviation during training for use at test time.
  - **Synchronization Overhead**: In distributed training (e.g., across GPUs), batch statistics must be synchronized, increasing computation time.

## Alternative Normalization Techniques
- **Layer Normalization (LayerNorm)**:
  - Computes mean and standard deviation across all channels for each individual data element (e.g., one image), not across the batch.
  - Formula: For an input $x$ with $C$ channels, $ \mu = \frac{1}{C} \sum_{c=1}^{C} x_c $, $ \sigma = \sqrt{ \frac{1}{C} \sum_{c=1}^{C} (x_c - \mu)^2 } $, $ \hat{x}_c = \frac{x_c - \mu}{\sigma + \epsilon} $.
  - Benefits: Eliminates batch dependency, ideal for sequence models or small batch sizes, and avoids synchronization issues.
- **Group Normalization (GroupNorm)**:
  - Divides channels into groups and computes statistics within each group per data element.
  - Offers a middle ground between BatchNorm and LayerNorm, with a tunable group size parameter.
- **Local Response Normalization (LRN)**:
  - An older method (e.g., used in AlexNet) that normalizes based on neighboring channels or spatial positions.
  - Formula: $ \hat{x}_i = \frac{x_i}{\left( k + \alpha \sum_{j \in \text{neighbors}} x_j^2 \right)^\beta} $, where $k$, $\alpha$, and $\beta$ are hyperparameters.
  - Less flexible than modern methods but conceptually related to GroupNorm.

## Placement of Normalization Layers
- **Option A: After Linear Layer, Before Non-Linearity (e.g., ReLU)**:
  - Normalizes linear layer outputs before applying the activation function.
  - Issue: Centering at zero means half of the activations may be zeroed out by ReLU.
  - Solution: Add learnable scale ($\gamma$) and bias ($\beta$) parameters: $ y = \gamma \hat{x} + \beta $.
- **Option B: After Non-Linearity, Before Next Linear Layer**:
  - Normalizes post-activation outputs.
  - Simpler, as the subsequent linear layer can adjust scale and bias, but empirically slightly less effective.
- **Preference**: Option A is more common despite its complexity, as it often yields better performance.

## Practical Recommendations
- **Starting Point**: Use LayerNorm for simplicity and independence from batch size.
- **Next Steps**: Try GroupNorm if channel scaling issues arise, tuning the group size as needed.
- **Special Cases**: Opt for BatchNorm in convolutional networks with image data, avoiding it for plain linear layers.

## Why Normalization Works
- **Empirical Benefit**: Ensures activations propagate through deep networks without vanishing, observable in practical experiments.
- **Mathematical Insight**: Maintains the eigenvalues of the network’s Jacobian near one, preventing exponential growth or decay of activations and gradients across layers.
- **Depth Limit**: Enables training of networks up to 20-30 layers, beyond which other challenges (e.g., optimization difficulties) emerge, to be covered in future lectures.

## Conclusion
Normalization is a cornerstone technique in deep learning, effectively combating vanishing gradients and enabling the training of deeper networks. By stabilizing activation magnitudes, it ensures meaningful signal propagation, with BatchNorm, LayerNorm, and GroupNorm offering tailored solutions for different architectures and use cases.

--- 

This summary provides a structured, detailed overview of the lecture, incorporating key concepts, mathematical formulations, and practical insights for a thorough understanding of normalization in deep learning.