
# Techniques to Enable Deeper Training

Extremely deep networks (e.g., 100+ layers) fail to train effectively due to optimization difficulties.  

### 1. **Normalizations**  
- **Purpose**: Regularize and structure networks to stabilize training dynamics.  
- **Scope**: General normalization methods (e.g., Batch/Layer Norm) help mitigate issues like vanishing/exploding gradients.  

### 2. **Residual Connections**  
- **Role**: Complementary technique that enables gradient flow through `skip connections`.  
- **Impact**: Allows training of networks with hundreds or even thousands of layers by bypassing non-linear transformations.  

---

## **Outcome**  
By combining **normalization** and **residual connections**, modern deep learning frameworks can successfully train previously intractable ultra-deep networks. This unlocks the potential for highly complex architectures while maintaining stable optimization.