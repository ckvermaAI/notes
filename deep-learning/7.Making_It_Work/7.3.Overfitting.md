# Overfitting

This lecture delves into the concept of overfitting in machine learning, a common challenge where models perform well on training data but fail to generalize to real-world data. It explores the causes of overfitting, methods to detect it, and various strategies to mitigate it, including data splitting, data augmentation, transfer learning, dropout, weight decay, and ensembling. The discussion emphasizes practical approaches and their implications for deep learning.

## Introduction to Overfitting

Overfitting is described as an annoying but inevitable part of machine learning. Despite its challenges, it is a natural aspect of model training that practitioners must learn to manage. Overfitting occurs when a model achieves perfect or near-perfect accuracy on the training set but performs poorly on unseen data, rendering it ineffective for real-world applications. The lecture stresses that a model's success on training data is merely a proxy for its ability to generalize to new, real-world data.

## Understanding and Detecting Overfitting

### Data Splitting for Generalization

To address overfitting and assess a model's real-world performance, the lecture recommends splitting the dataset into three distinct parts:

- **Training Set (60-80%):** Used to train the model and adjust its parameters. A well-trained model will always perform well on this set if the optimization and network capacity are adequate.
- **Validation Set (10-20%):** Used to tune hyperparameters (e.g., learning rate, layer sizes, normalization) without computing gradients. It provides an initial estimate of how the model might perform on unseen data after the first training iteration.
- **Test Set (10-20%):** Used exactly once to evaluate the final model's performance on data never seen during training or validation. This set predicts real-world performance but becomes contaminated if used repeatedly, leading to overfitting.

The simplest method to split the data is random sampling without replacement, typically allocating 70% to training, 15% to validation, and 15% to test. However, the lecture cautions that some datasets may have correlations (e.g., temporal relationships in video data, duplicate images across internet datasets, or label correlations like flower species). Such correlations can be problematic if they prevent generalization beyond the correlated data, though they can be beneficial in specific cases (e.g., automated labeling in autonomous driving datasets). To avoid cross-contamination, duplicates should be confined within a single set (e.g., all duplicates in the training set).

### Measuring Overfitting

Overfitting is detected by tracking the performance (loss or accuracy) on both training and validation sets:

- The training loss typically decreases continuously if the optimizer and network capacity are sufficient.
- The validation loss often plateaus or increases after a point, indicating overfitting.
- The gap between training and validation performance (in terms of loss or accuracy) quantifies the degree of overfitting.

The test set provides a final check on overfitting by comparing its performance to the validation set, offering a single, unbiased estimate of real-world performance.

### Is Overfitting Always Bad?

Overfitting is not inherently bad. A small gap between training and validation performance is natural in deep network training due to their high capacity. Overfitting becomes problematic only when it causes validation or test performance to decline despite continued training. If validation performance improves alongside training performance, this is considered acceptable and typical.

## Causes of Overfitting

### Mechanistic Cause

Overfitting occurs because deep networks are powerful classifiers that identify patterns specific to the training set during multiple epochs. Seeing the same data repeatedly (e.g., after the first epoch) allows the model to latch onto noise or idiosyncrasies unique to the training data, which do not generalize to validation or test sets. With a single epoch and a randomly sampled, representative training set, overfitting is less likely.

### Dimensionality and High-Dimensional Spaces

The lecture explains overfitting through the lens of data dimensionality:

- In low-dimensional spaces, training, validation, and test points are intermingled, making separation (e.g., green vs. blue points) challenging and requiring complex curves.
- In high-dimensional spaces (common in deep learning), these sets occupy distinct regions, making it easy to find hyperplanes or patterns that separate training data from others. This separation explains why deep networks overfit, as they exploit these high-dimensional differences.

With infinite data, overfitting is avoided because the model never encounters the same instance twice, assuming random sampling and representativeness.

## Strategies to Prevent Overfitting

### 1) Early Stopping

The simplest way to prevent overfitting is to monitor validation performance during training and stop when it begins to degrade (e.g., when validation loss increases). This is known as early stopping. Practically, this can be automated by:

- Evaluating validation accuracy every few epochs.
- Saving the model checkpoint with the highest validation accuracy.
- Selecting the best model post-training, avoiding the need for manual intervention.

### 2) Collecting More Data

Increasing the dataset size delays overfitting by reducing the likelihood of revisiting the same data instance. Large language models, for instance, use vast datasets to avoid multiple epochs, minimizing overfitting. This assumes the data is randomly sampled and representative of the target distribution.

### 3) Data Augmentation

Data augmentation artificially expands the dataset by applying random transformations to existing data while preserving labels. Common techniques for images include:

- Changing tint, hue, brightness, saturation, or converting to grayscale.
- Cropping, scaling, rotating, or flipping.

The most popular methods are flipping, scaling, and cropping (often combined). Augmentation creates slightly different views of the same data, tricking the network into treating them as new instances, thus reducing overfitting. This can be implemented in a data loader or before feeding data into the network. Over-aggressive augmentation might make the training task harder, potentially leading to better validation performance than training performance (a form of "negative overfitting").

A notable research direction uses data augmentation for unsupervised learning, where two augmented versions of an image are forced to have consistent outputs, enabling label-free training. This is beyond the scope of the current class.

### 4) Transfer Learning

When data is insufficient even with augmentation, transfer learning leverages pre-trained models:

- **Pre-training Stage:** A model is trained on a large dataset (e.g., ImageNet for supervised learning or unsupervised contrastive methods).
- **Fine-tuning Stage:** The pre-trained model is fine-tuned on the smaller target dataset using its pre-trained weights.

Pre-trained models are widely available (e.g., on GitHub, Hugging Face, or Timm) for computer vision and natural language processing. Transfer learning works because:

1. **Shared Domains:** Pre-trained models may handle similar inputs or labels, transferring relevant knowledge.
2. **Good Initialization:** Pre-trained weights provide a robust starting point, even for unrelated tasks (e.g., fine-tuning a color-image model on depth images), due to hierarchical feature learning.

Transfer learning is recommended for most scenarios outside this class (where it is prohibited for homework) to accelerate progress, though it requires careful selection of pre-trained models.

### 5) Dropout

Overfitting can propagate layer by layer, starting with the first layer overfitting to the data, followed by subsequent layers relying on these overfit activations. Dropout mitigates this by:

- Randomly setting a fraction ($α$) of activations to zero during training, breaking reliance between layers.
- Scaling activations by $1/(1-α)$ during training to compensate (simplifying evaluation to an identity operation).

In practice, dropout is added as a layer (e.g., `torch.nn.Dropout`) after fully connected layers or 1x1 convolutions, but not before general convolutions, as their receptive fields can undo the effect. In transformers, dropout is used before the multi-layer perceptron to prevent overfitting. Users must call `model.eval()` to disable dropout during evaluation and `model.train()` to enable it during training.

### 6) Weight Decay

The traditional view that larger models overfit more (a notion from the 1990s) does not fully apply to deep learning, where larger models can generalize better if properly regularized. However, complexity can still contribute to overfitting. Weight decay addresses this by:

- Penalizing large weight magnitudes, equivalent to adding an L2 norm penalty to the loss.
- Implemented via an optimizer parameter (e.g., in PyTorch’s AdamW), shrinking weights slightly each step.

Contrary to popular belief, weight decay does not significantly prevent overfitting. Instead, it prevents exploding gradients and weight explosions, making it a valuable practice despite its limited impact on overfitting.

### 7) Ensembling

Ensembling involves training multiple models (e.g., three smaller networks) and averaging their outputs to reduce overfitting:

- Each model overfits differently due to randomness in data loading, augmentation, and initialization, leading to diverse local minima.
- Averaging smooths out overfitting patterns, retaining the true signal while reducing noise.

The effectiveness of ensembling is supported by:

- Practical gains of 1-3% in accuracy, though it requires more compute.
- Theoretical justification via Jensen’s inequality, as the loss of an ensemble (assuming convex or near-convex losses) is better than the average loss of individual models.

Ensembling is ideal with abundant compute (e.g., in competitions), but its popularity has waned due to computational costs and uneven advantages in resource-limited settings.

## Conclusion and Recommendations

To manage overfitting effectively:

- **Detection:** Split data into training, validation, and test sets; measure the performance gap between training and validation.
- **Prevention Strategies:**
  - Use early stopping to select the best model based on validation performance.
  - Collect more data or use data augmentation to simulate a larger dataset.
  - Apply transfer learning with pre-trained models for small datasets.
  - Implement dropout after fully connected layers to break layer dependencies.
  - Use weight decay to prevent gradient explosions, though it won’t directly reduce overfitting.
  - Consider ensembling for maximum accuracy when compute is not a constraint.

The lecture concludes that overfitting is a manageable challenge in deep learning, with a range of tools to ensure models generalize well to real-world data.