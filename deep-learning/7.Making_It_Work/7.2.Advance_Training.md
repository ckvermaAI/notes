# Advanced Training

This lecture focuses on advanced techniques for training deep learning models, moving beyond basic Stochastic Gradient Descent (SGD) to explore more sophisticated optimizers like RProp, RMSProp, Adam, AdamW, and Lion. The discussion covers the limitations of SGD, the need for better optimization methods, the mechanics of these advanced optimizers, learning rate tuning, and practical considerations like memory usage, batch size, and learning rate schedules.

## Introduction to Model Training

The lecture begins by setting the stage for training a neural network. The data has been normalized, a model has been selected, and the focus is now on how to train the model effectively. While Stochastic Gradient Descent (SGD) is a foundational optimization technique, the lecture aims to explore its limitations and introduce more advanced optimizers to address these issues.

## Limitations of Stochastic Gradient Descent (SGD)

SGD with momentum is introduced as a starting point. It works well for smaller models and datasets when training speed is not a priority. The algorithm iterates through the dataset batch by batch, computing the gradient, updating a momentum term, and adjusting the weights accordingly. The Python code for SGD with momentum is:

```python
m = 0
for epoch in range(n):
    for (x, y) in dataset:
        J = grad(L(θ|x,y))
        m = J + momentum * m
        θ = θ - ε * m * m.T
```

### Challenges with Learning Rate Tuning

A significant challenge with SGD is the need to tune the learning rate ($ε$). If the learning rate is too small, the model converges very slowly, leading to inefficient training. Conversely, if the learning rate is too large, the model can become unstable, causing the weights to "jump around wildly" in the parameter space. This instability can result in the following scenarios:

- **Frustrating Convergence:** The model might converge temporarily but then jump to a non-converged state, requiring more iterations to stabilize.
- **Network Explosion:** In worse cases, a large learning rate can cause the loss to approach infinity, leading to "NaN" (Not a Number) values in the gradients, effectively halting training.

To illustrate this, the lecture describes an experiment using a simple linear network predicting the ten digits of the MNIST dataset. Different learning rates are tested:

- A small learning rate converges slowly but eventually reaches a good solution with enough compute.
- A slightly larger learning rate (green curve) converges faster.
- An near-optimal learning rate (red curve) converges quickly and stabilizes.
- A learning rate of 1 causes the loss to spike initially, recover temporarily, and then exhibit repeated spikes, indicating instability.

### Analyzing Training Dynamics

The spikes in loss are analyzed by examining the magnitude of gradients and weights:

- For stable learning rates, the magnitudes of weights and gradients remain relatively low and stable.
- With a learning rate of 1, the weight magnitudes spike initially and then stabilize, but the gradient magnitudes exhibit periodic spikes that correspond to the loss spikes.

The lecture introduces the concept of the gradient-to-weight ratio. A high ratio indicates that a single gradient update can overwrite the weights entirely, causing the observed spikes in loss. This is particularly problematic when the learning rate is too large, as it amplifies the effect of large gradients.

### Practical Solution for SGD

To mitigate these issues, the recommended approach is to:

1. Test multiple learning rates.
2. Monitor the loss curve for spiky behavior.
3. Choose the largest learning rate that avoids spikes and ensures stable convergence.

This can be done by visually inspecting the loss curve or, more rigorously, by measuring the gradient-to-weight ratio. However, most practitioners prefer the simpler method of observing the loss curve.

### SGD with Multiple Layers

The lecture extends the discussion to networks with multiple layers, using a two-layer network on MNIST as an example:

- For the first layer, a learning rate of 0.1 works well, while 0.01 is too slow.
- For the second layer, a learning rate of 0.1 causes instability, but 0.01 works well.

When using SGD, the learning rate must be set to the lowest value that ensures stability across all layers (in this case, 0.01). This means some layers train slower than they could, leading to inefficient training. Historically, practitioners tuned learning rates per layer, but this is a tedious and impractical solution, especially for deep networks.

## Advanced Optimizers

To address the limitations of SGD, the lecture introduces several advanced optimizers that automatically adapt learning rates for different parameters, eliminating the need to tune per-layer learning rates.

### RProp

RProp (Resilient Propagation) is one of the earliest adaptive optimizers. It scales gradients based on their magnitude, ensuring more consistent updates across parameters. It also incorporates a momentum term similar to SGD. The code for RProp is:

```python
for epoch in range(n):
    for (x, y) in dataset:
        J = grad(L(θ|x,y))
        m = J / J.norm() + momentum * m
        θ = θ - ε * m * m.T
```

However, RProp is noted to be less effective in practice compared to later optimizers.

### RMSProp

RMSProp improves on RProp by introducing a running average (momentum) for the gradient magnitude, making the estimation more stable. This optimizer is widely used and effective. The code for RMSProp is:

```python
v = 0, θ = 0
for epoch in range(n):
    for (x, y) in dataset:
        J = grad(L(θ|x,y))
        v = β_2 * v + (1-β_2) * J.square()
        m = J / v.sqrt()
        θ = θ - ε * m * m.T
```

RMSProp was not formally published in a paper but is referenced in Jeff Hinton’s lecture notes, highlighting its practical origins in the deep learning community.

### Adam (v0 and v1)

Adam (Adaptive Moment Estimation) builds on RMSProp by maintaining two running averages: one for the gradient (first moment, $m$) and one for the squared gradient magnitude (second moment, $v$). It normalizes the gradient by the square root of the second moment, ensuring adaptive scaling across parameters.

#### Adam v0

The initial version of Adam (v0) is:

```python
m, v = 0, θ = 0
for epoch in range(n):
    for (x, y) in dataset:
        J = grad(L(θ|x,y))
        v = β_2 * v + (1-β_2) * J.square()
        m = J + momentum * m
        θ = θ - ε / v.sqrt() * m * m.T
```

A challenge with this version is that the second moment ($v$) grows more slowly than the first moment ($m$) in the early iterations (due to a longer time horizon for averaging, typically 10x slower). This leads to improper scaling of the initial gradients.

#### Adam v1

The standard version of Adam (v1) introduces correction factors to account for the bias in the running averages, ensuring they reflect the true averages over the iterations seen so far. The code for Adam v1 is:

```python
m, v, t = 0, θ = 0, 1
for epoch in range(n):
    for (x, y) in dataset:
        J = grad(L(θ|x,y))
        v = β_2 * v + (1-β_2) * J.square()
        m = β_1 * m + (1-β_1) * J
        v_t = v / (1 - β_2^t)
        m_t = m / (1 - β_1^t)
        θ = θ - ε * m_t / (v_t.sqrt() + ε)
        t += 1
```

Adam keeps track of the iteration count ($t$) to apply these corrections. A notable property of Adam is that the first step normalizes the gradient to $\pm 1$ (since $v$ is initially zero), which limits the maximum learning rate to avoid destroying the initialization.

#### Mathematical Flaw in Adam

Adam is mathematically flawed and can diverge in certain cases. A fix exists (using the `AMSGrad=True` parameter), but this fix often performs worse in practice. Despite this, Adam remains the default optimizer in deep learning due to its practical effectiveness.

### AdamW

AdamW is a variant of Adam that incorporates weight decay (a regularization technique to prevent exploding gradients) directly into the weight update, rather than the gradient. This makes it more effective for regularization. The code for AdamW is:

```python
m, v, t = 0, θ = 0, 1
for epoch in range(n):
    for (x, y) in dataset:
        J = grad(L(θ|x,y))
        v = β_2 * v + (1-β_2) * J.square()
        m = β_1 * m + (1-β_1) * J
        v_t = v / (1 - β_2^t)
        m_t = m / (1 - β_1^t)
        θ = θ - ε * (m_t / (v_t.sqrt() + decay * θ))
        t += 1
```

AdamW is now considered the default optimizer for most deep learning tasks due to its balance of performance and regularization.

### Lion

Lion is a more recent optimizer, discovered through program synthesis rather than hand-design. It combines elements of Adam and SGD, using a single momentum term and taking the sign of the momentum to update the weights. This makes it more memory-efficient than Adam. The code for Lion is:

```python
m = 0
for epoch in range(n):
    for (x, y) in dataset:
        J = grad(L(θ|x,y))
        b = (1-β_1) * J + β_1 * m
        θ = sign(b)
        m = (1-β_2) * J + β_2 * m
        θ = θ - ε * (b * m.T + decay * θ)
```

Lion requires less memory than Adam (only one momentum term instead of two), but it is sensitive to batch size. While it can be a good choice for memory-constrained scenarios, AdamW is generally preferred if memory is not a limiting factor.

## Memory Usage of Optimizers

The lecture highlights the memory requirements of these optimizers:

- **SGD without Momentum:** Requires memory for weights and gradients (2x memory).
- **SGD with Momentum:** Adds a momentum term (3x memory).
- **Adam/AdamW:** Requires memory for weights, gradients, momentum ($m$), and squared gradient momentum ($v$) (4x memory).
- **Lion:** More memory-efficient than Adam, requiring only one momentum term (3x memory).

For memory-constrained applications, Lion may be preferable, but AdamW is the default choice otherwise.

## Learning Rate Tuning with Advanced Optimizers

Even with advanced optimizers like AdamW, the learning rate still needs to be tuned. The process remains similar to SGD:

- If the learning rate is too low, convergence is slow.
- If the learning rate is too high, the model may not converge and can exhibit instability.

The recommended approach is to:

1. Train for a few epochs.
2. Monitor validation accuracy (or loss) on a held-out dataset.
3. Choose the largest learning rate that allows the performance to increase as quickly as possible without instability.

## Learning Rate Schedules

For some problems, a fixed learning rate may not be optimal. The lecture discusses several learning rate schedules:

- **Step Schedule:** Train until progress stalls, then reduce the learning rate by a factor (e.g., 2 or 10). The timing of the reduction needs to be tuned.
- **Linear Schedule:** Decrease the learning rate linearly over time.
- **Cosine Schedule:** Follow a cosine curve, starting at a high learning rate and decreasing to zero. This is popular for large-scale industrial applications but can be problematic in reinforcement learning, as a learning rate of zero can cause the policy to collapse into a deterministic one.
- **Cyclical Schedule:** Uses cycles of increasing and decreasing learning rates. While this can work, it is harder to tune and less recommended compared to step or cosine schedules.

For most applications, the lecture suggests starting with a cosine schedule, especially if someone has already tuned a schedule for the specific problem or model. A warm-up phase (gradually increasing the learning rate at the start) is often beneficial, particularly for Adam, to initialize momentum terms and prevent early instability.

## Batch Size and Learning Rate Scaling

The lecture addresses the relationship between batch size and learning rate:

- **Linear Scaling Rule:** If the batch size is increased by a factor of $k$, the learning rate can also be increased by $k$. This is because doubling the batch size reduces the variance in the gradient estimate by approximately a factor of 2, allowing for a larger learning rate.
- **Limitations:** This rule holds for smaller learning rates but breaks down as the learning rate approaches 1, where instability becomes a concern.

Larger batch sizes can lead to faster convergence, especially when paired with a higher learning rate. A warm-up phase can also help when increasing batch size, particularly if the network is prone to exploding gradients. This is often combined with a cosine schedule in large-scale settings.

## Practical Recommendations

To recap, the lecture provides the following guidelines for training deep learning models:

- **Optimizer Choice:**
  - Use **AdamW** as the default optimizer for most tasks.
  - Consider **Lion** if memory is a constraint, but ensure it works well for your specific task.
- **Learning Rate Tuning:**
  - Select the largest learning rate that avoids instability, monitoring validation accuracy or loss.
- **Learning Rate Schedules:**
  - Use a cosine schedule with a warm-up phase for most applications, especially if a tuned schedule is unavailable.
  - Step or linear schedules are viable alternatives; cyclical schedules are harder to tune.
- **Implementation in PyTorch:**
  - Replace `torch.optim.SGD` with `torch.optim.AdamW` to use AdamW.
- **Batch Size Considerations:**
  - Scale the learning rate linearly with batch size, but be cautious of instability at high learning rates.
  - Use a warm-up phase for larger batch sizes to stabilize training.

## Conclusion

The lecture emphasizes the evolution from basic SGD to advanced optimizers like AdamW and Lion, which automatically scale learning rates for different parameters, improving convergence and stability. While these optimizers reduce the need for per-layer learning rate tuning, careful selection of the overall learning rate and schedule remains critical. Practical considerations like memory usage, batch size, and warm-up phases are also essential for efficient training, particularly in large-scale applications.