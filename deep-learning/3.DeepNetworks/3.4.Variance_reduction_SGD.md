# Variance Reduction in Stochastic Gradient Descent (SGD)

Stochastic Gradient Descent (SGD) is a cornerstone optimization algorithm in deep learning, adapting the principles of gradient descent by updating model parameters using gradients computed from individual data points. However, this approach introduces high variance in gradient estimates, causing erratic updates and slowing convergence. This lecture explores two key variance reduction techniques—**mini-batches** and **momentum**—that enhance SGD's efficiency and stability, making them indispensable for training deep neural networks. Below is a detailed summary of these techniques, their mechanisms, benefits, and practical considerations, based on the provided transcript with additional context for clarity.

---

## Introduction to Variance in SGD

- **SGD Overview**: Unlike traditional gradient descent, which computes gradients over the entire dataset, SGD updates parameters after computing the loss for each individual data point. This results in frequent but noisy updates, causing the optimization path to "jump around crazily" in the loss landscape. Despite this, SGD eventually converges to a solution.
- **Role of Variance**: The convergence rate of SGD is strongly tied to the variance of its gradient estimates. When gradients from individual data points align (low variance), convergence is rapid. When they disagree significantly (high variance), convergence slows as the algorithm struggles to find a consistent direction.

---

## Mini-Batches

### Definition and Purpose
Mini-batches modify SGD by computing gradients over a small subset (or batch) of the dataset rather than a single data point. The parameter update is based on the average gradient across this batch, reducing the variance of the gradient estimate and smoothing the optimization trajectory.

### Variance Reduction Mechanism
- **Mathematical Insight**: The variance of the mini-batch gradient estimate is inversely proportional to the batch size $B$:
  $\text{Var}(\nabla_{\theta} L_{\text{mini-batch}}) = \frac{1}{B} \text{Var}(\nabla_{\theta} l(\mathbf{o}, y))$
  Here, $\text{Var}(\nabla_{\theta} l(\mathbf{o}, y))$ is the variance of the gradient from a single data point. As $B$ increases, the variance decreases, making the mini-batch gradient a better approximation of the true gradient over the entire dataset.
- **Effect**: This averaging dampens the "crazy jumping" seen in vanilla SGD, leading to more stable and predictable convergence.

### Trade-offs
- **Batch Size Hyperparameter**: 
  - $B = 1$: Reverts to vanilla SGD with high variance.
  - $B = \text{dataset size}$: Becomes full-batch gradient descent, with low variance but high computational cost.
  - Intermediate $B$: Balances variance reduction and computational efficiency.
- **Computational Cost**: Larger batch sizes reduce variance but require more memory and computation per update. However, GPUs excel at parallelizing these computations, mitigating the cost.

### Empirical Observations
- **Convergence Behavior**: Plots from the lecture show that mini-batch SGD converges more smoothly than vanilla SGD, though slight spikes in loss may still occur. These spikes stem from either suboptimal gradient steps or variance in loss evaluation across different batches.
- **Comparison**: Full-batch gradient descent offers the smoothest convergence but is computationally prohibitive for large datasets, while mini-batch SGD strikes a practical middle ground.

### Practical Recommendations
- **Batch Size Selection**: Choose the largest batch size that fits within GPU memory constraints, as this maximizes variance reduction without sacrificing efficiency.
- **Size Conventions**: 
  - Small batch sizes (e.g., 3 or 5) need not be powers of two.
  - Larger batch sizes (e.g., 32, 64, 128) should ideally be powers of two for GPU optimization.
- **Usage**: Mini-batches are recommended whenever feasible, as their benefits outweigh the additional computational overhead.

---

## Momentum

### Definition and Mechanism
Momentum enhances SGD by maintaining a running average of past gradients, using this average to dictate parameter updates rather than relying solely on the current gradient. It introduces a hyperparameter, the momentum term $\mu$ (typically 0.9), which weights the contribution of past gradients.

### Update Rule
The momentum method updates parameters as follows:
- $\mathbf{v}_t = \mu \mathbf{v}_{t-1} + (1 - \mu) \nabla_{\theta} l(\mathbf{o}, y)$
- $\theta \leftarrow \theta - \epsilon \mathbf{v}_t$
Here, $\mathbf{v}_t$ is the velocity (running average of gradients), $\mu$ is the momentum term, and $\epsilon$ is the learning rate. This formulation smooths the update direction over time.

### Variance Reduction Mechanism
- **Temporal Averaging**: By incorporating historical gradients, momentum reduces the impact of high-variance single-sample gradients, stabilizing the optimization path.
- **Additional Benefit**: Momentum aids navigation through complex loss landscapes (e.g., ravines or plateaus) by accumulating velocity in consistent directions, accelerating convergence.

### Comparison with Mini-Batches
- **Mechanism**: Mini-batches average gradients across multiple data points within a batch, while momentum averages gradients over time across iterations.
- **Complementarity**: Both techniques reduce variance but operate on different axes—spatial (data points) versus temporal (iterations).

### Practical Usage
- **Momentum Value**: Set $\mu = 0.9$ for most applications; tuning is rarely needed as this value is robust across networks.
- **PyTorch Note**: The default momentum in PyTorch’s SGD optimizer is 0, so it must be explicitly set to enable momentum (e.g., `torch.optim.SGD(..., momentum=0.9)`).
- **Convergence**: Momentum can achieve convergence speeds akin to full-batch gradient descent but with lower computational cost, as demonstrated in lecture plots.

---

## Conclusion

### Benefits of Both Techniques
- **Mini-Batches**: Reduce variance by averaging gradients across data points, smoothing convergence and leveraging GPU parallelism.
- **Momentum**: Reduces variance by averaging gradients over time, enhancing stability and accelerating progress through the loss landscape.
- **Combined Effect**: Using mini-batch SGD with momentum combines spatial and temporal variance reduction, yielding faster, more reliable convergence.

### Recommendation
For optimal performance in training deep neural networks, employ **SGD with both mini-batches and momentum**. This hybrid approach mitigates the high variance inherent in vanilla SGD, balancing computational efficiency with convergence speed. Practically, set the batch size to the GPU’s memory limit (favoring powers of two for larger sizes) and use a momentum term of 0.9, ensuring these settings are explicitly configured in tools like PyTorch.