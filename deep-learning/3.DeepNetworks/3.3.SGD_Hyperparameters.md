# Stochastic Gradient Descent and Hyperparameters

## A) Stochastic Gradient Descent

### Introduction to Optimization in Deep Learning
- **Purpose**: Stochastic Gradient Descent (SGD) is described as a fundamental algorithm in deep learning, often referred to as a "magical workhorse" for its role in training deep networks by optimizing model parameters to minimize a loss function.
- **Dataset**: 
  - Datasets consist of input-label pairs $(\mathbf{x}, y)$, where $\mathbf{x}$ represents inputs (e.g., images) and $y$ represents corresponding labels.
  - These pairs form the basis for supervised learning in this context.
- **Model**: 
  - The model, denoted $f_{\theta}$, is a deep network composed of layers combining linear and non-linear functions, parameterized by $\theta$.
  - Its differentiability allows efficient gradient computation, enabling optimization of $\theta$ to fit complex functions.

### Loss Functions and Gradient Descent
- **Loss Function**: 
  - For a single data point, the loss is defined as $l(\mathbf{o}, y)$, where $\mathbf{o} = f_{\theta}(\mathbf{x})$ is the model’s output.
  - The overall loss, $L(\theta)$, is the expected loss across the entire dataset, which depends on the parameters $\theta$ and is the target of minimization.
- **Gradient Descent**: 
  - Gradient descent updates parameters using the rule $\theta \leftarrow \theta - \epsilon \nabla_{\theta} L(\theta)$, where $\epsilon$ is the learning rate and $\nabla_{\theta} L(\theta)$ is the gradient of the loss over the full dataset.
  - **Limitation**: Computing the full gradient requires processing the entire dataset per update, making it slow for large datasets or models with many parameters.

### Stochastic Gradient Descent (SGD)
- **Core Idea**: 
  - SGD accelerates training by computing the gradient and updating $\theta$ for each individual data point (or mini-batch) rather than the entire dataset.
  - This leads to frequent, smaller updates, improving scalability and speed.
- **Pseudocode**:
  - Initialize $\theta$ with small random values.
  - For each epoch:
    - For each data point $(\mathbf{x}_i, y_i)$:
      - Compute the gradient $\nabla_{\theta} l(f_{\theta}(\mathbf{x}_i), y_i)$.
      - Update $\theta \leftarrow \theta - \epsilon \nabla_{\theta} l(f_{\theta}(\mathbf{x}_i), y_i)$.
- **Behavior**:
  - Unlike gradient descent’s steady loss reduction, SGD’s loss fluctuates because:
    - Individual losses $l(f_{\theta}(\mathbf{x}_i), y_i)$ vary across data points.
    - Gradients from single samples may not reflect the overall gradient direction, sometimes leading to steps in the "wrong" direction.
  - Despite this noise, SGD converges effectively, especially with large datasets.

### Convergence and Variance
- **Convergence**: 
  - SGD converges reliably for convex loss functions and often in practice for non-convex functions (typical in deep learning).
  - Convergence depends on the consistency of gradients across samples.
- **Variance Impact**: 
  - Convergence speed hinges on the variance of gradient estimates from individual samples compared to the full dataset gradient.
  - Low variance (aligned gradients) accelerates convergence; high variance slows it, as steps may conflict.
- **Intuition**: 
  - Gradient descent computes an average gradient for a smooth descent to the minimum.
  - SGD takes noisier, sample-specific steps, likened to navigating a valley with frequent, local adjustments—sometimes uphill, but trending downward overall.

### Practical Example
- **Gradient Descent**: Starts at a point, computes the full gradient, and moves steadily toward the minimum.
- **SGD**: Begins similarly but updates after each sample’s gradient (e.g., red, green, purple functions), hopping between directions, occasionally moving away from the minimum but eventually converging.

### Summary of SGD
- SGD is the backbone of deep learning optimization, enabling efficient training on large datasets and models.
- Its speed advantage over gradient descent comes at the cost of noisier updates, with convergence speed tied to gradient variance.

---

## B) Hyperparameters

### Definition and Role
- **Hyperparameters**: These are parameters not optimized by SGD, distinct from model parameters $\theta$. Examples include:
  - **Learning Rate ($\epsilon$)**: Controls step size in updates.
  - **Number of Epochs**: Sets how many times the dataset is processed.
  - **Model Architecture**: Encompasses layer count, sizes, and non-linearities.
  - **Loss Function**: Defines the optimization objective.
  - **Optimizer Variants**: Includes SGD modifications (e.g., momentum).
- **Importance**: Hyperparameters shape the training process and model capacity, critically influencing performance and generalization.

### Challenges in Optimization
- **Non-differentiable**: Hyperparameters lie outside the differentiable computation graph, so gradients cannot be computed for them.
- **Manual Setting**: They require manual tuning or heuristic approaches, relying on experimentation rather than algorithmic optimization.

### Tuning Hyperparameters
- **Process**: 
  - Begin with standard or task-similar settings.
  - Iteratively adjust based on performance (e.g., validation loss, accuracy).
  - This trial-and-error method is humorously called "graduate student descent," reflecting its reliance on human effort.
- **Developing Intuition**: 
  - Practitioners gain intuition over time, learning to:
    - Increase model size for complex tasks.
    - Tune $\epsilon$ for stability vs. speed.
    - Select loss functions suited to tasks (e.g., cross-entropy for classification).

### Examples and Effects
- **Learning Rate ($\epsilon$)**: 
  - High values risk overshooting minima; low values slow progress.
- **Number of Epochs**: 
  - Too few may underfit; too many may overfit.
- **Model Architecture**: 
  - Deeper networks increase capacity but risk overfitting without ample data.
- **Optimizer Variants**: 
  - Enhancements like momentum can stabilize and speed up convergence.

### Summary of Hyperparameters
- Hyperparameters are essential yet challenging, requiring manual tuning due to their non-differentiable nature.
- Effective tuning blends experience, intuition, and iterative refinement, making it a key skill in deep learning practice.
