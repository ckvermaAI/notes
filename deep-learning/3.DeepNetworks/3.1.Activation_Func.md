# Non-linearities and Activation Functions

## Introduction to Non-linearities in Deep Learning
- **Limitations of Linear Models**: The lecture begins by highlighting the shortcomings of linear models, which struggle to capture complex patterns. For instance, a linear model cannot distinguish a dark point image (e.g., a dog paw) from a gray background due to its inability to model non-linear relationships.
- **Role of Non-linearities**: To address this, non-linearities are introduced into the model, making it more expressive and capable of solving complex tasks. This is achieved by incorporating activation functions between linear layers in a deep network.

## What are Activation Functions?
- **Definition**: Activation functions are non-linear functions applied after linear transformations (e.g., matrix multiplications) in neural networks. They enable the network to learn and represent complex, non-linear patterns.
- **Necessity**: Without non-linearities, stacking multiple linear layers would still result in a linear transformation, severely limiting the model's expressive power. Non-linearities allow deep networks to approximate a wide range of functions.

## Structure of Deep Networks
- **Composition**: A deep network alternates between linear layers (e.g., matrix multiplication plus bias) and non-linear activation functions. This combination enhances the network's ability to model intricate data relationships.
- **Layers Defined**: 
  - A **layer** is a fundamental computational unit, such as a linear transformation or an activation function, that can be reused across the network.
  - The **depth** of a network is determined by the number of linear transformations an input undergoes to reach the output, not counting non-linearities. For example, a network with two linear layers and a ReLU in between is considered a two-layer network.

## Universal Approximation Theorem
- **Concept**: A two-layer network (linear → non-linear → linear) can theoretically approximate any continuous function, as per the universal approximation theorem.
- **Caveats**:
  - **Training Difficulty**: Finding the right parameters to fit a specific function can be challenging.
  - **Efficiency**: The network may require an impractical number of parameters or computations, making it inefficient for real-world use.

## Types of Activation Functions
The lecture explores a variety of activation functions, detailing their definitions, behaviors, advantages, and disadvantages.

### 1. **ReLU (Rectified Linear Unit)**
- **Definition**: $\text{ReLU}(x) = \max(0, x)$
- **Behavior**: Linear for \($x > 0$\) (outputs \($x$\)), zero for \($x < 0$\).
- **Gradient**:
  - 1 for \(x > 0\)
  - 0 for \(x < 0\)
  - Undefined at \(x = 0\) (typically set to 0 or 1 in practice)
- **Advantages**:
  - Simple and computationally efficient.
  - Widely used due to its effectiveness and speed.
- **Disadvantages**:
  - **Dying ReLU Problem**: If a neuron's input is consistently negative, its output remains zero, and its gradient becomes zero, halting learning for that neuron.
- **Mitigation**:
  - Careful initialization (e.g., avoiding large negative biases).
  - Smaller learning rates to prevent large updates pushing activations into the negative region.

### 2. **Leaky ReLU**
- **Definition**: $\text{Leaky ReLU}(x) = \max(\alpha x, x)$, where $\alpha$ is a small positive constant (e.g., 0.01).
- **Behavior**: Similar to ReLU, but allows a small slope $(\alpha x)$ for negative inputs.
- **Gradient**:
  - 1 for \(x > 0\)
  - $\alpha$ for \($x < 0$\)
- **Advantages**:
  - Prevents dying neurons by ensuring a non-zero gradient for negative inputs.
- **Disadvantages**:
  - Requires tuning the hyperparameter \($\alpha$\).
  - Does not fully suppress negative signals, which may allow unwanted information to persist.

### 3. **ELU (Exponential Linear Unit)**
- **Definition**: 
  $\text{ELU}(x) =
  \begin{cases} 
    x & \text{if } x > 0 \\
    \alpha (\exp(x) - 1) & \text{if } x \leq 0 
  \end{cases}$
- **Behavior**: Linear for positive inputs, exponential for negative inputs, approaching \($-\alpha$\) for large negative values.
- **Gradient**:
  - 1 for \(x > 0\)
  - $\alpha \exp(x)$ for \(x < 0\)
- **Advantages**:
  - Always provides a gradient, avoiding dead neurons.
  - Smoothly dampens negative signals.
- **Disadvantages**:
  - Computationally expensive due to the exponential function.
  - Requires tuning \($\alpha$\).

### 4. **GELU (Gaussian Error Linear Unit)**
- **Definition**: $\text{GELU}(x) = x \cdot \Phi(x)$, where $\Phi(x)$ is the cumulative distribution function of the standard normal distribution.
- **Behavior**: Smooth and non-monotonic, with a slight dip below zero for small positive \(x\), then recovering toward zero.
- **Gradient**: Non-zero everywhere, but complex to compute.
- **Advantages**:
  - Empirically improves performance in state-of-the-art networks.
  - The dip near zero has been shown to benefit training dynamics.
- **Disadvantages**:
  - Computationally intensive due to the Gaussian CDF.
- [Additional details](https://medium.com/@shauryagoel/gelu-gaussian-error-linear-unit-4ec59fb2e47c)

### 5. **Sigmoid and Tanh**
- **Definitions**:
  - **Sigmoid**: $\sigma(x) = \frac{1}{1 + \exp(-x)}$ (ranges from 0 to 1)
  - **Tanh**: $\tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$ (ranges from -1 to 1)
- **Behavior**: S-shaped curves that saturate at both ends.
- **Gradient**: Vanishes (approaches 0) for large positive and negative inputs, causing the **vanishing gradient problem**.
- **Disadvantages**:
  - Saturation at both ends hinders learning in deep networks.
  - Rarely used in modern architectures due to these issues.

## Choosing an Activation Function
- **Recommended Approach**:
  - **Start with ReLU**: It’s simple, fast, and widely tested.
  - **If Issues Arise**: Switch to Leaky ReLU to address dead neurons, or debug training setup and revert to ReLU.
  - **Advanced Scenarios**: Use GELU for state-of-the-art networks where computational cost is justified by performance gains.
  - **Avoid**: Sigmoid and Tanh due to vanishing gradients.
- **Additional Tips**:
  - Use proper initialization (e.g., PyTorch defaults) and small learning rates to minimize issues like dead ReLUs.
  - Monitor activations during training to diagnose problems.

## Historical Context
- **Early Days**: Sigmoid and Tanh were common in early neural networks but fell out of favor due to vanishing gradients.
- **ReLU Era**: Introduced around 2010, ReLU became a standard due to its simplicity and effectiveness.
- **Modern Variants**: Leaky ReLU, ELU, and GELU emerged to address ReLU’s limitations, with GELU gaining traction in cutting-edge models.

## Conclusion
Non-linearities, implemented via activation functions, are the cornerstone of deep learning, enabling models to transcend the limitations of linear transformations. ReLU stands out for its simplicity and efficiency, while variants like Leaky ReLU, ELU, and GELU offer solutions to its shortcomings. The choice of activation function significantly influences training dynamics and model performance, making it a critical design decision in deep learning.

