# Input/Output Representations and Loss Functions

## A) Input/Output Representations

### Overview of Output Transformations
Deep networks excel at approximating continuous functions, mapping continuous inputs to continuous outputs. However, many tasks—such as classification or constrained regression—require outputs that are not continuous or are restricted in some way (e.g., positive values or discrete labels). To address this, **output transformations** are applied to the network’s real-valued outputs, denoted as $\mathbf{o}$, to produce the desired format. The overall function to approximate, denoted $\psi$, combines the deep network $f_{\theta}$ (parameterized by $\theta$) with an output transformation. The inputs are typically vectors $\mathbf{x} \in \mathbb{R}^n$, and the network generates real-valued outputs that are then transformed during inference.

### Types of Output Transformations

- **Regression**
  - **No Transformation**: For standard regression tasks, the network’s output $\mathbf{o}$ is used directly as the predicted value, suitable when the target is any real number.
  - **Positive Regression**: When outputs must be positive (i.e., in $\mathbb{R}^+$), transformations are applied:
    - **ReLU**: Defined as $\max(0, o)$, this ensures non-negative outputs. It is simple but non-differentiable at $o = 0$, yielding a gradient of 0 for negative inputs and 1 for positive inputs.
    - **Soft Version**: The function $\log(1 + e^o)$ guarantees positive outputs and is differentiable everywhere, providing a smooth alternative. However, achieving an exact zero requires very large negative $o$, posing numerical precision challenges.

- **Binary Classification**
  - **Thresholding**: The output $o$ is transformed into a binary label (0 or 1) using an indicator function, e.g., $\mathbb{I}(o > 0)$, assigning 1 if $o > 0$ and 0 otherwise. This is intuitive but non-differentiable, making it unsuitable for gradient-based training.
  - **Sigmoid**: The sigmoid function $\sigma(o) = \frac{1}{1 + e^{-o}}$ transforms $o$ into a probability between 0 and 1, offering a differentiable, probabilistic interpretation (e.g., probability of class 1). It is preferred during training due to its smoothness.

- **Multi-Class Classification**
  - **Argmax**: For a $c$-dimensional output vector $\mathbf{o}$, $\arg\max_i o_i$ selects the index of the largest value, corresponding to the predicted class (1 to $c$). It is non-differentiable and arbitrarily resolves ties (e.g., by choosing the smallest index).
  - **One-Hot Vector**: This assigns 1 to the position of the maximum $o_i$ and 0 elsewhere, creating a vector of length $c$. Like argmax, it is non-differentiable, but it can flag ties by allowing multiple 1s (though this is rare in practice).
  - **Softmax**: Defined as $\text{softmax}(\mathbf{o})_i = \frac{e^{o_i}}{\sum_j e^{o_j}}$, this transforms $\mathbf{o}$ into a probability distribution over $c$ classes. It is differentiable, numerically stable when implemented correctly, and gracefully handles ties by assigning proportional probabilities.

### Key Considerations
- **Differentiability**: Non-differentiable transformations (e.g., thresholding, argmax, one-hot) cannot provide gradients, making them impractical for inclusion in the network during training. Differentiable alternatives (e.g., sigmoid, softmax) are used when gradients are needed.
- **Numerical Stability**: Transformations like softmax can suffer from overflow or underflow if raw outputs are too large or small. For example, if the difference between two $o_i$ values exceeds 100, smaller terms may become numerically zero, causing issues when taking logs.
- **Best Practices**:
  - Output **raw values** (logits for classification, regression values for regression) from the network.
  - Apply transformations **during inference** or within loss functions during training, rather than embedding them in the network.
  - Avoid non-differentiable transformations in the network to ensure stable training, as they can disrupt gradient flow or introduce numerical instability.

---

## B) Loss Functions

### Overview of Loss Functions
Loss functions are critical during training, providing gradients that guide the optimization of the network’s parameters $\theta$. Unlike output transformations, which are applied post-training for inference, loss functions operate on individual data points (denoted $l(\mathbf{o}, y)$ for output $\mathbf{o}$ and target $y$) and are aggregated over the dataset as the expected loss $L(\theta)$. The lecture emphasizes that loss functions must produce meaningful gradients to adjust $\mathbf{o}$ toward the desired $y$, distinguishing them from output transformations.

### Common Loss Functions

- **Regression**
  - **L1 Loss**: Defined as $l(\mathbf{o}, y) = |o - y|$, this measures the absolute difference between the predicted and true values. It provides constant gradients, making it robust to outliers.
  - **L2 Loss**: Defined as $l(\mathbf{o}, y) = (o - y)^2$, this measures the squared difference, offering larger gradients for larger errors due to its quadratic nature. It is smoother and more commonly used.
  - **Choice**: In practice, both L1 and L2 losses yield good results with sufficient data and model capacity, though L2 is often the default due to its differentiability and gradient properties.

- **Binary Classification**
  - **Binary Cross-Entropy (BCE) Loss**: 
    $l(\mathbf{o}, y) = - \left[ y \log(\sigma(o)) + (1 - y) \log(1 - \sigma(o)) \right]$
    where $\sigma(o)$ is the sigmoid function. This minimizes the negative log likelihood, encouraging the network to assign high probability to the correct label (1 or 0). It provides strong gradients when predictions are incorrect and near-zero gradients when correct.
  - **Numerical Stability**: Computing $\log(\sigma(o))$ directly can fail for large $|o|$ due to precision limits (e.g., $\sigma(o)$ becoming 0 or 1), making the log undefined. PyTorch’s `BCEWithLogitsLoss` combines sigmoid and log operations for stability.

- **Multi-Class Classification**
  - **Cross-Entropy Loss**: 
    $l(\mathbf{o}, y) = - \log \left( \frac{e^{o_y}}{\sum_j e^{o_j}} \right)$
    where $y$ is the true class index. This minimizes the negative log likelihood of the correct class’s softmax probability. It is well-defined and provides effective gradients.
  - **Numerical Stability**: Manually applying softmax and then taking the log can lead to errors if $o_i$ values are large (causing underflow/overflow). PyTorch’s `CrossEntropyLoss` handles this internally.

### Properties of Loss Functions
- **Gradient Behavior**:
  - **Regression**: L2 loss amplifies gradients for larger errors, while L1 loss provides consistent gradients regardless of error size.
  - **Classification**: BCE and cross-entropy losses yield large gradients when predictions are wrong (e.g., high negative log likelihood) and small gradients when correct, avoiding updates when unnecessary.
- **Log Likelihood Interpretation**: Classification losses are framed as negative log likelihoods, maximizing the probability of the true label. Using logs (versus raw probabilities) ensures better gradient behavior, avoiding vanishing gradients when predictions are far off.
- **Avoiding Raw Probabilities**: Optimizing raw sigmoid outputs directly flattens gradients when predictions are very wrong, stalling training. Log-based losses maintain gradient signals.

### Best Practices
- **Use Built-in Functions**: For classification, leverage PyTorch’s `BCEWithLogitsLoss` and `CrossEntropyLoss` to ensure numerical stability and proper gradient computation. Regression losses (L1, L2) can be implemented manually but are also available in libraries.
- **Monitor Loss Behavior**: Ensure the loss provides actionable gradients, especially when predictions deviate significantly from targets.
- **Match Loss to Task**: Align the loss function with the task and output transformation (e.g., L2 for regression, cross-entropy for multi-class classification).