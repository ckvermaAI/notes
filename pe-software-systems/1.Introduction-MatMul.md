# Introduction & Matrix Multiplication

The lecture introduces the course, emphasizing why performance engineering is critical and a case study on matrix multiplication to demonstrate performance optimization techniques.

### Why Study Performance Engineering?
- **Perspective on Software Development**:
  - Performance is often secondary to other software priorities such as:
    - **Deadlines**: Meeting project timelines.
    - **Cost**: Budget constraints.
    - **Correctness**: Ensuring the software works as intended.
    - **Extensibility**: Ability to modify or extend the software.
  - Despite its lower priority, performance is described as the **currency of computing**, a resource used to "buy" other desirable properties like usability, security, or ease of programming.
  - Example analogy: Money (performance) can buy water (essential features), even though water is critical for life, because money is a versatile currency.

- **User Perspective**:
  - Users frequently complain about software being "too slow," highlighting the practical importance of performance despite developers' tendencies to overlook it.

- **Historical Context**:
  - In the early days of computing (1964–1977), performance engineering was essential due to limited machine resources:
    - Example: A 1964 computer had 524 kilobytes of memory and a 33 kHz clock rate, compared to modern machines with gigabytes of memory and 2–4 GHz clock rates.
    - Programs often strained these limited resources, necessitating intense performance optimization.
  - Notable quotes from the era reflect caution against premature optimization:
    - **Donald Knuth**: "Premature optimization is the root of all evil" (noted as often taken out of context).
    - **Bill Wulf**: Highlighted inefficiencies committed in the name of performance.
    - **Michael Jackson**: Advised against optimization until necessary, especially for non-experts.

- **Technology Scaling (Pre-2004)**:
  - **Moore's Law**: Chip density doubled every two years, increasing transistor counts and enabling faster clock speeds through miniaturization.
  - **Dennard Scaling**: As chip dimensions shrank, power consumption decreased, allowing higher clock speeds without excessive heat.
  - Example: Apple computers from 1977 to 2004 showed dramatic improvements in clock speed (1 MHz to 1.8 GHz) and memory, with costs remaining similar.
  - During this period, performance was "free" because hardware improvements outpaced software optimization needs—waiting a couple of years made software faster without code changes.

- **Post-2004 Challenges**:
  - Around 2004–2005, clock speeds plateaued at 2–4 GHz due to **power density issues**:
    - Intel's projections showed that scaling clock speeds further would lead to unsustainable temperatures (e.g., nuclear reactor or sun's surface levels).
    - The shift from dynamic power (from circuit switching) to static power (leakage currents) limited further clock speed increases.
  - **Solution**: Hardware vendors introduced **multicore processors**, leveraging Moore's Law to increase the number of cores per chip rather than clock speed.
    - By 2025, single-core chips are nearly obsolete for laptops and workstations; multicore processors dominate.
  - **Impact**: Performance is no longer "free." Software must now exploit parallelism (multicore, vector units, GPUs, cache hierarchies) to achieve high performance, making performance engineering essential.

- **Evidence of Growing Importance**:
  - A study of open-source projects post-2004 showed an increase in bug reports mentioning "performance."
  - Job postings since the mid-2000s increasingly emphasize performance skills.
  - Anecdote: A student who took 6.172 received five job offers, each involving performance-related questions, with offers 20–30% higher than those for less performance-focused roles.

- **Course Focus**:
  - The course focuses on **multicore computing**, as mastering this domain provides a strong foundation for other performance-related areas (e.g., GPUs, file systems, networks), which are not covered due to their complexity but are informed by multicore principles.

---

## Case Study: Matrix Multiplication

### Problem Overview
- **Matrix Multiplication Basics**:
  - Matrix multiplication is a well-studied problem requiring $2n^3$ operations for two $n \times n$ matrices (assuming standard algorithm, not advanced methods like Strassen’s).
    - Each element in the result matrix is computed as a dot product of a row from matrix A and a column from matrix B, involving $n$ multiplications and $n$ additions, hence $2n^3$ total operations.
    - For simplicity, the lecture assumes $n$ is a power of 2 (specifically, $n = 4096$).

- **Target Machine**:
  - A compute-optimized AWS machine with:
    - **Haswell microarchitecture**, 2.9 GHz clock speed.
    - **2 processor chips**, each with **9 cores** (total 18 cores).
    - **Two-way hyperthreading** (disabled for measurements to simplify analysis, as it complicates performance metrics but correlates with non-hyperthreaded performance).
    - **Floating-point unit**: Capable of 8 double-precision (64-bit) operations per core per cycle, including fused-multiply-add (FMA), counting as 2 operations.
    - **Cache hierarchy**:
      - **L1 cache**: 32 KB (instruction and data, 8-way set associative).
      - **L2 cache**: 256 KB.
      - **L3 cache (Last-Level Cache)**: 25 MB.
      - **DRAM**: 60 GB.
    - **Cache line size**: 64 bytes.
    - **Peak performance**: $2.9 \, \text{GHz} \times 2 \, \text{chips} \times 9 \, \text{cores/chip} \times 16 \, \text{flops/core/cycle} \approx 836 \, \text{gigaflops}$.

### Baseline: Python Implementation
- **Code Description**:
  - A simple Python implementation with a triply-nested loop for matrix multiplication ($i, j, k$ order).
  - Typically, Python uses library routines for matrix multiplication, but this example illustrates raw performance for educational purposes.
  - Timing is measured by recording the start and end times around the triply-nested loop.

- **Performance**:
  - For $n = 4096$, the Python code takes ~21,000 seconds (~6 hours).
  - Operations: $2 \times 4096^3 \approx 2^{37}$ floating-point operations.
  - Performance: $\frac{2^{37}}{21000} \approx 6.25 \, \text{megaflops}$.
  - Compared to the machine’s peak of 836 gigaflops, this achieves only **0.00075% of peak performance**, indicating extreme inefficiency.

### Optimization Step 1: Language Choice (Java and C)
- **Java Implementation**:
  - Using the same triply-nested loop in Java, the runtime drops to ~2,900 seconds (~46 minutes).
  - **Relative speedup**: ~7x over Python.
  - **Performance**: ~44.7 megaflops, or ~0.0053% of peak.
  - **Reason**: Java compiles to bytecode, which is interpreted and then just-in-time (JIT) compiled to machine code, reducing overhead compared to Python’s fully interpreted execution.

- **C Implementation**:
  - Using the Clang/LLVM 5.0 compiler (noted as 6.0 in the current term), the same loop in C runs in ~1,100 seconds (~19 minutes).
  - **Relative speedup**: ~2.6x over Java, ~18x over Python.
  - **Performance**: ~117 megaflops, or ~0.014% of peak.
  - **Reason**: C compiles directly to machine code, minimizing overhead compared to Python’s interpreter and Java’s JIT compilation.

- **Why the Performance Difference?**:
  - **Python**: Interpreted, with high overhead for each statement (parsing, interpreting, executing, and updating machine state). It supports high-level features (e.g., dynamic code alteration) at the cost of performance.
  - **Java**: Compiles to bytecode, interpreted initially, with JIT compilation for frequently executed code, balancing flexibility and performance.
  - **C**: Compiled directly to machine code, closely aligned with hardware, offering low overhead and predictable performance.
  - **Course Choice**: C is used in 6.172 because its performance model is easier to analyze than Python’s, though a guest lecture will cover performance in managed languages like Python.

### Optimization Step 2: Loop Order and Cache Locality
- **Loop Order Variations**:
  - The triply-nested loop can be reordered (e.g., $i, j, k$; $i, k, j$; $k, j, i$) without affecting correctness, but performance varies significantly.
  - **Impact**: Changing loop order affects **cache locality**, leading to an 18x performance difference across orders.
  - **Reason**: Cache locality depends on how data is accessed relative to the memory layout.

- **Memory Layout**:
  - In C, matrices are stored in **row-major order** (rows are contiguous in memory: row 1, row 2, etc.).
  - In contrast, Fortran uses **column-major order**, which impacts performance differently.
  - **Cache lines**: Processors read/write memory in 64-byte cache lines. Accessing contiguous memory (spatial locality) is faster than non-contiguous access.

- **Analysis of Loop Orders**:
  - **Order $i, j, k$**:
    - **C**: Fixed $i, j$ means accessing the same element repeatedly (excellent spatial locality, always in cache).
    - **A**: Accesses row $i$ contiguously (good spatial locality).
    - **B**: Accesses column $j$ (elements 4096 apart, poor spatial locality), fetching 64 bytes per access but using only one 8-byte double, wasting 7/8 of the cache line.
  - **Order $i, k, j$**:
    - **C**: Good spatial locality (accessing row $i$).
    - **A**: Excellent spatial locality (fixed $i$, accessing same row).
    - **B**: Good spatial locality (accessing row $k$ contiguously).
    - **Result**: Best performance due to optimal cache usage.
  - **Order $k, j, i$**: Poor spatial locality for both A and B, leading to high cache miss rates.

- **Tool Used**: **Cachegrind** (part of the Valgrind suite) measures cache miss rates, helping identify why certain loop orders perform better.
- **Result**: The best loop order ($i, k, j$) yields a **6.5x speedup** over the baseline C implementation, reaching ~0.09% of peak performance.

### Optimization Step 3: Compiler Optimization Flags
- **Approach**: Using Clang’s optimization flags (`-O0`, `-O1`, `-O2`, `-O3`) to improve performance without altering code.
  - `-O0`: No optimization.
  - `-O1`: Basic optimization.
  - `-O2`: More aggressive optimization.
  - `-O3`: Even more optimization, though heuristic and not always better.
- **Result**: Using `-O2` (surprisingly better than `-O3` for this case) provides a **3.25x speedup**, reaching ~0.3% of peak performance.
- **Additional Techniques**: Profile-guided optimization (using runtime performance data to guide compilation) can further improve results, but not detailed here.

### Optimization Step 4: Parallelization with Multicore
- **Issue**: The code uses only one of the 18 available cores, leaving significant performance potential untapped.
- **Solution**: Use the **Cilk infrastructure** to parallelize loops with `cilk_for`, which schedules loop iterations across cores.
  - Parallelizing the outer loop ($i$) or inner loop ($j$), or both, is tested.
  - **Constraint**: The $k$ loop cannot be parallelized directly due to data dependencies (left as a homework problem to understand why).

- **Results**:
  - **Parallelizing $i$ loop**: Runtime drops to ~3.18 seconds, yielding an **18x speedup** (near-linear scaling with 18 cores).
  - **Parallelizing $j$ loop**: Slower due to scheduling overhead.
  - **Parallelizing both $i$ and $j$**: Also slower due to overhead.
  - **Rule of Thumb**: Parallelize outer loops to minimize scheduling overhead.
- **Performance**: Achieves ~5.4% of peak performance, a significant improvement but still far from optimal.

### Optimization Step 5: Cache Tiling
- **Problem**: Cache misses remain a bottleneck, as the code doesn’t fully exploit cache reuse.
- **Memory Access Analysis**:
  - Computing one row of C ($4096$ elements):
    - **Writes**: 4096 to C (good spatial locality).
    - **Reads**: 4096 from A (one row, good spatial locality), but all of B ($4096 \times 4096$), totaling ~16.8 million memory accesses.
  - Computing a $64 \times 64$ block of C:
    - **Writes**: 4096 to C (same as row).
    - **Reads**: ~200,000 from A (64 rows), ~262,000 from B (64 columns), totaling ~0.5 million accesses—33x fewer than row-based computation.

- **Tiling Approach**:
  - Divide matrices into $s \times s$ submatrices (tiles, e.g., $64 \times 64$).
  - Use two levels of matrix multiplication: outer loops process tiles, inner loops perform standard matrix multiplication within tiles.
  - Results in **6 nested loops** (3 for outer tile processing, 3 for inner multiplication).
  - **Tuning Parameter**: Tile size $s$ (e.g., 32, 64, 128) is optimized experimentally, with $s = 32$ found to be best for this machine.
- **Result**: Tiling yields a **1.7x speedup**, reaching ~9.2% of peak performance, with Cachegrind showing significantly reduced cache misses.

- **Multi-Level Tiling**:
  - The machine has three cache levels (L1: 32 KB, L2: 256 KB, L3: 25 MB, DRAM: 60 GB).
  - Two-level tiling (with parameters $s$ and $t$) requires 9 nested loops; three-level tiling requires 12, making code complex and less readable.
  - **Solution**: Use **divide-and-conquer** to tile recursively for all powers of 2 simultaneously:
    - Divide each matrix into 4 submatrices, solving 8 subproblems of size $n/2 \times n/2$, plus an addition.
    - Implemented using Cilk’s `spawn` and `sync` for parallel execution of subproblems.

- **Initial Result**: Recursive tiling reduces cache misses but increases runtime to ~93 seconds due to **function call overhead** at small base cases (e.g., $n = 1$).
- **Optimization**: Set a threshold (e.g., $n = 32$) below which standard matrix multiplication is used instead of recursive calls.
  - **Result**: Runtime drops to ~1.3 seconds, achieving **12% of peak performance**, with Cachegrind confirming low cache miss rates.

### Optimization Step 6: Vectorization
- **Opportunity**: The machine’s vector units (8 double-precision operations per core per cycle, including FMA) are underutilized.
- **Vector Hardware**:
  - Operates in **SIMD** (Single Instruction, Multiple Data) mode, processing 4 doubles per vector register.
  - Requires data to be aligned and operated on as a single vector chunk.

- **Compiler Vectorization**:
  - Use flags like `-mavx`, `-mavx2`, `-mfma` (for fused-multiply-add), or `-march=native` to target the machine’s architecture.
  - **Fast Math Flag** (`-ffast-math`): Allows reordering of floating-point operations (non-associative in 64-bit doubles), improving vectorization at the cost of potential numerical differences.
  - **Vectorization Report**: Generated to identify which loops are vectorized and which are not.
- **Result**: Compiler vectorization with `-march=native` and `-ffast-math` doubles performance, reaching ~24% of peak.

- **Manual Vectorization**:
  - Use **AVX intrinsics** to explicitly call vector instructions from C, bypassing compiler conservatism.
  - Additional techniques: Matrix transposition, data alignment, and optimized base-case algorithms.
  - **Testing**: Leverages cloud computing to run multiple tests in parallel, reducing experimentation time.
- **Result**: Achieves **41% of peak performance** (~343 gigaflops), with a **50,000x speedup** over the Python baseline.

### Final Comparison
- **Intel Math Kernel Library (MKL)**:
  - The optimized code outperforms Intel’s MKL for $n = 4096$ (a power of 2), but MKL is more robust across matrix sizes.
  - MKL’s professional engineering accounts for non-power-of-2 sizes, unlike the course’s assumptions.

- **Analogy**:
  - The 50,000x speedup is likened to improving a jumbo jet’s fuel efficiency to match a Vespa scooter’s.
  - Such dramatic improvements are rare; matrix multiplication is an extreme case due to its regular structure and optimization potential.

- **Why Not 100% of Peak?**:
  - Remaining bottlenecks include memory bandwidth, imperfect load balancing, or unexploited hardware features.
  - Students are encouraged to explore these limitations further.

---

## Key Takeaways
- **Performance as Currency**: Performance is sacrificed for other software qualities but is critical for usability and efficiency.
- **Historical Shift**: Pre-2004, hardware improvements (Moore’s and Dennard scaling) made performance engineering less critical; post-2004, multicore and parallelism require explicit software optimization.
- **Matrix Multiplication Case Study**:
  - Demonstrates optimization techniques: language choice (Python → Java → C), loop reordering, compiler flags, parallelization, cache tiling, and vectorization.
  - Achieves a 50,000x speedup from 6 hours to 1.3 seconds, reaching 41% of peak performance.
- **Tools and Techniques**:
  - **Cachegrind**: Analyzes cache miss rates.
  - **Cilk**: Enables parallel loop execution.
  - **Compiler Flags**: Optimize code without altering logic.
  - **Experimental Tuning**: Essential for finding optimal parameters (e.g., tile size).
- **Broader Implications**: Mastering multicore performance engineering equips developers for other domains and enhances career prospects, as evidenced by job market trends and student outcomes.

---