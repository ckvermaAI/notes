# Low Rank Adapters (LoRA)

## A) Training Only a Few Parameters
1. **Motivation**:
   - Training large models requires substantial memory for weights, gradients, and momentum.
   - Reducing the number of trainable parameters significantly reduces memory usage.

2. **Strategies**:
   - **Output Layer (Classifier)**:
     - Freeze the backbone (intermediate layers).
     - Train only the final layer, which requires fewer gradients and momentum terms.
     - Efficient but not expressiveâ€”only changes outputs, not the internal computation.
   - **Input Layer (Embedding/Input Adapters)**:
     - Train initial layers to preprocess inputs differently.
     - Often used in Vision-Language Models (VLMs) or soft prompting for large language models.
     - Requires backpropagation through the entire network.

---

## B) Fine-Tuning Intermediate Parameters with LoRA
1. **Limitations of Standard Fine-Tuning**:
   - Training intermediate parameters requires significant memory for gradients and momentum.
   - Randomly selecting parameters for training leads to suboptimal results.

2. **LoRA: Core Idea**:
   - Instead of updating all parameters, use a **low-rank decomposition** of the weight matrix.
   - Represent the weight update $\Delta W$ as the product of two smaller matrices: $\Delta W = A \cdot B$
     where $A \in \mathbb{R}^{n \times r}$ and $B \in \mathbb{R}^{r \times m}$, with $r \ll n, m$.

3. **Benefits**:
   - Captures sufficient information with fewer parameters.
   - Enables memory-efficient fine-tuning of intermediate layers.

---

## C) Training with LoRA Adapters
1. **Initialization**:
   - Matrix $A$: Initialized randomly.
   - Matrix $B$: Initialized to zeros to avoid noisy outputs during the first forward pass.

2. **Forward and Backward Passes**:
   - **Forward Pass**:
     - Use pre-trained weights for the main computation.
     - Compute the additional $\Delta W$ using $A$ and $B$ matrices.
   - **Backward Pass**:
     - Gradients flow through the pre-trained model and LoRA adapters.
     - $B$ updates in the first step; $A$ updates in subsequent steps.

---

## D) Layers Optimized with LoRA
1. **Applicable Layers**:
   - **Linear Layers**: Most common target for LoRA.
   - **Attention Layers**: Add LoRA to $Q, K, V$ projections in transformers for fine-tuning attention weights.
   - **Multi-Layer Perceptrons (MLPs)**: Apply LoRA to feed-forward layers.
   - **Optional**: Train input embeddings and classifiers alongside LoRA.

2. **Not Typically Used**:
   - **Convolutions**: Already memory-efficient; rarely paired with LoRA.

---

## E) LoRA in Practice: Code Snippet
### Implementation of LoRALinear in PyTorch
```python
import torch
from torch import nn

class LoRALinear(nn.Linear):
    def __init__(self, in_features, out_features, rank, alpha=1.0, bias=False):
        super().__init__(in_features, out_features, bias=bias)
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank

        # Initialize LoRA A and B matrices
        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))

    def forward(self, x):
        # Standard linear forward pass
        output = super().forward(x)
        # Add LoRA adjustment
        lora_output = (x @ self.lora_A) @ self.lora_B
        return output + self.scaling * lora_output
```
Usage: Replace standard nn.Linear layers with LoRALinear in pre-trained models.


## F) Memory Requirements with LoRA
1. **Parameter Count**:
    - Original model parameters: $N$.
    - LoRA parameters: $M = r.(n + m),$ where $r << n, m$
    - Total memory: Original weights remain frozen; gradients/momentum are computed only for LoRA parameters.
2. **Comparison**:
    - Full fine-tuning: Memory scales with $N$ (all parameters updated).
    - LoRA fine-tuning: Memory scales with $M$ (only a fraction of parameters updated).
    - Typical reduction: $M \approx 1 - 5 \%$ of $N $

## Summary

- **LoRA Advantages**: 
    - Enables memory-efficient fine-tuning by training a low-rank update to weights.
    - Retains expressiveness, allowing intermediate layers to adapt. 
    - Widely applicable to linear and attention layers.
- **Trade-offs**: 
    - Requires access to pre-trained weights. 
    - Slight increase in forward computation due to added adapters. 
    - LoRA is a practical solution for fine-tuning large models with limited hardware resources, reducing the need for memory while maintaining performance.