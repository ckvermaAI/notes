# Lecture Summary: Activation Checkpointing

## A) Understanding Backpropagation

### Forward Pass:
- During the forward pass, we compute activations and store them.
- Non-linear layers require activations for backpropagation.
- Linear layers require activations to compute weight gradients.

### Backward Pass:
- Computes gradients using stored activations.
- Discards activations after use to free memory.

## B) What Happens When We Execute Only the Forward Pass?
- If no backpropagation is required, activations do not need to be stored.
- Efficient memory usage can be achieved using buffer reuse:
  - Maintain two buffers (A and B).
  - Store layer outputs alternately in A and B.
  - Reuse buffers when no longer needed.

- PyTorch’s `torch.no_grad()` utilizes this principle to avoid storing activations, making forward-only computation very memory efficient.

## C) Backpropagation Without Storing Activations

### Method:
- Run the forward pass without storing activations.
- During the backward pass, recompute activations by calling forward again.

### Limitations:
- **Computational Overhead**: Requires `D` forward passes for `D` layers, making computation **quadratic** in complexity.
- **Not practical** for deep networks due to excessive recomputation.

## D) Activation Checkpointing

### Concept:
- Store a few key activations (checkpoints).
- During backpropagation, recompute activations only within a **block** instead of the entire network.
- Reduces memory usage while maintaining reasonable compute overhead.

### Memory and Compute Tradeoff:
| Method                   | Memory Usage  | Compute Cost  |
|--------------------------|--------------|--------------|
| Standard Backprop        | High         | Standard     |
| No Activation Storage    | Minimal      | Quadratic    |
| Activation Checkpointing | Moderate     | 2× Forward Pass |

- **Checkpointing reduces memory usage** significantly while requiring only **two forward passes per backward pass**.
- With further optimizations, storage can be reduced to **O(D^(1/3)) activations** by increasing forward passes.

### Implementation Details:
1. **Activation Storage Strategy**:
   - Save only selected activations at strategic layers.
   - During backward pass, recompute activations within stored blocks.
   - Delete activations after processing each block.

2. **Challenges**:
   - **Randomness Control**: Ensure dropout layers and other stochastic components produce identical outputs in both forward passes.
   - **Model Modification**: Cannot be applied automatically; requires explicit wrapping of model components using `torch.utils.checkpoint`.

3. **PyTorch Implementation**:
   ```python
   import torch.utils.checkpoint as cp
   checkpointed_output = cp.checkpoint(model_block, input_tensor)
   ```
  - `model_block`: The module being checkpointed.
  - `input_tensor`: Input to the checkpointed block.
  - Forward pass runs without gradient tracking, backward pass recomputes activations

## E) CPU Offloading
- Problem: In NLP tasks, varying input sequence lengths can cause memory spikes, leading to crashes.
- Solution: Use CUDA Unified Memory to automatically move activations to CPU when GPU memory is full.
- Implementation:
  - Utilize CUDA Unified Memory (torch.cuda.memory) to manage memory across CPU/GPU.
  - Used in techniques like QLoRA to handle large models efficiently.

## Summary
- Activation checkpointing provides a balance between memory efficiency and compute cost.
- PyTorch’s checkpointing API facilitates easy implementation.
- CPU offloading prevents memory crashes due to input size variability.

By using these techniques, large-scale deep learning models can be trained with significantly lower memory requirements while maintaining efficient computation.