# Detailed Summary of Mixed Precision Training for Large Deep Networks

## 1. Floating Point Representation: Basics

A floating-point number consists of:

- **Sign:** Determines whether the number is positive or negative (1 bit).
- **Exponent:** Scales the number (number of bits defines the range of representable values).
- **Mantissa (Fraction):** Determines the precision of the number.
- **Precision (Relative):** The smallest representable difference between two numbers relative to their magnitude.
- **Max Value:** The largest representable number.
- **Min Value (Normal):** The smallest positive representable number.

The representation follows this formula:

\[ $\text{Value} = (-1)^{\text{Sign}} \times (1 + \text{Fraction}) \times 2^{\text{Exponent} - K}$ \]

Where \( $K$ \) is a constant for bias.

---

## 2. Characteristics of FP32, FP16, and BF16

| **Data Type** | **Sign** | **Exponent** | **Mantissa** | **Precision (Relative)** | **Max Value**           | **Min Value (Normal)**     |
|---------------|----------|--------------|--------------|---------------------------|--------------------------|----------------------------|
| FP32          | 1 bit    | 8 bits       | 23 bits      | \( $\sim 1E{-07}$ \)      | \( $\sim 3.4 \times 10^{38}$ \) | \( $\sim 1.2 \times 10^{-38}$ \) |
| FP16          | 1 bit    | 5 bits       | 10 bits      | \( $\sim 1E{-04}$ \)      | \( $\sim 6.5 \times 10^4$ \)    | \( $\sim 6.1 \times 10^{-5}$ \)  |
| BF16          | 1 bit    | 8 bits       | 7 bits       | \( $\sim 7.8E{-03}$ \)      | \( $\sim 3.4 \times 10^{38}$ \) | \( $\sim 1.2 \times 10^{-38}$ \) |

---

## 3. Comparison of FP32, FP16, and BF16

- **FP32:** High precision and wide range. Often used for tasks requiring stability and accuracy.
- **FP16:** Reduced precision and range, requiring gradient scaling for stability.
- **BF16:** Combines FP32’s range (due to 8-bit exponent) with reduced precision (7-bit mantissa).

### Advantages of BF16:
- Comparable range to FP32 ensures stability.
- Lower precision (7 bits) suffices for most deep learning tasks.
- Reduces memory usage and accelerates computation due to smaller data size.

---

## 4. Memory and Compute Efficiency with BF16

### Memory Reduction:
- Switching from FP32 to BF16 reduces memory requirements by half since each parameter requires fewer bits.

### Compute Acceleration:
- GPUs can perform 2.82 times more operations with BF16 compared to FP32 due to:
  1. Reduced memory bandwidth usage.
  2. Increased GPU shared memory utilization.
  3. Faster matrix multiplication in block computations.

---

## 5. Training with FP16

### Workflow:
1. **Master Copy of Weights:** Weights are stored in FP32 to preserve precision.
2. **Forward Pass:** Cast weights and activations to FP16.
3. **Backward Pass:** Compute gradients in FP16.
4. **Gradient Scaling:** Scale gradients to prevent underflow.
5. **Update Weights:** Use FP32 master weights for stable updates.
6. **Operations that work well in FP32:** Certain operations like Normalizations in a network don't work extremely well on either FP16/BF16. Just because they don't have the precision required to compute the normalization factor properly. And so what torture auto cast here does? Is it it has a list of sort of blacklisted functions that don't work well in, in low bit, regimes. And it will make sure that it always cast things up to full precision and then down again to, to smaller precision for these operations.

### Example Implementation (PyTorch):
```python
with torch.cuda.amp.autocast():
    output = model(input)
    loss = criterion(output, target)

scaled_loss = scaler.scale(loss)
scaled_loss.backward()
scaler.step(optimizer)
scaler.update()
```

---

## 6. Gradient Scaling in FP16 vs. BF16

- **FP16:**
  - Limited precision causes gradient underflow (values too small to represent).
  - Requires gradient scaling (e.g., multiplying by a factor like \( $2^{16}$ \)) to maintain stability.

- **BF16:**
  - Larger dynamic range avoids underflow.
  - Gradient scaling is typically unnecessary.

---

## 7. Momentum in Mixed Precision Training

- **First Momentum (BF16):** Can be computed in BF16 with minimal instability issues.
- **Second Momentum (FP32):** Recommended to keep in FP32 for stability during optimization (e.g., Adam optimizer).

---

## 8. Concluding Remarks

### Key Benefits:
- Mixed precision training reduces memory requirements from \( 16N \) bytes to \( 12N \) bytes (where \( N \) is the number of parameters).
- BF16 is particularly advantageous for:
  - Memory savings.
  - Computation acceleration (up to 2.82×).
  - Simplified training pipeline (no gradient scaling needed).

### Future Directions:
- Combining mixed precision with other techniques (e.g., gradient checkpointing, quantization) can further optimize training for large-scale models.



# **Explaining FP16 Arithmetic with Example: Addition of 2 and 20**

## **Overview of FP16 Format**

### **Structure**:
- **1-bit Sign**: Determines if the number is positive (`0`) or negative (`1`).
- **5-bit Exponent**: Encodes the scale of the number, biased by 15 (i.e., actual exponent = `stored exponent - 15`).
- **10-bit Mantissa**: Fractional part, normalized so that the leading 1 is implicit.

The value of a number in FP16 is:

\[
(-1)^{\text{Sign}} \times 2^{\text{Exponent} - 15} \times (1.\text{Mantissa})
\]

---

## **Step-by-Step Example: 2 + 20**

### **1. Convert `2` to FP16**:
- Binary representation of `2`: `10.0`
- Normalize: `1.0 \times 2^1`
  - Exponent = `1 + 15 = 16` (stored as `10000`)
  - Mantissa = `0000000000` (after the leading 1, nothing follows in `1.0`)
- **FP16 representation**: `0 10000 0000000000`

### **2. Convert `20` to FP16**:
- Binary representation of `20`: `10100.0`
- Normalize: `1.01 \times 2^4`
  - Exponent = `4 + 15 = 19` (stored as `10011`)
  - Mantissa = `0100000000` (binary digits after the leading 1)
- **FP16 representation**: `0 10011 0100000000`

### **3. Align the Binary Points**:
- Before addition, the exponents of both numbers must match.
- Larger exponent: `4` (from `20`).
- Smaller exponent: `1` (from `2`).
- **Shift `2`'s mantissa to the right** to align it with `20`'s exponent. The shift is by `4 - 1 = 3` bits:
  - Original mantissa of `2`: `1.0000000000`
  - Shifted mantissa: `0.0010000000`
- Now, `2` is represented as:
  - Exponent: `4` (matches `20`)
  - Mantissa: `0.0010000000`

### **4. Perform the Addition**:
- Add the aligned mantissas:
  - `20`: `1.0100000000`
  - `2`: `0.0010000000`
  - **Sum**: `1.0110000000`
- The resulting mantissa is `1.0110000000` with an exponent of `4`.

### **5. Normalize the Result**:
- The result is already normalized as `1.0110000000 \times 2^4`.
- Convert back to decimal: `1.011 \times 2^4 = 21.0`.

### **6. Final FP16 Representation**:
- **Sign**: `0` (positive)
- **Exponent**: `4 + 15 = 19` (`10011`)
- **Mantissa**: `0110000000`
- **FP16 Representation**: `0 10011 0110000000`

---

## **Key Points**
- The smaller number (`2`) had its mantissa **right-shifted** to align its binary point with the larger number (`20`).
- In FP16, this shifting can result in a **loss of precision** if the smaller number’s mantissa is shifted out of the 10-bit limit.
- For this specific example, the result was computed correctly without loss because `2` was still representable after shifting.

If the smaller number had been even smaller (e.g., `0.002`), significant bits might have been truncated, and the addition could lose precision.

