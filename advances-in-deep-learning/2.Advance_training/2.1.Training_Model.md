# Summary of the Lecture: Training Deep Neural Networks/Models

## Brief History of Training and Challenges

### Pre-2012: The Early Era
- **Hardware:** Training relied entirely on CPUs, with limited computational power.
- **Algorithms:** Emphasis on convex optimization due to its well-understood convergence properties.
- **Feature Engineering:** Features were hand-engineered, especially in domains like computer vision.
- **Limitations:** Constrained by human engineering time, small datasets, and limited computational resources.

### 2012-2018: The Rise of GPUs and Large Datasets
- **Hardware:** Shift to GPUs for training deep neural networks (e.g., AlexNet in 2012).
- **Deep Learning Models:** Replaced hand-engineered features with features learned from data.
- **Large Datasets:** Explosion of dataset availability for tasks across computer vision and NLP.
- **Optimization Algorithms:** Emergence of better optimizers like Adam and momentum-based stochastic gradient descent.
- **Limitations:** GPU compute power and the need for innovative ideas to scale models.

### 2019-2022: Scaling Up with Larger Models
- **Larger Data and Models:** Models trained on vast datasets scraped from the internet (e.g., images, text).
- **Attention Mechanisms:** Rise of attention-based models (e.g., Transformers) for scalability.
- **Multi-GPU Systems:** Transition to multi-GPU setups (e.g., 8 GPUs) as single GPUs became insufficient.
- **New Challenges:** Memory became a significant bottleneck as models outgrew GPU memory capacity.

### 2023 Onward: The Era of Massive Models
- **Massive Models:** Models ranging from 8 billion to 400 billion parameters.
- **Distributed Training:** Training requires splitting models across multiple GPUs and nodes (e.g., 40,000+ nodes in advanced data centers).
- **Data Sources:** Shift from curated datasets to massive internet-scale data.
- **Limitations:** Memory constraints, compute cost, and financial resources dominate.

---

## Changes in Deep Learning Research (2012-Today)

### Machine Learning (ML) vs Systems (SysML) Research
- **Early Era (2012-2014):** Focused on ML innovations, with significant engineering effort to enable GPU-based training (e.g., AlexNet, Caffe).
- **Mature Era (2014-2020):** Frameworks like TensorFlow and PyTorch standardized workflows, allowing researchers to focus on ML advancements.
- **Modern Era (2020+):** Increasing focus on systems-level innovations to manage the memory, compute, and data challenges of training massive models.
  - Systems research now contributes to up to half of impactful deep learning papers.
  - Innovations include memory-efficient GPU management and distributed training techniques.

---

## Memory Requirements for Training Large Models
- **Weights:** Require memory proportional to the number of parameters (“N”). Each parameter typically uses 4 bytes (e.g., floating-point precision).
- **Gradients:** Equal memory as weights for backpropagation (4 bytes per parameter).
- **Momentum:** Additional 4 bytes per parameter.
- **Second Momentum (Adam Optimizer):** Another 4 bytes per parameter.

### Memory Usage Breakdown
For an 8 billion parameter model:
- **Weights:** 32 GB (4 bytes × 8 billion).
- **Gradients:** 32 GB.
- **Momentum:** 32 GB.
- **Second Momentum:** 32 GB.
- **Total Memory Without Optimizations:** 128 GB.
  - This exceeds the memory capacity of current GPUs like the NVIDIA A100.

---

## Memory Optimizations for Training Large Models

### Mixed Precision Training
- **Description:** Use lower precision (e.g., FP16 or BF16) for weights and gradients to reduce memory usage while maintaining accuracy.
- **Benefits:** Reduces memory footprint and speeds up computations.

### Distributed Training
- **Data Parallelism:** Splits the data across GPUs, with each GPU holding a copy of the model.
- **Model Parallelism:** Splits the model itself across GPUs, distributing parameters and operations.
- **Pipeline Parallelism:** Breaks the model into stages, each handled by a different GPU.

### Additional Optimizations
1. **Gradient Checkpointing:** Stores only select intermediate results during the forward pass to save memory, recomputing others as needed during backpropagation.
2. **Quantization:** Reduces precision of model parameters (e.g., 8-bit integers).
3. **Low-Rank Approximation:** Represents weights using low-rank matrices to save memory.
4. **Flash Attention:** Memory-efficient implementation of attention mechanisms.
5. **Zero Redundancy Optimizer (ZeRO):** Efficiently partitions model states across GPUs to reduce memory duplication.
6. **Sharded Data and Model States:** Shares memory usage for weights and gradients across GPUs.

---

### Key Insight
With the best optimizations, memory usage can be reduced to 1-2 bytes per parameter, enabling efficient training even for massive models.

