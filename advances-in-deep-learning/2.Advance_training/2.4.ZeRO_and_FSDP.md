# ZeRO (Zero Redundancy Optimizer)

## A) Issues with Data and Model Parallelism
1. **Data Parallelism**:
   - Splits dataset across GPUs.
   - Requires each GPU to have a full copy of the model and optimizer states, leading to memory inefficiency.

2. **Model Parallelism**:
   - Splits model layers across GPUs.
   - Introduces high communication overhead during forward and backward passes.

### Key Challenges:
- Both approaches are limited by memory, restricting model size.
- Inefficient memory usage due to redundant storage of weights, gradients, and optimizer states.

---

## B) What is ZeRO?
ZeRO is a family of optimizations aimed at reducing memory redundancy in distributed training:
- **Core Idea**: Partition weights, gradients, and optimizer states across GPUs.
- **Benefits**: Enables training larger models by trading memory usage for communication overhead.

---

## C) Variants of ZeRO

### 1. **ZeRO-1: Optimizer State Partitioning**
- **Description**:
  - Only partitions optimizer states across GPUs.
  - Weights and gradients are still replicated across GPUs.
- **Communication**: 
  - Requires a `reduce_scatter` operation to sum gradients (& redistributed partitions) for the optimizer state partition.
- **Memory Reduction**: Reduces memory requirement by up to 4x compared to data parallelism.

### 2. **ZeRO-2: Gradient Partitioning**
- **Description**:
  - Partitions both gradients and optimizer states across GPUs.
  - Weights remain replicated.
- **Communication**:
  - Requires an `all_reduce` operation after each backward pass to sum gradients across GPUs.
- **Memory Reduction**: Enables models 6–8x larger than ZeRO-1.

### 3. **ZeRO-3: Full Model Partitioning**
- **Description**:
  - Partitions weights, gradients, and optimizer states across GPUs.
  - Synchronization required during forward and backward passes to access partitioned weights.
- **Communication**:
  - Uses `all_gather` during forward/backward passes to fetch partitioned weights.
  - Uses `all_reduce` and `reduce_scatter` for gradients and optimizer states.
- **Memory Reduction**: Supports models up to 50x larger but incurs high communication overhead.

---

## D) Communication Operations in ZeRO
### Key Operations:
1. **Reduce-Scatter**:
   - Combines gradients across GPUs and scatters them to partitions.
   - Used in ZeRO-1, ZeRO-2, and ZeRO-3 for optimizer state updates.

2. **All-Reduce**:
   - Aggregates gradients across GPUs.
   - Used in ZeRO-2 for gradient synchronization.

3. **All-Gather**:
   - Collects partitioned weights or gradients across GPUs.
   - Used extensively in ZeRO-3 for forward and backward passes.

4. **Broadcast**:
   - Distributes updated weights back to all GPUs.
   - Required after optimizer steps in all ZeRO variants.

---

## E) FSDP (Fully Sharded Data Parallel) vs. ZeRO-3
- **FSDP Overview**:
  - A more efficient implementation of ZeRO-3.
  - Groups layers or parameters to minimize synchronization overhead.
  - Reduces communication bottlenecks by scheduling operations more efficiently.

- **Advantages Over ZeRO-3**:
  1. **Reduced Communication Overhead**:
     - FSDP communicates in groups rather than individual layers.
  2. **Improved Scalability**:
     - Handles large clusters better by reducing inter-node communication.
  3. **Optimized Implementation**:
     - Used in training large models like LLaMA at Meta.

---

## F) Summary and Memory Optimization
1. **ZeRO-1**: Reduces optimizer state memory (4x reduction).
2. **ZeRO-2**: Reduces optimizer state and gradient memory (6–8x reduction).
3. **ZeRO-3**: Reduces memory for weights, gradients, and optimizer states (50x reduction).

### Memory Requirements in Terms of Parameters (N):
- **Data Parallelism**: \(12-16N\).
- **ZeRO-1**: \(\approx 4N + \frac{\text{Optimizer State}}{M}\).
- **ZeRO-2**: \(\approx 2N + \frac{\text{Gradients and Optimizer State}}{M}\).
- **ZeRO-3/FSDP**: \(\approx \frac{16N}{M}\), where \(M\) is the number of GPUs.

### Key Trade-off:
- ZeRO reduces memory usage at the cost of increased communication overhead.
- FSDP mitigates these issues with better synchronization and grouping strategies.

**Conclusion**: These optimizations enable the training of increasingly large models by distributing memory efficiently across GPUs, making the best use of available hardware resources.


# Explanation of Communication Operations in ZeRO

## 1) Is a Broadcast Operation Needed After Optimizer Step if Reduce-Scatter is Executed?

The **broadcast** operation may not always be necessary if **reduce_scatter** is executed properly because:

- **Reduce-scatter** combines two operations:
  1. **Reduction**: Summing gradients across all GPUs.
  2. **Scattering**: Distributing the reduced results to specific GPUs.

- After a **reduce_scatter** operation, each GPU holds the specific partition of reduced gradients corresponding to its optimizer state. This eliminates the need for full synchronization of updated weights across GPUs.

- **Broadcast** is required only when:
  1. The framework assumes all GPUs need a synchronized copy of the updated weights.
  2. Forward/backward passes require access to the full model or layers beyond the local partition.

In systems like **ZeRO-3**, where weights and gradients are partitioned and dynamically accessed, a **broadcast** is unnecessary because each GPU updates only its local partitions.

---

## 2) Why is **All-Reduce** Required After Backward Step but Not Reduce-Scatter?

The difference between **all_reduce** and **reduce_scatter** lies in how gradients are stored and synchronized:

### **Backward Step with All-Reduce**
- Gradients are computed redundantly on all GPUs for the full model.
- **All-reduce**:
  - Aggregates gradients across GPUs by summing them.
  - Ensures that all GPUs have a synchronized copy of the complete gradient tensor.
- Necessary for traditional data-parallel training or **ZeRO-2**, where each GPU holds a full gradient tensor.

### **Backward Step with Reduce-Scatter**
- Gradients are partitioned across GPUs instead of being stored redundantly.
- **Reduce-scatter**:
  - Reduces (sums) gradients and scatters the results to GPUs.
  - Each GPU retains only the slice of the gradient tensor it is responsible for, ensuring memory efficiency.
- Suitable for **ZeRO-3**, where gradient storage is partitioned during computation.

---

## Why Use All-Reduce Instead of Reduce-Scatter After Backward?

1. **All-Reduce for Redundant Gradient Storage**:
   - When all GPUs store the same full gradient tensor, **all_reduce** ensures synchronization across all GPUs.

2. **Reduce-Scatter for Partitioned Gradient Storage**:
   - When gradients are partitioned across GPUs, **reduce_scatter** reduces memory usage and eliminates the need for redundant storage.

---

## Summary of Operations

| **Operation**      | **When Used**                     | **Purpose**                                                                             |
|---------------------|-----------------------------------|-----------------------------------------------------------------------------------------|
| **All-Reduce**      | After backward in full-gradient systems | Aggregates and synchronizes full gradients across all GPUs.                              |
| **Reduce-Scatter**  | After backward in partitioned systems   | Reduces (sums) and scatters gradients, ensuring memory efficiency by storing partitions. |
| **Broadcast**       | After optimizer step (if needed)       | Synchronizes updated weights across GPUs if full-model synchronization is required.      |

---

## Key Takeaways
- **Broadcast** is unnecessary in partitioned systems (e.g., **ZeRO-3**) as updated weights are accessed dynamically.
- **All-Reduce** is used for full-gradient systems to synchronize gradients across GPUs.
- **Reduce-Scatter** is preferred in memory-efficient systems to partition and synchronize gradients without redundancy.

By tailoring communication operations to the system's gradient and weight storage strategy, memory and communication efficiency can be significantly improved.
