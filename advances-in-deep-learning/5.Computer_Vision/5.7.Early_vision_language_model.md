# Early Vision Language Models

## Introduction to Early Vision Language Models
The lecture reflects on the evolution of computer vision from task-specific deep learning (e.g., mapping images to outputs like labels or boxes) to a paradigm integrating **language models**. Previous segments highlighted this shift: **image captioning** replacing classification for pretraining and **visual question answering (VQA)** emerging as an evaluation task. The next two segments explore how simple captioning systems transform into versatile vision-language models, potentially rivaling traditional vision systems. This segment focuses on early models, starting with **CLIP** as context.

### Recap of CLIP
- **Function**: Embeds images and text into a joint space, aligning captions with images via **contrastive loss**.
- **Limitation**: Primitive—retrieves sentences but cannot generate them, lacks dialogue or user interaction.

## Unified IO: A Multimodal Sequence Model
**Unified IO** (2022) ambitiously unifies diverse vision and language tasks:
- **Tasks and Modalities**:
  - **Inputs/Outputs**: Images, text, binary masks, segmentation masks, bounding boxes, keypoints, surface normals, depth maps.
  - **Examples**: Image-to-box (detection), box-to-image (generation).
- **Approach**: Tokenizes all modalities into sequences for a single **sequence model** (e.g., a large language model).
- **Tokenization**:
  - **Text**: Uses **SentencePiece** (common in 2022; modern models prefer **Byte Pair Encoding (BPE)** from GPT).
  - **Images**: **Vector Quantization (VQ)** via VQ-GAN, converting pixels into discrete tokens for classification-based prediction (easier than continuous embeddings).
  - **Dense Modalities**: Applies VQ-GAN to masks, depth maps, etc., despite suboptimal results for non-image-like data (a shortcut avoiding per-modality tokenizers).
  - **Sparse Modalities**: Introduces 1,000 special tokens for coordinates (e.g., (0,0) to (1,1)), encoding bounding boxes (four tokens for two corners) and keypoints (two tokens per point), with syntactic markers (e.g., identifying a keypoint’s joint).
- **Training**:
  - Sequences combine tokenized inputs and outputs (e.g., image then labels, or vice versa) with 1D/2D positional embeddings.
  - Supervised pretraining on diverse data, fine-tuned on 95 datasets.
- **Performance**: Impressive for 2022, though outperformed by specialized models; beaten by competitors soon after.
- **Significance**: A simple, pioneering idea showing multimodal potential, rediscovered two years later as a scalable approach.

## Flamingo: Vision Spliced into Language
**Flamingo** (DeepMind) integrates vision into a pretrained language model:
- **Base**: **Chinchilla**, a DeepMind language model.
- **Architecture**:
  - Adds a vision tower (e.g., CLIP-like encoder) to inject image features.
  - **Gated Cross-Attention Layers**: Language attends to vision features, gated by **tanh** (initialized at zero, trainable to [-1, 1]), preserving the language model’s integrity initially.
  - **Gradient Flow**: Skip connections ensure gradients reach the gate, gradually activating cross-attention.
- **Input Handling**:
  - Special **image tokens** interleave text and images (e.g., text-image-text-image).
  - Text attends only to the preceding image per layer, but deeper layers indirectly access earlier images via language features, enabling multi-image context (e.g., one or two images back).
- **Results**: Excels in VQA with few-shot learning, outperforming prior benchmarks using text-image pairs.
- **Impact**: Ahead of its time (1–2 years), popularizing interleaved vision-language sequences; cross-attention later replaced by simpler methods.

## BLIP Series: Multi-Task Embedding and Captioning
**BLIP** (Bootstrapped Language-Image Pretraining) evolves from CLIP:
- **Original BLIP**:
  - **Components**:
    1. **CLIP-like Embedding**: Transformer encodes image and text with a contrastive loss (ITC tower).
    2. **Image-Text Matching (ITM)**: Adds cross-attention, predicts binary (0/1) match score, uses self-attention.
    3. **Captioning**: Causal masking for autoregressive text generation.
  - **Training**: Shares layers across tasks, supervised on image-text pairs (e.g., COCO), with distinct losses (contrastive, binary, teacher-forcing).
  - **Applications**: Configurable for VQA, visual dialogue; pretrained on COCO/web data, fine-tuned separately for ITM and captioning.
  - **Data Bootstrapping**: Uses ITM to filter web data and captioning to generate new captions, creating a cleaner, larger dataset with minimal human effort.
- **BLIP-2**: Shifts toward a language model with **Q-Former**:
  - **Inputs**: Image, text (e.g., instructions), and queries.
  - **Q-Former**:
    - **Image Encoding**: Vision Transformer produces features.
    - **Query Processing**: Queries self-attend and cross-attend to image features, guided by text via shared encoder with masking (non-causal for matching, causal for generation).
    - **Losses**: CLIP-like (contrastive), ITM (binary), captioning (teacher-forcing), with masks preventing cheating.
  - **Output**: Tokens fed into a pretrained language model via a fully connected adapter (avoids fine-tuning the language model).
  - **Pretraining**: Early instruction tuning with prefix text, yielding strong captioning, retrieval, and dialogue demos.
- **InstructBLIP**: Enhances BLIP-2 with instruction tuning:
  - **Data**: 26 datasets (13 used), with crafted instruction templates for visual reasoning (e.g., Q&A, captioning).
  - **Process**: Q-Former encodes image with instructions; instructions repeated for the language model, enabling dialogue and focused attention.
  - **Results**: Excels in global tasks (e.g., whole-image Q&A), still simple but effective.

## LLaVA Series: Simple yet Data-Driven
**LLaVA** (Large Language and Vision Assistant) simplifies architecture:
- **LLaVA-1**:
  - **Architecture**: Vision Transformer (e.g., CLIP) projects features via an MLP into a pretrained language model (e.g., LLaMA-2, Vicuna).
  - **Data**: Uses GPT-4 (text-only) to generate dialogues from bounding box annotations, creating a large, public dataset.
  - **Training**: Trains the adapter first, then fine-tunes with the language model; uses ScienceQA dataset too.
  - **Impact**: Trains in a day on 8 GPUs, democratizing vision-language models in academia/industry.
- **LLaVA-1.5**: Increases image resolution, chunks images into longer token sequences, adds more data, still fast (1 day on GPUs).
- **LLaVA-1.6**: Higher resolution, OCR data, better model, trains on 32 GPUs, competes with closed-source models (e.g., GPT-4V).
- **M3**: Adds multi-resolution and granularity control (2x2 pooling reduces detail), though criticized as underwhelming.

## Other Models
- **Qwen-VL**: Similar to LLaVA, with a larger adapter; supports bounding box inputs/outputs via tokens (e.g., “box,” “ref”), akin to Unified IO.
- **CLIP-ViT-L336px**: Apple’s work on improving captioning data, focusing on dataset quality.
- **Perceiver IO**: Tokenizes images as patch rows with newline tokens, enabling dynamic resolution; instruction-tuned with high-resolution benchmarks.
- **VILLA (NVIDIA)**: Implements others’ methods with detailed tricks for training vision-language models.

## Trends in Early Vision Language Models
1. **Architectural Stability**:
   - Converged on sequence models (image tokens, text tokens) with minor variations (cross-attention vs. self-attention).
   - Innovations shifted from architecture to data.
2. **Data-Centric Focus**:
   - Performance gains from larger pretraining (e.g., web-scale data), instruction tuning (e.g., LLaVA, InstructBLIP), and curation (e.g., BLIP’s bootstrapping).
   - Data exhaustion drives pseudo-labeling and cleanup efforts.
3. **Future Direction**:
   - Next-generation models will feature more complex architectures, surpassing dialogue to handle broader tasks, though data remains critical.

## Conclusion
Early vision-language models progressed from CLIP’s embeddings to Unified IO’s multimodal unification, Flamingo’s vision-language fusion, BLIP’s multi-task versatility, and LLaVA’s simplicity with rich data. Stabilized architectures highlight data as the key driver, setting the stage for more capable successors.