# Object Detection

## Introduction to Object Detection
Object detection is presented as the next evolutionary step in computer vision following image classification. While image classification assigns a single label to an entire image (e.g., "car" or "apple"), it struggles with complex scenes containing multiple objects. For example, an image of a messy office might be labeled "office," "monitors," or "newsroom," but such a label fails to capture the rich information present—multiple objects like chairs, bottles, and monitors interacting in context.

The lecture emphasizes objects as central to perception, not just in computer vision but in biological systems too. A video of a chimpanzee experiment illustrates this: two chimpanzees return rocks for treats, one receiving a cucumber and the other a preferred grape. The first chimpanzee perceives unfairness by recognizing objects (rock, cucumber, grape) and their manipulation by itself and its peer, highlighting how object understanding underpins complex inferences. Object detection in computer vision mimics this by identifying and localizing objects with bounding boxes and labels (e.g., "monitor," "chair"), enabling a deeper scene understanding.

## Dataset: Microsoft COCO
The primary dataset discussed is **Microsoft COCO (Common Objects in Context)**:
- **Size**: 120,000 training images.
- **Design**: Images are collected using a search engine tasked with finding scenes containing at least two object categories (e.g., "banana and bear," "ball and people"), ensuring multiple objects in context, unlike ImageNet’s single-object focus.
- **Categories**: 80 object categories, selected through a laborious process involving committee meetings over six months.
- **Annotations**: Initially marked with object center points, then segmented with outlines via Mechanical Turk. Bounding boxes are derived from these segmentations. Annotations are exhaustive for the 80 categories, with unannotated regions flagged as ambiguous.
- **Significance**: Moves away from ImageNet’s bias toward centered, isolated objects, providing a dataset for scenes with relational object contexts.

## Evolution of Object Detection Architectures

### 1. R-CNN (Regions with CNN Features)
- **Approach**: A heuristic extracts potential object regions based on low-level image statistics (e.g., edges, textures), producing noisy proposals. Each region is cropped and fed into a convolutional neural network (CNN) for classification into a specific class or "no object."
- **Challenges**:
  - Heuristic is lossy and unreliable.
  - Slow due to processing thousands of cropped regions individually through the CNN (e.g., 1,000 boxes per image).
  - Imbalanced "no object" class complicates training.
- **Duration**: Popular for about one to two years before being superseded.

### 2. Fast R-CNN and Faster R-CNN
- **Fast R-CNN**:
  - **Improvement**: Processes the entire image through a CNN once to create a feature map. Regions are cropped from this feature map (not the raw image) and classified with a lightweight classifier, reducing computation significantly.
  - **Advantage**: Heavy lifting is done upfront, making subsequent classifications faster.
- **Faster R-CNN**:
  - **Innovation**: Replaces the heuristic with **anchor boxes**—predefined box shapes slid across the feature map. For each feature location, multiple anchor shapes (e.g., seven) are tested with classifiers to predict object presence, followed by cropping and detailed classification.
  - **Bounding Box Regression**: Adjusts rigid anchor boxes to fit objects precisely by predicting coordinate offsets.
  - **Type**: Two-stage detector—first stage identifies potential objects, second stage classifies and refines boxes.
  - **Advantages**: Efficient for many classes; straightforward supervision per stage. Popular for about three years.
  - **Limitation**: Not fully end-to-end trainable due to separate stages and cropping.

### 3. YOLO (You Only Look Once)
- **Concept**: A one-stage detector predicting box coordinates and class labels directly from the feature map in a single pass.
- **Process**: For each feature map location, predicts multiple boxes with:
  - Regression to exact coordinates.
  - Classification into a class (e.g., "whiteboard").
- **Advantages**:
  - Faster than Faster R-CNN for small class sets (<100–200 classes) due to avoiding cropping.
  - Nearly end-to-end, though ground truth assignment to predicted boxes requires heuristics.
- **Evolution**: Originated with three versions by the first author; later versions (up to YOLOv11) became a naming trend for improved detectors.
- **Limitation**: Slower for large vocabularies (thousands of classes) as classification scales quadratically with locations and classes, unlike two-stage detectors’ linear scaling for object presence.

### 4. Center-Based Detection (e.g., CenterNet)
- **Concept**: Detects objects by their center points rather than boxes, reducing complexity.
- **Complexity**: Number of centers is $O(n \times m)$ (pixels), versus $O(n^2 \times m^2)$ for all possible boxes in an $n \times m$ image.
- **Process**:
  - Feeds image through a backbone (e.g., CNN).
  - Predicts a dense output (like a segmentation mask) with peaks at object centers (e.g., monitor centers, chair centers).
  - Encodes box width and height in additional channels at each center.
- **Advantages**:
  - Unambiguous center assignment (overlap of same-class objects occurs in <0.1% of cases).
  - Fully end-to-end trainable: converts ground truth to centers and sizes, simplifying supervision.
  - Handles odd shapes better than rigid boxes (e.g., a chair’s ambiguous parts).
- **Origin**: Developed at UT Austin, gaining popularity before transformer-based methods.

### 5. Transformer-Based Detection (e.g., DETR)
- **Concept**: Uses transformers for fully end-to-end detection.
- **Process**:
  - Encodes image into a feature map.
  - Introduces a fixed number of **queries** (e.g., 100) that cross-attend to all image features via transformer layers.
  - Predicts class and box coordinates per query.
  - Matches predictions to ground truth using **bipartite matching** (e.g., Hungarian algorithm), supervising matched pairs and marking unmatched as "no object."
- **Advantages**: Truly end-to-end from image to detections; simple architecture and loss.
- **Limitation**: Fixed query number caps detectable objects (rarely >100 needed).
- **Evolution**: Later variants anchor queries in the image (like CenterNet) instead of using a fixed set, enhancing flexibility.

## Object Detection in 3D
Object detection extends to 3D, with two main approaches:
1. **Top-Down (Map-Based)**:
   - **Use Case**: Autonomous navigation (e.g., vehicles).
   - **Method**: Treats a 3D map (e.g., from LIDAR) as a 2D image, applying 2D detectors like CenterPoint to find object centers and regress 3D properties (height, width, length, orientation).
   - **Input**: Often LIDAR or depth-inferred images projected into a top-down view.
2. **Depth Prediction (Image-Based)**:
   - **Method**: Detects objects in 2D image space (e.g., from one or few images), predicting depth to project into 3D (e.g., Lift-Splat-Shoot).
   - **Advantage**: Mimics human monocular depth estimation (effective beyond 10 meters, unlike stereo). Likely to dominate with sufficient data due to simpler localization and reduced depth ambiguity.

## Why Boxes?
- **Advantages**:
  - Reduces detection to classifying regions.
  - Easy to annotate and measure overlap (e.g., Intersection over Union, IoU).
- **Challenges**:
  - Too many possible boxes ($O(n^2 \times m^2)$), requiring heuristics for ground truth assignment and overlap thresholds.
  - Multiple detections per object necessitate post-processing (e.g., Non-Maximum Suppression, NMS) to remove duplicates, risking loss of overlapping objects.
  - Poor fit for odd shapes (e.g., a chair’s disjointed parts).

## Trends in Object Detection
1. **Transformer Dominance**:
   - Cross-attention-based architectures (e.g., DETR) dominate, replacing hand-engineered convolutional detectors. Non-transformer variants are less successful, with little chance of a comeback.
2. **Naming vs. Finding**:
   - Locating objects is easier than naming them. Assigning meaning or properties (e.g., physical attributes, interactions) is the harder challenge, especially with ambiguous objects.
3. **Maturity and Applications**:
   - Object detection has matured over a decade from lab demos to real-world use (e.g., autonomous vehicles, photo search, surveillance at airports or in some states).
   - Challenges remain in data collection for new classes, driving interest in:
     - **Open Vocabulary Detection**: Uses natural language for flexible labeling.
     - **Text Grounding**: Localizes objects via text descriptions, bypassing traditional detection.

## Conclusion
Object detection builds on image classification by localizing and labeling multiple objects, evolving from heuristic-driven R-CNN to end-to-end transformer-based methods like DETR. Datasets like COCO and innovations like center-based and 3D detection reflect its progress. Now mature, it faces challenges in naming and data scalability, shifting research toward language-integrated approaches.