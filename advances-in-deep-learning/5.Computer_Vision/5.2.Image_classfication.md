# Image Classification

## Introduction to Image Classification
Image classification is a foundational task in modern deep learning, credited with sparking its widespread adoption. It involves assigning one predefined label to an image from a set of many possible labels. For example:
- An image with a car, despite containing other elements, is labeled simply as "car."
- Other examples include labeling images as "apple," "Christmas tree," or "broccoli."

The task is deliberately simplified:
- **Input**: A fixed-resolution image (e.g., resized to a standard size like 224x224 pixels, regardless of original complexity or resolution).
- **Output**: A probability distribution over possible classes rather than a direct label. For instance, a network might assign a high probability to "apple" but also suggest "pear" or "kitchen" as alternatives, depending on the dataset.
- **Training**: Supervised using **cross-entropy loss**, a standard loss function comparing predicted probabilities to true labels.

Despite its simplicity, image classification has limited direct applications, such as **visual search** (e.g., finding images with similar objects). However, its value lies in serving as a **testbed** for developing and evaluating deep learning architectures and computer vision techniques.

## Datasets Supporting Image Classification
Datasets have been pivotal in advancing image classification. The lecture covers several key datasets, their characteristics, and their impact:

### 1. CIFAR-10
- **Description**: One of the earliest popular datasets for deep learning in image classification.
- **Resolution**: Low, at 32x32 pixels.
- **Classes**: 10 (e.g., cat, dog, car).
- **Size**: Relatively small by modern standards, derived from a subset of the "80 Million Tiny Images" dataset.
- **Balance**: Perfectly balanced with equal numbers of training, validation, and test images per class.
- **Significance**: Its simplicity (low resolution, balanced classes) made it ideal for early deep networks, including fully connected ones. However, it was primarily a machine learning dataset and not widely accepted in major computer vision venues due to its toy-like nature.

### 2. ImageNet
- **Description**: The cornerstone dataset for computer vision, significantly larger and more complex than CIFAR-10.
- **Classes**: 1,000 in the standard version; a larger version has 21,000.
- **Size**: 1.2 million images (standard); 14 million in the larger version.
- **Resolution**: Higher, typically around 224x224 pixels after resizing.
- **Balance**: Roughly balanced across classes.
- **Characteristics**: Images feature centrally positioned objects, simplifying early models’ tasks by eliminating the need for object localization.
- **Creation**: Sourced from Google Search (using text around images) and annotated via Amazon Mechanical Turk, a major effort around 2007-2008 that pushed the platform to its limits.
- **Impact**: Became the default dataset for training deep networks, with its annual competition (ImageNet Large Scale Visual Recognition Challenge, ILSVRC) driving progress.

### 3. Yahoo Flickr 100 Million
- **Description**: An attempt to scale beyond ImageNet using 100 million Flickr images.
- **Annotation**: Uses user-generated tags (e.g., "car," "city") instead of strict labels, reflecting real-world complexity.
- **Challenges**: Tags are noisy and non-exclusive (e.g., "car" vs. "cars"), complicating traditional classification. Early models struggled until large language models (around 2020) leveraged their language understanding to interpret tags effectively.
- **Significance**: Highlights the shift toward handling unstructured, "in-the-wild" data.

### 4. Open Images
- **Description**: Developed by Google researchers, containing 9 million Creative Commons-licensed images.
- **Classes**: Around 20,000.
- **Annotations**: Includes both image-level labels and object-level details (e.g., bounding boxes, pose information).
- **Significance**: A richer, publicly accessible alternative to ImageNet, bridging classification and detection tasks.

### 5. MIT Places
- **Description**: Focuses on scenes (e.g., "runway," "canyon," "kitchen") rather than objects.
- **Size**: 10 million images, 400+ classes.
- **Insight**: Networks trained on Places implicitly learn object representations en route to scene understanding, demonstrating the interconnectedness of vision tasks.

### 6. Proprietary Datasets
- **Google JFT-3B**: 3 billion images, 30,000 hierarchical classes, semi-automatically labeled. A closed dataset giving Google models an edge.
- **Facebook 142 Million**: 142 million uncurated images, deduplicated and paired with curated datasets (e.g., ImageNet) to retrieve similar images. Used for unsupervised learning by discarding labels and focusing on image similarity.

## Evolution of Architectures
The lecture traces the development of key architectures that shaped image classification:

### 1. AlexNet (2012)
- **Breakthrough**: Won the 2012 ILSVRC, halving the error rate on ImageNet and legitimizing deep learning.
- **Structure**: 
  - Large convolutional layers (e.g., 11x11 kernels).
  - Normalization, pooling, and multiple convolutional layers.
  - Three-layer classifier on top.
- **Innovation**: Trained on two GPUs due to memory constraints (2GB each), splitting layers across them. This GPU implementation, not the architecture (similar to 1990s LeNet), was revolutionary.
- **Impact**: Shifted computer vision from hand-crafted systems to learned representations, spurring frameworks like Caffe, PyTorch, and TensorFlow.

### 2. ResNet (2015)
- **Breakthrough**: Won the 2015 ILSVRC with near-human performance, signaling the end of significant ImageNet gains.
- **Structure**: Introduced **residual blocks**:
  - Two convolutions with batch normalization.
  - A residual connection skipping the block, allowing direct gradient flow.
  - Repeated blocks with downsampling.
- **Innovation**: Residual connections enabled training of much deeper networks (e.g., 152 layers) by mitigating vanishing gradients.
- **Evolution**: Early versions had non-linear residuals; later versions removed them for better performance.

### 3. ConvNeXt
- **Context**: Post-transformer era, inspired by transformer designs.
- **Structure**: A residual network with a redesigned block:
  - Starts with a convolution increasing kernel size (unlike traditional downsizing to 64 channels).
  - Two 1x1 convolutions mimic a transformer’s MLP, expanding then contracting channels (e.g., 96 to 384 to 96).
- **Significance**: Matches state-of-the-art transformer performance while retaining convolutional efficiency.

### 4. Vision Transformer (ViT)
- **Concept**: Chops images into patches, feeds them into a transformer encoder with positional encodings and a classification token.
- **Complexity**: Scales as $O(n^2)$ due to attention over all patches, limiting resolution or patch granularity.
- **Strength**: Outperforms ConvNets with sufficient data, despite no vision-specific tailoring, due to the transformer’s robust design.

### 5. Swin Transformer
- **Innovation**: Uses a sliding window for local attention, shifting windows across layers to enable global communication.
- **Benefit**: Convolution-like locality reduces complexity, making it versatile beyond classification (e.g., detection, segmentation).

### 6. DINO
- **Description**: A ViT variant pre-trained on a large, curated Facebook dataset, not ImageNet.
- **Training**: Uses **exponential moving average (EMA)** to create a teacher model supervising a student:
  - Matches classification outputs.
  - Predicts embeddings for masked patches, forcing contextual reasoning.
- **Significance**: General-purpose backbone outperforming ImageNet-trained models, less biased toward classification.

## Trends in Image Classification
The lecture concludes with three major trends:

1. **Stable Architectures**:
   - New architectures emerge less frequently. ResNet dominated for four years, and Vision Transformers have persisted for over five, suggesting maturity in design.

2. **Declining Value of Labels**:
   - ImageNet pre-training is less critical. Unsupervised methods (e.g., DINO) and captioning datasets (e.g., internet images with alt text) offer richer, scalable supervision.

3. **Shift in Evaluation**:
   - Models are rarely trained and tested solely on ImageNet. Common approaches include:
     - **Zero-shot**: Using language components to predict labels without training.
     - **Frozen Encoder**: Fine-tuning only the classifier on ImageNet.

## Conclusion
Image classification, while narrow, has driven deep learning’s evolution through simple yet powerful datasets and architectures. From CIFAR-10 to JFT-3B, and AlexNet to DINO, the field has matured, with current trends favoring stable, versatile models and broader data sources over traditional labeled datasets like ImageNet.