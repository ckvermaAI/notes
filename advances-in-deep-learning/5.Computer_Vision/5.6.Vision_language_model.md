# Vision and Language Models

## Introduction to Vision and Language Models
The lecture frames computer vision as transitioning from traditional deep learning approaches (e.g., image classification, object detection) to a unified paradigm integrating **language models**. Earlier segments hinted at this shift (e.g., open vocabulary detection), and this segment dives into **vision and language models**, focusing on early examples. These models generalize **image classification**—originally assigning one label per image—into **captioning**, producing short sentences to describe images. This evolution addresses limitations of classification while leveraging abundant web data.

## Limitations of Image Classification
Image classification, a foundational task, assigns a single label (e.g., "dog") to an image:
- **Applications**: Limited to visual search.
- **Value**: A testbed for network design and evaluation, simple to implement.
- **Issues**:
  1. **Monolithic Treatment**: Assumes one dominant object, ignoring complexity (e.g., multiple objects or contexts).
  2. **Single Label Constraint**: Oversimplifies rich image content, reducing it to one word.

**Captioning** overcomes this by generating descriptive sentences (e.g., "A dog chasing a ball"), capturing more detail and nuance.

## Data Advantage: Web Alt Text
Captioning data is cheaper than single labels, counterintuitively:
- **Source**: **Alt text** in HTML, added by web designers to describe images for accessibility (e.g., for visually impaired users via screen readers) or as placeholders in early internet browsers.
- **Abundance**: Widely available on the web, unlike curated labels from Mechanical Turk (e.g., ImageNet).
- **Scale**: OpenAI capitalized on this, crawling vast amounts of internet images with alt text, forming the basis for models like **CLIP**.

## CLIP: Contrastive Language-Image Pretraining
**CLIP**, developed by OpenAI, is a pioneering vision-language model:
- **Dataset**: A massive collection of internet images and alt text, cleaned up extensively (exact size unspecified but implied to be near-exhaustive).
- **Architecture**:
  - **Image Encoder**: A Vision Transformer (ViT) or convolutional variant, akin to ImageNet classifiers.
  - **Text Encoder**: A GPT-like model, embedding text (up to 64 characters) into fixed-size vectors.
- **Training**:
  - **Objective**: Align image and text embeddings so matching pairs (e.g., image and its alt text) are close in embedding space, while non-matching pairs are far apart.
  - **Loss**: **Contrastive loss** with two softmaxes:
    1. For an image, classify its correct text among a batch.
    2. For a text, classify its correct image among a batch.
  - **Batch Processing**: Encodes batches of images and texts, forming a comparison matrix where correct pairs are positive, others negative.
- **Outcome**:
  - Generalizes classification to arbitrary English sentences, not fixed labels.
  - Achieves near state-of-the-art ImageNet performance without ImageNet training, highlighting **scale’s importance** over curated data.
- **Limitations**:
  - Solves the label issue but not the monolithic image problem—loses fine details irrelevant to alt text.
  - Poor localization (e.g., cannot pinpoint objects for detection).

## Replication Efforts
CLIP’s data remained proprietary, prompting academic replications:
1. **OpenCLIP and LAION**:
   - **Dataset**: LAION downloaded ~2 billion images with alt text, using OpenAI’s CLIP to filter quality pairs.
   - **Challenges**: Included not-safe-for-work (NSFW) content, making it legally risky (e.g., a crime in some regions, sensitive for universities/companies).
   - **Outcome**: Replicated CLIP’s research but couldn’t publish the dataset due to legal/ethical issues.
2. **DataComp**:
   - **Dataset**: Started with ~13 billion viable images from the internet (half skipped, 25% links broken, 10% discarded for NSFW/illegal content).
   - **Approach**: Shifted focus from model design to **data curation**:
     - Released raw data as a competition task: participants select/delete samples, add external data, then train a fixed CLIP model.
     - Baselines filtered by:
       - **CLIP Score**: Discarded low-scoring image-text pairs.
       - **Text Rules**: Removed captions too long, short, non-descriptive, or redundant.
       - **Image Clustering**: Kept features close to ImageNet’s curated set (similar to Meta’s DINOv2 approach).
   - **Results**: Reducing to 1–4 billion images maintained strong performance, but baselines were hard to beat.
   - **Significance**: Flipped research from method innovation to data selection, emphasizing data’s role in vision-language models.

## Alternative Approach: CapPa
**CapPa** predicts captions from images, differing from CLIP’s embedding alignment:
- **Architecture**:
  - **Image Encoder**: Similar to CLIP’s.
  - **Text Decoder**: Generates captions instead of embeddings.
- **Decoding Variants**:
  1. **Autoregressive**: Predicts tokens sequentially (e.g., "A cow…" → "A cow in…"), simpler as later tokens rely on context, using a masked, causal language model.
  2. **Parallel**: Predicts all tokens at once, harder but provides richer supervision (no causal mask, fully connected self-attention), despite weaker language modeling.
- **Performance**:
  - Matches CLIP for classification (single-label extraction).
  - Excels at captioning and OCR due to its design.
  - Poor localization, like CLIP, as it treats images monolithically.

## Localized Captioning: LoCa
**LoCa**, developed at Google, improves localization:
- **Dataset Creation**:
  - Starts with off-the-shelf image-text pairs.
  - Uses an enhanced **open vocabulary detector** (from the previous segment) to annotate objects with arbitrary descriptions (e.g., "red car," "tall tree"), all automated.
- **Architecture**:
  - **Vision Transformer**: Encodes the image.
  - **Transformer Decoder**: Produces text.
- **Tasks**:
  1. **Captioning**: Generates a sentence for the whole image (like CapPa).
  2. **Referring Expressions**: Pairs text with bounding boxes (encoded as text-based coordinates), linking descriptions to regions.
  3. **Grounded Captioning**: Given a box, generates text; or given text, predicts a box (forcing detection).
- **Training**: Autoregressive decoding integrates these tasks, compelling the model to align text and spatial regions.
- **Capabilities**:
  - Excels in classification, detection, captioning, and **visual question answering (VQA)** by understanding image parts.
  - Outperforms CLIP/CapPa in localization-heavy tasks.

## Characteristics of Early Vision-Language Models
- **Simplicity**: Map images to text (CLIP, CapPa) or add basic grounding (LoCa), lacking complex interactions.
- **Limitations**:
  - **Localization**: CLIP and CapPa discard spatial details; LoCa improves but is still basic.
  - **Text Complexity**: Limited to ~60 characters, missing nuances (e.g., "rider on horse" vs. "horse on rider").
  - **No Dialogue**: No user interaction, just static mappings.
- **Impact**:
  - Jumpstarted vision-language research and industry.
  - Replaced traditional pretraining (e.g., ImageNet) with captioning data, offering richer supervision and larger scale.

## Trends and Stability
- **Architectural Stability**: Use off-the-shelf components (ViTs, Transformers, GPT-like decoders) with little innovation, relying on data scale.
- **Data Exhaustion**: The internet’s image pool is largely tapped (e.g., 13 billion in DataComp), shifting focus to curation, cleanup, and pseudo-labeling for more information.

## Conclusion
Early vision-language models like CLIP, CapPa, and LoCa generalize classification into captioning, leveraging vast internet alt text. CLIP’s contrastive approach, CapPa’s decoding variants, and LoCa’s localization highlight diverse strategies, all proving data scale trumps curated sets. While primitive—lacking spatial finesse or complex text—they’ve redefined pretraining and set the stage for more advanced, multimodal models, to be explored in future segments.