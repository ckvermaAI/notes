# Segmentation

## Introduction to Segmentation
The lecture transitions from image classification (labeling an entire image) and object detection (localizing objects with bounding boxes) to **segmentation**, which aims to understand images at the pixel level. Segmentation assigns meaning to every pixel, providing a detailed breakdown of an image’s content. Three types of segmentation are introduced:

1. **Instance Segmentation**:
   - Groups pixels belonging to the same object instance (e.g., distinguishing two separate people or four individual shoes in an image).
   - Focuses on identifying unique objects rather than categories.

2. **Semantic Segmentation**:
   - Groups pixels of the same category or "stuff" (e.g., all road pixels labeled as "road," all tree pixels as "tree"), regardless of individual instances.
   - Does not differentiate between instances within a category (e.g., all trees are one "tree" class).

3. **Part Segmentation**:
   - Segments distinct parts of an object (e.g., arms, legs, torso of a human body).
   - Often nested within instance segmentation, using similar techniques, and popular in fields like fashion for body part analysis.
   - Skipped in detail as its technology overlaps with instance segmentation.

The lecture focuses on **semantic segmentation** first, as it was the earliest segmentation task tackled by deep learning, followed by **instance segmentation**, with a note that understanding instance segmentation covers part segmentation.

## Semantic Segmentation

### Concept and Approach
Semantic segmentation labels every pixel with a class (e.g., "person," "road," "tree"). It’s framed as a **per-pixel classification problem**, extending image classification to every pixel rather than the whole image. This simplicity makes it an elegant engineering challenge:
- **Input**: An image.
- **Output**: A class label per pixel, supervised with **cross-entropy loss**.

### Early Methods: Fully Convolutional Networks (FCNs)
The earliest approach leverages **convolutional neural networks (CNNs)** made **fully convolutional**:
- A classification CNN slides over the image, patch by patch, predicting a label for each region.
- **Efficiency**: Shared computations in overlapping regions (due to convolution’s locality) make this computationally feasible, unlike processing each pixel independently.
- **Stride**: Determines patch size and output resolution (e.g., a larger stride reduces resolution but speeds up computation).
- **Impact**: FCNs delivered a significant performance leap for semantic segmentation, surpassing the initial boost AlexNet gave to image classification.

### Evolved Architectures
Post-2010s, architectures evolved beyond sliding classifiers:
- **Downsampling and Upsampling**: Networks narrow (downsample) to capture context, then widen (upsample) to restore resolution.
- **Skip Connections**: Link layers of the same resolution across downsampling and upsampling stages, preserving fine details.

Two prominent architectures:
1. **U-Net**:
   - A single block that downsamples then upsamples, resembling a "U" shape.
   - Widely used beyond segmentation (e.g., denoising in diffusion models, medical imaging for MRI/CT anomaly detection).
2. **Stacked Hourglass**:
   - Repeats the U-Net structure multiple times, stacking "hourglasses."
   - Offers higher resolution outputs with similar efficiency.

Both are versatile for **image-to-image tasks** (e.g., input image → output map), remaining efficient and adaptable.

### Modern Example: DepthPro
- **Context**: A transformer-based network from Apple, loosely inspired by U-Net/hourglass.
- **Approach**:
  - Processes multiple image resolutions through a patch-based transformer encoder.
  - Merges resolutions into a unified output via concatenation and decoding.
- **Output**: Predicts a **depth map** (distance from camera per pixel) instead of class labels, plus focal length to resolve depth ambiguity.
- **Training**:
  - Stage 1: Real-world and synthetic depth data (synthetic from rendering engines).
  - Stage 2: Fine-tuned on synthetic data only, as real-world depth lacks precision.
  - Supervision includes depth gradients and Laplacian for sharp boundaries.
- **Performance**: Generates metrically accurate depth maps in <1 second at 1500x1500 resolution, enabling applications like novel view synthesis (rotating a scene with occlusion caveats).
- **Significance**: Shows semantic segmentation principles applied to geometric tasks.

### Status and Applications
- **Maturity**: A mature field, with little architectural change in the last five years.
- **Applications**:
  - **Monocular Depth Estimation**: Predicting depth from single images.
  - **Lane Detection**: In autonomous vehicles (e.g., Kinect).
- **Challenges**: Requires dense, pixel-level annotations, which are costly and scarce at scale, limiting further growth.

## Instance Segmentation

### Concept and Relation to Object Detection
Instance segmentation identifies pixels belonging to specific object instances, akin to object detection but with precise outlines (masks) instead of boxes. It’s an extension of detection, focusing on fine-grained object delineation (e.g., two people as separate instances).

### Dataset: Microsoft COCO
- **Details**: 200,000 images, 80 classes, 1.5 million labeled objects.
- **Annotations**: Initially instance segmentations (object outlines), now includes "stuff" labels (e.g., road, sky).
- **Role**: Dominant dataset for instance segmentation, also used in object detection.

### Other Datasets
- **Driving Datasets**: Smaller, exhaustively labeled for pixel-accurate driving scenes (e.g., roads, vehicles), but expensive.
- **Synthetic Datasets (e.g., CARLA)**: Rendered driving scenes provide free semantic and instance labels, offsetting annotation costs.

### Early Architecture: Mask R-CNN
- **Base**: Extends **Faster R-CNN** (two-stage object detector).
- **Process**:
  - CNN generates a feature map.
  - Region Proposal Network (RPN) identifies potential objects.
  - Features are extracted, classified, and bounding boxes regressed.
  - Adds mask regression: Predicts a foreground/background mask per region.
- **Output**: Detects objects and outputs masks, achieving instance segmentation.
- **Challenges**:
  - Overlapping boxes require post-processing (e.g., Non-Maximum Suppression).
  - Ground truth assignment during training is heuristic-driven.
  - Ambiguity in overlapping instances (e.g., prioritize one or count all?).

### Transformer-Based Evolution: MaskFormer and Mask2Former
- **Base**: Builds on **DETR** (transformer-based object detector).
- **Process**:
  - Encodes image into a feature map.
  - Uses a transformer decoder with cross-attention from queries to features.
  - Predicts class labels and masks (via dot product of a query feature vector and a high-resolution feature map from a pixel decoder).
- **Mask2Former Enhancements**:
  - Multi-resolution mask prediction.
  - Masks influence attention, focusing on relevant features.
- **Flexibility**: Handles both instance segmentation (object-specific masks) and semantic segmentation (category-specific masks), enabling **panoptic segmentation** (combining instances and stuff).

### Segment Anything (SAM)
- **Concept**: A general-purpose segmentation model based on MaskFormer principles.
- **Architecture**:
  - Image encoder produces embeddings.
  - Cross-attention from queries to embeddings generates masks.
- **Queries**: Flexible inputs—points, boxes, regions, or text (via CLIP embeddings in early experiments).
- **Data Collection**:
  - **Scale**: 11 million high-resolution images with segmented objects.
  - **Process**:
    1. **Manual Assistance**: Users refine initial segmentations from a small dataset-trained model.
    2. **Semi-Automatic**: A more accurate model densely samples points, generates masks, and users label missed objects.
    3. **Automatic**: Final model labels remaining data without intervention.
- **Training**:
  - Uses masks from the dataset, sampling points, boxes, or regions as queries.
  - Trains to predict masks from these queries, scaling to vast object counts.
- **Impact**: Extends to high-resolution images and videos, potentially replacing specialized segmentation tasks with query-conditioned mask prediction.

## Panoptic Segmentation
- **Definition**: Combines instance segmentation (objects) and semantic segmentation (stuff), labeling everything densely.
- **Trend**: Evolving from instance segmentation, with Mask2Former and SAM enabling this unified approach.

## Trends in Segmentation
1. **Semantic Segmentation’s Decline**:
   - Peaked around 2016, losing ground due to costly data needs and limited scalability.
   - Superseded by instance and panoptic approaches benefiting from richer data.

2. **Generalizing Architectures**:
   - From Mask R-CNN (detection-based) to MaskFormer (unified segmentation) to SAM (zero-shot, query-driven).
   - Simplifies segmentation into grouping pixels, reducing reliance on class-specific datasets.

3. **Decoupling from Labels**:
   - SAM exemplifies a shift away from predefined labels, focusing on mask generation guided by flexible queries (points, text), making off-the-shelf models broadly applicable.

## Conclusion
Segmentation has progressed from semantic (classifying all pixels) to instance (delineating objects) and panoptic (unifying both). Semantic segmentation, mature and peaked, supports applications like depth estimation but wanes due to data constraints. Instance segmentation, active and evolving, leverages datasets like COCO and transformers (e.g., Mask2Former, SAM) to become general-purpose, label-agnostic, and poised to dominate future segmentation tasks.