# Current Vision Language Models

## Introduction to Current Vision Language Models
The lecture transitions from earlier topics—image captioning and early vision-language models capable of dialogues—to **current vision-language models** approaching universal computer vision systems. These models surpass simple conversations, aiming to replace traditional task-specific vision systems by handling diverse inputs and outputs with flexibility and generality.

## Ferret: Visual Grounding Beyond Boxes
**Ferret**, from Apple, advances **visual grounding** beyond traditional bounding boxes:
- **Capabilities**:
  - **Output Grounding**: Produces special tokens for box coordinates (similar to Unified IO), using natural language (e.g., "top-left: (x1, y1), bottom-right: (x2, y2)") to indicate regions, enabling the model to point at referenced objects.
  - **Input Grounding**: Accepts user-specified regions (e.g., boxes, points, arbitrary masks) with text prompts (e.g., "What is this?"), focusing attention on specific areas.
- **Mechanism**:
  - Encodes the entire image via an image encoder (e.g., CLIP-like).
  - Samples features from user-defined regions:
    - Dense sampling, clustered via **K-means** into $k$ tokens (e.g., $k=10$ for uniform coverage).
    - For boxes, samples within; for points, samples nearby; for masks, adapts accordingly.
  - Tokens feed into a language model (similar to LLaVA’s pipeline).
- **Dataset**: **GRIT** (Grounded and Refer Instruction Tuning) includes grounded Q&A pairs (e.g., "What’s in this region?" → "A corgi"), auto-generated for scale.
- **Applications**:
  - Interactive dialogues with pointing (e.g., user points, Ferret responds; Ferret points to clarify).
  - **Ferret-UI**: Tailored for phone UI interactions using UI-specific data, supporting tasks like "Open the phone app and dial this number."
- **Performance**:
  - Strong on benchmarks like **Ferret-Bench** (custom) and UI understanding.
  - Mixed results elsewhere: excels at element recognition but struggles with complex interactions (e.g., worse than GPT-4V), as it wasn’t trained for advanced tasks.
- **Ferret-2**: Enhances Ferret with:
  - Multi-resolution encoding (from LLaVA).
  - **DINO features** alongside CLIP for richer vision, plus higher-resolution pretraining.
  - Improved results, maintaining the grounding focus.

## Custom Work: 3D Spatial Understanding
The lecturer’s student project extends grounding to 3D:
- **Data**:
  - Converts 2D/3D vision datasets (e.g., driving data with 3D annotations) into dialogues:
    - Object detection: "Where’s the bus?" → 3D coordinates.
    - Grounding: Pointing to objects with coordinates.
  - Uses ground-truth annotations, not synthetic Q&A.
- **Training**: Instruction tuning with dialogues and 3D coordinate prediction.
- **Emergent Behavior**:
  - Despite lacking explicit relational data (e.g., "What’s next to the blue car?"), the model generalizes:
    - Combines dialogue knowledge (e.g., "Where do I sleep?" → "bed") with 3D grounding to output bed coordinates.
  - Spatial understanding emerges from basic 3D supervision (e.g., "This is the car, here’s its 3D location").
- **Results**: Strong on 2D/3D grounding, referring expressions, and VQA, leveraging sparse spatial annotations translated into text.

## Spatial VLN: Synthetic Spatial Q&A
**Spatial VLN** (Vision-Language Navigation) takes a different approach:
- **Data**: Generates synthetic Q&A pairs from 3D annotations (e.g., "Does a stool lie in front of the microwave?" → Yes/No, derived from box positions).
- **Goal**: Teaches 3D spatial reasoning via natural dialogue.
- **Results**:
  - Valuable dataset, but mixed performance.
  - Questionable baselines (e.g., GPT-4V at 1% success rate, implausible), suggesting calibration issues; Spatial VLN outperforms but isn’t exceptional.
- **Significance**: Emphasizes dialogue-driven spatial understanding, contrasting with direct coordinate prediction.

## Chameleon: Unified Token-Based Model
**Chameleon** aims for a fully unified vision-language system:
- **Approach**: Encodes all inputs/outputs (text, images) as tokens in a single autoregressive model.
- **Tokenization**:
  - Custom tokenizer atop VQ-VAE, targeting 512x512 resolution (long sequences, e.g., thousands of tokens).
  - Supports text-only, image-then-text, text-then-image, and interleaved sequences.
- **Training**:
  - **Stage 1**: Large-scale pretraining on mixed data (text, image-text pairs).
  - **Stage 2**: Reduces Stage 1 data weight, adds curated high-quality data; 4 million CPU hours (~$millions).
  - **Challenges**: Stability issues—attention softmax favored vision or language tokens, requiring dropout removal and normalization tweaks.
  - **Fine-Tuning**: Supervised and instruction tuning for task versatility.
- **Capabilities**:
  - Outputs text, images, or both (e.g., "Write about penguins with pictures").
  - Grounding possible via image outputs (e.g., highlighting regions).
- **Results**: Matches or exceeds GPT-4V and Gemini in human evaluations; visually impressive outputs.
- **Significance**: A compute-intensive, ambitious unification, unreplicated due to cost.

## PaliGemma: Task-Specific Prompting
**PaliGemma** adapts pretrained models for broad vision tasks:
- **Architecture**: Combines unimodal pretrained models (e.g., CLIP for vision, text encoder).
- **Pretraining**:
  - Prefix text specifies tasks: "Caption: English," "OCR," "Question: English," "Detect: object," "Segment: object," "Caption: region."
  - Outputs match the task (e.g., caption, OCR text, boxes).
- **Stages**:
  - Stage 1: Pretrains on diverse datasets (captioning, OCR, Q&A, detection, segmentation).
  - Stage 2: Increases resolution.
  - Fine-tuning on small datasets excels at traditional vision tasks.
- **Significance**: Flexible task-switching via English prompts, approaching zero-shot generality with fine-tuning.

## Moondream with PixMo: Data-Driven Versatility
**Moondream**, paired with the **PixMo** dataset, expands capabilities:
- **PixMo Dataset**:
  - **Voice Captioning**: Users caption images vocally, cleaned by LLaMA, mimicking natural speech.
  - **Q&A Pipeline**: Generates Q&A from captions/OCR, refined by users.
  - **Pointing Data**: Users point at objects with category labels.
  - **Detection/Captioning**: Dense datasets (e.g., watch faces).
  - **Code Figures**: LMs generate Q&A from figure code (e.g., paper diagrams).
- **Training**:
  - Supervises captioning, Q&A, pointing (e.g., "Where can I write?" → multiple points), counting (point then number), and synthetic tasks.
- **Significance**: Open-source data enables pointing, counting, and beyond-text outputs, showcasing data’s role.

## Trends in Current Vision Language Models
1. **Data Importance**:
   - Progress hinges on creative data generation (e.g., GRIT, PixMo, synthetic 3D Q&A).
   - Existing vision datasets are translated into text, but new tasks demand innovative supervision.
2. **Generalization**:
   - Models aim for zero-shot versatility via English prompts (e.g., PaliGemma hints; Ferret/Chameleon enable grounding).
   - Emerging capabilities (e.g., 3D reasoning, screen interaction) suggest broader potential.
3. **Future Vision**:
   - Universal systems will process any visual input and produce any describable output, marginalizing specialized models.
   - **Specialized Systems**: Persist for reliability/speed (e.g., automotive detection).
   - **General Adoption**: Most users will favor generalist models, resorting to specialized ones only if needed.

## Conclusion
Current vision-language models like Ferret, Chameleon, PaliGemma, and Moondream transcend dialogues, integrating grounding, 3D reasoning, and multimodal outputs. Data innovation drives progress, with models approaching zero-shot universality. While specialized systems endure for niche reliability, generalist models promise a future where vision tasks are universally accessible via natural language, concluding the vision section.