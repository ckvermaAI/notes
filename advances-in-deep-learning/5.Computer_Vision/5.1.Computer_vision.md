# Computer Vision

## Introduction to Computer Vision in Deep Learning
- **Historical Context**: Computer vision has historically been the most prominent application domain for deep learning, serving as a key playground for advancing deep learning technologies until around 2021–2022. During this period, large language models (LLMs) began to rise, shifting some focus away from vision.
- **Current State**: As of March 20, 2025 (the current date), computer vision is at a crossroads, split between two paradigms: traditional deep learning-based computer vision and emerging unified computer vision models.

## Two Paradigms in Computer Vision
### 1. Traditional Deep Learning-Based Computer Vision
- **Origins**: This approach began gaining traction around 2012–2014, leveraging task-specific datasets and models.
- **Characteristics**:
  - Focuses on predefined tasks (e.g., object detection, image classification).
  - Models produce task-specific outputs, designed and optimized for individual problems.
  - Relies on breaking down complex problems into simpler, hand-specified sub-tasks.
- **Applications**: Most real-world applications, such as self-driving cars, still operate within this paradigm. For example, autonomous vehicles use task-driven models rather than unified systems that interpret commands via language.
- **Limitations**: Highly effective but time-consuming and resource-intensive due to the need for task-specific datasets, models, and evaluation metrics.

### 2. Unified Computer Vision
- **Emergence**: Over the past few years, there has been a significant push toward unifying computer vision into a single, versatile model capable of handling all vision tasks.
- **Influence of LLMs**: The success and investment in large language models have fueled this trend, as LLMs exemplify the unified approach by processing diverse inputs and outputs through a single framework.
- **Goal**: Eliminate the need for task-specific models by creating a general-purpose model that can adapt to various vision problems, potentially through language-based reasoning.
- **Momentum**: The unified approach has strong support, but it has not yet overtaken traditional methods in practical applications.
- **Architecture Flow**: Most model architectures originate in traditional computer vision and are later adapted to unified models, rather than the reverse.

## Traditional Computer Vision: Understanding Visual Data
- **Objective**: The primary goal is to interpret visual data, which includes:
  - **Images**: Standard 2D photographs.
  - **Videos**: Sequences of images over time.
  - **Range Images**: Images where pixel values represent distances (e.g., depth) rather than colors, captured using technologies like:
    - **Active Methods**: Emit light (e.g., LIDAR) and measure reflections based on phase or time-of-flight.
    - **Passive Methods**: Use stereo vision (e.g., two cameras at different angles) to infer depth.
  - **Other Modalities**: Infrared imaging, etc.
- **Challenges**:
  - Visual data is "messy" and computationally intensive to process, unlike language data, which is human-designed and structured.
  - Humans struggle to manually produce such data, but cameras capture it easily, though interpreting it remains complex.
- **Inverse Graphics Analogy**: Computer vision is sometimes called "inverse graphics," where graphics renders a scene from a description, and vision infers a description from a scene. However, vision often discards more information and operates at a higher level of abstraction than graphics.

## Tasks in Traditional Computer Vision
Tasks in traditional computer vision are categorized into three types based on the granularity of output:

### 1. Global Labeling
- **Definition**: Assigns a single label to an entire image or sequence.
- **Examples**:
  - **Image Classification**: Labels an image with a category (e.g., "cat") or scene type (e.g., "cityscape").
  - **Action Recognition**: Identifies actions in a video sequence (e.g., "running").
  - **Image Retrieval**: Finds the most similar image in a collection to a given query image.
  - **Captioning/Visual Question Answering (VQA)**: Combines vision with language to describe an image or answer questions about it (e.g., "What is in this image?" → "A cat").

### 2. Sparse Labeling
- **Definition**: Labels specific regions or points in an image, more than one output but fewer than all pixels.
- **Examples**:
  - **Object Detection**: Draws bounding boxes around objects and identifies them (e.g., "cat in a box").
  - **Tracking**: Follows objects across video frames using boxes or points.
  - **Pose Estimation**: Tracks key points (e.g., a skeleton) of a person or object, often for actions or gestures.
  - **Face Recognition**: Identifies facial keypoints for expression analysis or lip-reading.
  - **Optical Character Recognition (OCR)**: Detects and reads text in images.
- **Key Property**: Outputs depend on the image’s internal structure, not its resolution.

### 3. Dense Labeling
- **Definition**: Assigns a label to every pixel in the image, producing an output image.
- **Examples**:
  - **Semantic Segmentation**: Labels each pixel with a category (e.g., "sky," "person," "tree").
  - **Instance Segmentation**: Differentiates multiple instances of the same class (e.g., "person 1," "person 2").
  - **Depth Estimation**: Estimates distance from the camera for each pixel, using monocular or stereo images.
  - **Intrinsic Image Decomposition**: Separates lighting/shading ($I = R \cdot S$, where $I$ is the image, $R$ is reflectance, and $S$ is shading) from object color at each pixel.
  - **Dense Tracking (Optical Flow)**: Tracks the motion of every pixel between frames.

## Evolution and Challenges of Traditional Computer Vision
- **Task Proliferation**: By 2020, the number of tasks had grown vast, with major tasks (e.g., classification, detection) nearing saturation in performance, while niche tasks required new datasets and benchmarks.
- **Autonomous Driving Example**: Vision systems in self-driving cars achieved human-level performance for specific tasks, but the approach remained slow and costly due to manual task definition and model tuning.
- **Scalability Issue**: Defining and solving each task individually is inefficient, prompting the shift toward unified models.

## Unified Computer Vision: A New Frontier
- **Motivation**: Addresses the inefficiency of task-specific models by aiming for a single, adaptable system.
- **Vision-Language Models**: A promising approach where tasks are framed as dialogues:
  - **Classification**: "What is in this image?" → "A cat."
  - **Sparse Tasks**: "Where is the cat?" → "At this location."
  - **Action Recognition**: "What is this cat doing?" → "Begging for food."
  - **Dense Tasks**: Models could output images (e.g., depth maps, segmentations) alongside text if trained to do so.
- **Generalization Hope**: Training on diverse inputs and outputs could enable models to generalize across tasks without explicit retraining, relying on reasoning and emergent capabilities.
- **Current Status**: It’s uncertain whether unified or traditional models will dominate, as practical applications still lean on the latter.

## Structure of the Lecture
- **Part 1**: Covers traditional computer vision, including tasks and models.
- **Part 2**: Explores unified models, their potential, and their connection to vision-language paradigms.

## Conclusion
- Computer vision remains a dynamic field, balancing the proven success of task-specific deep learning with the ambitious promise of unified models. The interplay with large language models and the demand for scalable, general-purpose systems will shape its future trajectory.
