# Training and Generation

The lecture focuses on the technical aspects of **training** and **generation (inference)** for large language models (LLMs), specifically transformers. It provides a high-level overview of the LLM pipeline, details the processes of tokenization, training, and generation, and analyzes their computational and memory requirements. The lecture highlights inefficiencies in both training and generation, introduces optimization techniques like activation checkpointing and sequence parallelism for training, and previews solutions like caching and speculative decoding for generation. The accompanying image summarizes the memory, runtime, and forward call requirements for training, training with checkpointing, and generation.

---

## Overview of LLM Pipeline

The lecture begins by recapping the LLM development process:
- **Pre-training**: LLMs are trained on vast text corpora to absorb general knowledge.
- **Instruction Tuning**: Models are fine-tuned to follow instructions and engage in dialogue.
- **Alignment (RLHF/DPO)**: Models are aligned with human preferences using techniques like Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO).
- **Evaluation**: Models are assessed using various datasets to measure performance.

The focus then shifts to the technical infrastructure required for **training** (updating model weights) and **generation/inference** (producing output tokens), emphasizing their similarities and differences.

---

## Tokenization and Workflow

Both training and generation begin with **tokenization**, a process that converts raw text into a sequence of tokens (numerical representations):
- **Training**: The entire training corpus is tokenized.
- **Generation**: The input prompt or instruction is tokenized.

### Training Workflow
- **Forward Pass**: The tokenized sequence is fed through the transformer layer by layer, producing predictions for the next token at each position.
- **Loss Computation**: A loss function (typically cross-entropy) compares predictions to ground truth tokens.
- **Backward Pass**: Gradients are computed and backpropagated through the network to update weights.
- **Optimizer Step**: Weights are updated using an optimizer (e.g., Adam).
- This process repeats iteratively over the training data.

### Generation Workflow
- **Forward Pass**: The tokenized prompt is fed through the transformer to predict the probability distribution of the next token.
- **Token Sampling**: A token is sampled from the distribution and appended to the sequence.
- **Iterative Process**: The updated sequence is fed back into the model to predict the next token, repeating for $ N $ tokens (where $ N $ is the desired output length).
- **Detokenization**: The final token sequence is converted back to text.
- **Streaming vs. Batched**: Generation can be streamed (producing tokens one-by-one) or batched (producing all tokens before detokenization).

---

## Transformer Architecture and Efficiency

Transformers, the backbone of modern LLMs, consist of $ L $ layers, processing a sequence of $ N $ tokens:
- **Input**: A tensor of shape $ [N, d] $, where $ d $ is the embedding dimension.
- **Processing**: Each layer applies attention mechanisms and feed-forward networks (MLPs), with attention being causal (masked to only attend to previous tokens).
- **Training Efficiency**: During pre-training and fine-tuning, transformers are efficient because they can process the entire sequence in one forward pass, predicting the next token at each position simultaneously. This parallelism leverages the causal attention mask, allowing a single loss computation for all positions.
- **Generation Inefficiency**: At inference time, tokens are generated autoregressively (one at a time). Each new token depends on the previous output, requiring a full forward pass through the transformer for each of the $ N $ output tokens.

---

## Computational Analysis: Training

The lecture analyzes the memory and runtime requirements for training, using $ N $ (sequence length) and $ L $ (number of layers) as variables.

### Vanilla Training
- **Peak Memory**:
  - Dominated by activations stored for the backward pass, proportional to the number of tokens and layers.
  - Using **Fully Sharded Data Parallel (FSDP)** reduces memory usage by distributing weights across GPUs, trading off memory for communication.
  - Memory complexity: $ O(NL) $, as activations are stored for each token and layer.
- **Runtime**:
  - Each layer performs attention ( $ O(N^2) $ due to each token attending to all previous tokens) and MLP computations ( $ O(N) $).
  - Total runtime across $ L $ layers: $ O(N^2L) $.
  - For short sequences, optimizations like **Flash Attention** can hide the $ N^2 $ factor, making it feel closer to $ O(NL) $, as MLPs dominate computation.
- **Forward Calls**: 1 per sequence, as the entire sequence is processed in one pass.
- **Limitation**: Vanilla training is memory-intensive, filling up GPU memory quickly (e.g., 500–1000 tokens on good hardware), making it impractical for large models or long sequences.

### Training with Activation Checkpointing
- **Purpose**: Reduces memory usage by recomputing activations during the backward pass instead of storing them all.
- **Peak Memory**:
  - Checkpoints are stored at intervals (ideally $ \sqrt{L} $), reducing memory to $ O(\sqrt{L}N) $.
- **Runtime**:
  - Requires two forward passes per sequence (one to compute checkpoints, another to recompute activations during backpropagation).
  - Runtime: $ O(2N^2L) $, doubling the forward pass cost.
- **Forward Calls**: 2 per sequence.
- **Advantages**:
  - Significantly reduces memory usage, enabling training on larger sequences.
  - Implemented in standard frameworks (e.g., PyTorch), making it accessible.
- **Limitation**: For very long sequences (e.g., 8,000 tokens during pre-training, up to 30,000 during fine-tuning), the $ N $-dependent memory term still dominates, requiring further optimization.

### Sequence Parallelism (Preview)
- **Purpose**: Addresses memory constraints for very long sequences by splitting the sequence across multiple GPUs.
- **Approach**:
  - Each GPU processes a portion of the sequence (e.g., GPU 1 handles tokens 1–1000, GPU 2 handles 1001–2000).
  - Challenges arise in maintaining proper inference, as attention mechanisms require access to all previous tokens.
- **Details**: To be explored in a later segment, focusing on how to manage dependencies across GPUs.

---

## Computational Analysis: Generation

Generation (inference) is inherently less efficient than training due to its autoregressive nature.

- **Peak Memory**:
  - No backward pass, so no need to store activations for gradients.
  - Memory is proportional to the sequence length at each step: $ O(N) $, where $ N $ is the current sequence length (grows with each token generated).
  - Minimal memory usage compared to training, as only the current sequence’s activations are needed.
- **Runtime**:
  - Requires $ N $ forward passes to generate $ N $ tokens, with each pass processing a sequence of increasing length.
  - Each forward pass involves attention ( $ O(N^2) $) across $ L $ layers: $ O(N^2L) $.
  - Total runtime for $ N $ tokens: $ O(N \cdot N^2L) = O(N^3L) $, cubic in sequence length.
- **Forward Calls**: $ N $, one per token.
- **Comparison to Training**:
  - Training runtime is quadratic ( $ O(N^2L) $), manageable for large sequences.
  - Generation runtime is cubic ( $ O(N^3L) $), making it significantly slower.
- **Limitation**: Vanilla generation is computationally expensive, especially for long sequences, making it impractical for real-time applications.

---

## Summary Table from Image

The image provides a concise comparison of memory, runtime, and forward calls for the three scenarios:

| **Metric**      | **Training** | **Training (Checkpointing)** | **Generation** |
|-----------------|--------------|------------------------------|----------------|
| **Peak Memory** | $ O(NL) $  | $ O(\sqrt{L}N) $           | $ O(N) $     |
| **Runtime**     | $ O(N^2L) $ | $ O(2N^2L) $               | $ O(N^3L) $  |
| **# Forward Calls** | 1        | 2                            | $ N $        |

---

## Future Solutions for Generation

The lecture previews two techniques to improve generation efficiency, to be explored in later segments:
1. **Caching Intermediate Computation**:
   - Stores intermediate states (e.g., key-value pairs in attention) to avoid redundant computations for previously processed tokens.
   - Reduces the computational cost of processing the same prefix repeatedly.
2. **Speculative Decoding**:
   - Generates multiple tokens simultaneously by predicting a sequence of tokens and verifying them in parallel.
   - Reuses computation to decode more than one token at a time, reducing the number of forward passes.

---

## Additional Context and Trends

- **Sequence Length Trends**: Recent LLMs are pre-trained on sequences up to 8,000 tokens and fine-tuned on up to 30,000 tokens, reflecting a trend toward handling longer contexts (e.g., for tasks like document summarization or long-form dialogue).
- **Hardware Constraints**: The lecture emphasizes practical limitations, such as GPU memory capacity, which cap sequence lengths in vanilla training setups. Techniques like FSDP, activation checkpointing, and sequence parallelism are critical for scaling to modern LLMs.
- **Transformer Dominance**: Transformers are favored due to their training efficiency, enabled by parallel processing of sequences during training. However, their autoregressive nature at inference time remains a bottleneck.

---

## Conclusion

The lecture provides a technical foundation for understanding training and generation in LLMs, focusing on transformers. Training benefits from parallelism but faces memory challenges with long sequences, addressed by activation checkpointing and sequence parallelism. Generation, however, is computationally expensive due to its cubic runtime complexity, necessitating optimizations like caching and speculative decoding. The analysis underscores the trade-offs between memory and computation, setting the stage for deeper exploration of these optimization techniques in future segments.
