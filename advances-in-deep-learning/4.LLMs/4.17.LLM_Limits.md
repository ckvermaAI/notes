# LLM Limits

## Introduction to LLM Capabilities and Limitations
The lecture begins by recapping the impressive capabilities of large language models (LLMs), describing them as "awesome tools." However, the focus shifts to their limitations, aiming to provide a framework for understanding what LLMs cannot yet do. The discussion is prefaced with an exploration of the political and motivational landscape surrounding LLM limitations, which shapes how these shortcomings are perceived and communicated.

### Three Camps Discussing LLM Limitations
The lecturer identifies three distinct groups with different perspectives on LLM limitations:
1. **Model Builders**:
   - Comprising researchers, engineers, and users, this group is focused on developing new models and advancing the field.
   - Motivations include commercial success (building profitable products), fame, glory, and, for a small subset, the pursuit of artificial general intelligence (AGI).
   - These machine learning experts push the boundaries of research and engineering, often portraying LLMs as limitless or soon-to-be limitless (e.g., reaching human-level intelligence in months). This optimism is partly driven by corporate incentives—downplaying limitations preserves stock value and public perception.
   - Privately, individuals may acknowledge flaws, but the public "company line" emphasizes unrelenting progress.

2. **AI Safety Researchers**:
   - Operating within machine learning, this group studies LLM limitations, biases, and societal dangers (e.g., who is harmed by model failures, how biases are inherited or amplified from training data).
   - The majority aim to understand societal impacts, but a vocal minority sensationalizes risks (e.g., "AI will destroy us all"), giving AI safety a mixed reputation.
   - Compared to model builders, they adopt a conservative, sometimes pessimistic stance, partly as a counterbalance to the builders’ optimism. This mirrors historical debates over nuclear safety in the 1940s-50s, where experts feared catastrophic misuse.

3. **External Analysts (Outside Machine Learning)**:
   - These researchers apply scientific methods from fields like biology, linguistics, or systems analysis to study LLMs.
   - Motivations include academic prestige—well-executed studies yield highly cited papers, funding, and further research opportunities.
   - Their analyses are more balanced and evidence-based but less predictive due to limited domain expertise. They may struggle to assess technical challenges or distinguish temporary inconveniences from fundamental barriers.

The lecturer highlights these camps to contextualize their motivations: builders hype capabilities, safety researchers emphasize risks, and external analysts seek balanced insights, often for academic gain.

## Specific Limitations of LLMs
With the political context established, the lecture delves into specific LLM limitations, supported by referenced studies and examples.

### 1. Hallucination and "Bullshitting"
- **Definition**: The paper "ChatGPT is Bullshit" (term clarified as "bullshitting" from Harry Frankfurt’s book) describes LLMs producing text without regard for truth, distinct from intentional lying. "Bullshitting" prioritizes plausibility over accuracy, often termed "hallucinations."
- **Examples**: LLMs generate convincing but factually incorrect responses (e.g., flawed math reasoning from prior segments), prioritizing semantic coherence over reality.
- **Hard vs. Soft Bullshitting**:
  - **Hard**: Intentional misleading (e.g., flawed chain-of-thought reasoning designed to deceive).
  - **Soft**: Unintentional fabrication without verifying truth (common in GPT models).
- **Paper’s Argument**:
  - LLMs produce bullshit because they’re trained to generate plausible text, not to represent the world or pursue goals like humans.
  - Quotes:
    - "The problem… is that they’re not designed to represent the world at all. Instead, they’re designed to convey convincing lines of text."
    - "ChatGPT is at minimum a soft bullshitter… because it is not an agent."
  - Suggests this is inherent to LLMs, lacking agency or truth-oriented attributes.
- **Lecturer’s Critique**:
  - Agrees the observation of bullshit is spot-on and the hard/soft distinction is useful.
  - Questions the conclusion that LLMs can’t represent the world via text (e.g., Wikipedia’s text conveys significant world knowledge) or that agency is impossible with instruction tuning. Suggests this may be a fixable flaw rather than a fundamental limit.

### 2. Poor Self-Correction
- **Observation**: LLMs struggle to self-correct, as seen in prior segments (e.g., asking "Is this correct?" after a math problem).
- **Study Evidence**: A referenced paper shows that prompting LLMs to reflect on their answers often worsens performance:
  - Correct answers flip to incorrect more frequently than incorrect answers flip to correct.
  - Visualized as "red" (correct-to-incorrect) outweighing "green" (incorrect-to-correct).
- **Implications**: Self-awareness is limited; models lack robust mechanisms to reassess and fix errors post-output.
- **Open Question**: Unclear if this stems from the pre-training/instruction-tuning paradigm or insufficient tuning data.

### 3. Language vs. Reasoning Conflation
- **Human Bias**: Humans equate linguistic fluency with intelligence and reasoning ability, a correlation that holds statistically for people but not LLMs.
- **LLM Disparity**:
  - State-of-the-art LLMs match human language proficiency when prompted correctly but falter at basic reasoning (e.g., math or logic tasks).
  - Early models (e.g., GPT-3, ChatGPT) seemed "magical" due to this fluency, reinforcing the misconception of intelligence.
- **Paper Analysis**: "Let’s Sit Down Here" separates:
  - **Formal Competence**: Producing grammatically correct language.
  - **Functional Competence**: Using language to deliver accurate answers.
  - Tests disentangle linguistic skill from reasoning, showing LLMs excel at the former but not the latter.
- **Lecturer’s View**: A balanced first step in breaking this bias, applicable to both overestimating fluency and underestimating poor speakers.

### 4. Physics of Language Models (Synthetic Data Studies)
- **Approach**: A series of papers examines LLMs on exhaustive synthetic datasets to probe theoretical limits under current architectures.
- **Findings**: Key Results**:
  1. **Context-Free Grammars**:
     - Causal (backward-only attention) LLMs learn to parse context-free grammars via dynamic programming (discovered by gradient descent), unlike bidirectional models (e.g., BERT).
  2. **Mathematical Reasoning**:
     - LLMs learn generalizable strategies for math puzzles, mimicking human problem-solving, not just memorization.
     - Requires deep networks (20+ layers).
  3. **Self-Correction**:
     - Possible only with pre-training data on corrections, not instruction tuning alone.
  4. **Information Storage**:
     - Causal attention excels at compression ($2$ bits per parameter, even quantized to 8-bit), far surpassing traditional compression (e.g., ZIP).
- **Order Sensitivity**:
  - Example: "Claire married Cassie. Who is Barry married to?" struggles due to causal attention’s directional nature (Cassie attends to Claire, not vice versa).
  - Inverting relationships (e.g., "Who married Claire?") is hard unless pre-trained or explicitly re-parsed.
- **Repetition**: Information storage improves with repeated exposure (e.g., celebrity biographies), aiding organization even for rare examples.
- **Implications**: Causal models outperform bidirectional ones in reasoning and storage, but limitations persist in dynamic relationship handling.

## Conclusion and Recommendations
- **Perspective**: LLMs are imperfect, as is their analysis. Claims of limitless potential or insurmountable flaws should be scrutinized.
- **Advice**: Evaluate sources’ motivations (builders, safety researchers, analysts) to contextualize claims and assess their longevity.
- **Closing**: The lecture ends this section, urging critical thinking about LLM limits amidst ongoing advancements.
