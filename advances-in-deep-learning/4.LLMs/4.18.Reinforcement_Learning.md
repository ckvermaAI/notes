# Reinforcement Learning and LLMs

## Teacher Forcing: The Standard Approach
- **Definition**: Teacher forcing is the primary method for training modern LLMs, converting the inherently sequential generation problem into a supervised learning task.
- **Mechanism**:
  - During training, the LLM is given a prompt (context) and the ground truth output from the dataset (minus the last token).
  - The model predicts the next token based on this input, and supervision ensures it matches the correct next token from the ground truth.
- **Surprising Success**: This works despite differing from testing, where the model predicts a token, feeds it back as input, and repeats—only matching the ground truth if every prediction is perfect. The lecturer finds this effectiveness surprising yet foundational to current LLMs.
- **Limitation**: Requires the entire target sequence at training time, preventing flexible supervision (e.g., evaluating only the last few tokens), as the full sequence must be fed in during training.

## Outcome Supervision: A Shift to RL
- **Definition**: Outcome supervision allows freeform generation by the LLM, evaluating the output’s quality (e.g., the last few tokens) rather than enforcing a specific sequence.
- **Example**: For a math question, only the final answer tokens matter, leaving intermediate steps to the model’s discretion.
- **Challenge**: Unlike teacher forcing, this involves sampling tokens and feeding them back into the model, a non-differentiable process that breaks gradient descent compatibility, making it unsuitable for supervised learning.
- **Solution**: This problem can be reframed as an RL problem, where the model learns to optimize outcomes based on rewards.

### RL Formulation
- **Probability Distribution**: An LLM models the probability of the next token given a context $C$, expressed as $P(token | C)$. Chaining these probabilities autoregressively yields a distribution over entire sequences.
- **Sampling**: Tokens or full sequences are sampled from this distribution during generation.
- **Markov Decision Process (MDP)**:
  - Objective: Maximize the expected reward over sampled sequences, written as $E_{s \sim P(s|C)} [R(s)]$, where $s$ is a sequence, $P(s|C)$ is the model’s distribution, and $R(s)$ is the reward/return measuring output quality.
  - Unlike loss minimization, RL maximizes positive rewards.
- **Simplification**: While RL literature uses complex frameworks, for LLMs, it’s a straightforward sampling-and-evaluation process.

### The Reinforce Trick
- **Gradient Computation**: Optimizing $E_{s \sim P(s|C)} [R(s)]$ seems complex, but the "reinforce trick" simplifies it:
  - Gradient: $\nabla E_{s \sim P(s|C)} [R(s)] = E_{s \sim P(s|C)} [R(s) \nabla \log P(s|C)]$.
  - Derivation involves the log-gradient identity: $\nabla \log f(x) = \nabla f(x) / f(x)$.
- **Monte Carlo Estimate**: Instead of computing the full expectation, sample $k$ sequences, evaluate their rewards, and compute a weighted gradient average:
  - Good output (e.g., $R = 1$): Follow its gradient.
  - Bad output (e.g., $R = -1$): Follow the opposite direction.
  - Neutral output (e.g., $R = 0$): Ignore.
- **Implementation**: Uses standard LLM generation and gradient computation (akin to teacher forcing), but with sampled outputs instead of ground truth.

### Vanilla Reinforce Algorithm
- **Process**:
  1. Sample a context $C$ (e.g., a question).
  2. Generate an output sequence.
  3. Compute the return $R$.
  4. Take a gradient step weighted by $R$.
- **Efficiency**: Works with a single sample, though multiple samples improve stability. Positive gradients reinforce good outputs; negative gradients penalize poor ones.
- **Drawback**: High variance if returns aren’t centered (e.g., all positive), slowing convergence.

## Advanced RL Methods
### Policy Gradient and Advantage Functions
- **Improvement**: Policy gradient methods use an advantage function $A(s) = R(s) - V(s)$, where $V(s)$ (value function) estimates the expected return, centering rewards around zero to reduce variance.
- **Components**: Involves training a value network alongside the policy (sampling) network, a complexity avoided in simpler methods.

### Proximal Policy Optimization (PPO)
- **Enhancement**: PPO builds on policy gradient, allowing off-policy updates (reusing samples) via importance weighting per token, not per sequence, avoiding rapid weight decay to zero.
- **Efficiency**: More data-efficient than vanilla reinforce, reusing rollouts longer, but involves multiple networks and complex tuning.

### Simplification via Contextual Bandits
- **Insight**: LLMs align better with contextual bandits (one action per context) than MDPs, treating entire sequences as single actions.
- **Implication**: Vanilla reinforce suffices, avoiding PPO’s complexity.

### Leave-One-Out Reinforce (ReLU)
- **Algorithm**:
  - Sample $k$ sequences per context.
  - Compute a baseline as the average return across all $k$ samples.
  - Use advantage $A = R - baseline$ to weight gradients.
- **Benefit**: Naturally balances good and bad outputs (e.g., 5 good, 5 bad samples cancel out), reducing variance without extra networks.
- **Performance**: Outperforms PPO and RAFT (imitating the best trajectory), as shown in a 2019 workshop paper "Before and For Samples Get a Baseline For Free."

### DeepSeek and R1’s Approach
- **Parallel Discovery**: DeepSeek independently developed a similar method, replacing PPO’s value model with a leave-one-out Monte Carlo estimate, but normalized by standard deviation—a choice the lecturer critiques as skewing weights for uniform success/failure cases.
- **R1 Development**:
  - **Data**: Large dataset of math puzzles with verifiable answers (e.g., multiple choice), though details are scarce.
  - **R1-0**: Trained with "TRPO" (effectively ReLU with normalization), focusing on correct answers, not language quality (mixed languages observed).
  - **R1**: Enhanced with instruction tuning and human reasoning traces, producing a chat-style reasoning model competitive with OpenAI’s offerings.
- **TRPO = ReLU**: R1’s paper notes a single PPO update per exploration, nullifying clipping and reducing TRPO to ReLU with normalization.

## Practical Application: Interactive Agents
- **Example**: The lecturer’s work trained LLMs as interactive agents executing Python API requests in "App World" (24 scenarios).
- **Method**: Used a ReLU/PPO hybrid with leave-one-out advantage, doubling performance over baselines (e.g., o1) on these tasks.
- **Caveat**: Training data overlap is uncertain, though App World’s obscurity suggests novelty.

## Benefits of RL in LLMs
- **Diversity**: RL maintains diverse generation strategies (e.g., 100 unique solutions to one prompt, 98 successful), aiding exploration early and preventing overfitting late, even with few examples (24 scenarios).
- **Simplicity**: Algorithms like ReLU are straightforward—generation, reward computation, baseline subtraction, gradient step—leveraging existing tools.
- **Flexibility**: Enables supervision beyond exact sequences, less data-hungry than supervised fine-tuning.

## Future Outlook
- **Trend**: RL’s integration with LLMs, via general loss functions like outcome supervision, is likely a staple of future fine-tuning, enhancing adaptability and performance.
- **Conclusion**: The lecturer hopes this bonus content was enjoyable, emphasizing RL’s potential to simplify and elevate LLM training.
