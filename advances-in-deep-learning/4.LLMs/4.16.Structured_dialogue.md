# Structured Dialogues

## Introduction to Large Language Models (LLMs) and Information Storage
The lecture begins by wrapping up a section on large language models (LLMs), discussing their construction, pre-training, instruction tuning, and alignment to produce better responses. The focus shifts to enhancing LLM performance by exploring how they store and process information, which is crucial for improving their outputs.

### Where LLMs Store Information
LLMs store information in three primary locations:
1. **Weights**:
   - Information is embedded in the model's weights, including the multi-layer perceptron (MLP) and the attention mechanism within transformer blocks.
   - Transformers use attention not only for reasoning about sequences but also for storing data.
   - In an ideal scenario, transformers can store approximately $2$ bits of information per weight, even when quantized (e.g., to 8-bit integers). This efficiency requires extensive training or overfitting, where the same information is presented in about $1000$ variations during pre-training.
   - Example: Asking "What is the capital of France?" yields "Paris," stored directly in the weights, not retrieved externally.

2. **Special Tokens and Activations**:
   - Transformers leverage special tokens (e.g., beginning-of-sequence tokens) and large activations to store information.
   - In vision-language models, information is often stored in irrelevant background patches of images, as observed in the paper "Transformers Need Registers." These patches act as "registers" to build up data about the input, since they are not critical to the main content.
   - In language models, attention spikes at the beginning-of-sequence token, which serves as a reference point for localizing sequence elements and storing general knowledge across layers.

3. **Context**:
   - The context, including system prompts and provided documents, contains significant information that influences responses.
   - Example: A system prompt like "You are a professor at the University of Texas at Austin with a deep southern accent" alters the model's tone and style (e.g., using "Darlin'" or "City of Lights" for Paris).
   - Techniques like retrieval-augmented generation (RAG) and tool use pack additional data into the context, enhancing the model's ability to respond.

## Improving LLM Performance
The lecture transitions to methods for enhancing LLM performance, emphasizing how structured dialogues manipulate these storage mechanisms to elicit desired outputs.

### In-Context Learning
- **Definition**: In-context learning involves teaching LLMs new tasks by providing examples within the context, leveraging their ability to process contextual information.
- **Example**: Translating English to German in JSON format:
  - Input: "car" → `{"english": "car", "german": "auto"}`
  - Input: "sun" → `{"english": "sun", "german": "sonne"}`
  - Query: "moon" → `{"english": "moon", "german": "mond"}`
  - The model infers the JSON structure and German capitalization rules (e.g., nouns like "Mond" are capitalized) from examples, without explicit instructions.
- **Why It Works**:
  - **Pre-training**: LLMs learn to recognize and replicate patterns (e.g., structured data) from extensive pre-training data.
  - **Instruction Tuning**: Models see in-context prompts with correct answers during tuning, reinforcing this capability.
- **Limitations**: Works well for formatting and simple tasks but struggles with complex reasoning (e.g., math problems like $12345 + 54321$).

### Chain of Thought (CoT)
- **Definition**: CoT prompts LLMs to reason step-by-step before providing an answer, improving accuracy by guiding the thought process.
- **Historical Context**: Pre-instruction tuning, CoT relied on in-context examples with reasoning (e.g., from the original CoT paper). Post-tuning, models can be directly instructed to "think step by step."
- **Example**: Adding $12345 + 54321$:
  - Without CoT, the model may fail or guess incorrectly.
  - With CoT, it might attempt: $1 + 2 + 3 + 4 + 5 = 15$, $5 + 4 + 3 + 2 + 1 = 15$, then $15 + 15 = 30$ (incorrect due to tokenization issues).
  - A better prompt (e.g., "write as comma-separated digits: 1,2,3,4,5 + 5,4,3,2,1") can yield the correct sum, $66666$.
- **Why It Works**:
  - **Computation**: More tokens generated mean more attention-based computation, increasing the chance of a correct answer.
  - **Guidance**: Narrow prompts help the model avoid errors, though it may not truly "reason."
  - **Tokenization Fix**: Rephrasing inputs (e.g., digit-by-digit) bypasses tokenization limitations.
- **Order Matters**: If the answer is forced first (e.g., "Give me the answer in one word, then explain"), the model may guess incorrectly (e.g., $99766$) and justify it with flawed reasoning, unable to retract tokens.

### Variants of Chain of Thought
1. **Self-Consistency**:
   - Query the model multiple times with CoT, sampling answers at a non-zero temperature, then select the most frequent result.
   - Mathematically, this "marginalizes out" reasoning steps, focusing on the answer distribution.
   - Example: Asking $12345 + 54321$ multiple times might consistently yield $66666$, improving robustness.

2. **Tree of Thought (ToT)**:
   - Generates multiple reasoning paths in a tree-like structure, evaluates partial answers with a scoring function, and expands promising branches.
   - Requires a state evaluator to distinguish good vs. poor reasoning, unlike vanilla CoT or self-consistency.
   - Future slides will discuss training an LLM as this evaluator.

3. **ReAct (Reasoning + Acting)**:
   - Integrates CoT with tool use in a loop: Thought → Action (tool call) → Observation → Thought.
   - Example: In a simulator (e.g., AlphaFold), the model reasons about a room, takes actions (e.g., "look at object"), observes results, and repeats.
   - Popular for prototyping robust tool use when standard APIs fail, enhancing traditional action-observation loops with reasoning.

### Reflextion
- **Definition**: Wraps CoT or ReAct in an outer loop where the model (or another model) reflects on its performance and iterates until satisfied.
- **Example**:
  - Query: $12345 + 54321$. Model answers $66666$, then is asked, "Is this correct?" It may affirm or attempt correction (sometimes incorrectly).
  - Smaller models (e.g., 8B-parameter LLaMA 3.1) lack strong self-awareness, often misjudging their answers.
- **Implementation**:
  - Same model reflection: May assume it’s wrong if questioned, leading to errors.
  - Alternating models: One model answers, another evaluates, feeding feedback to refine the response.
- **Connection to RL**: Labeled "self-reinforcement learning" in the original paper, but it’s more akin to planning/exploration, requiring an evaluator (trainable LLM) rather than true RL.
- **Programming Trick**: For coding tasks, the model generates unit tests, iterating until the code passes, using tests as the evaluator.

## Conclusion
- **Current State**: Structured dialogues (in-context learning, CoT, self-consistency, ToT, ReAct, reflection) rely heavily on prompt engineering, developed partly due to GPT’s dominance as a black-box model before open alternatives like LLaMA emerged.
- **Future Directions**: Debate persists on whether prompt engineering will remain key or if reasoning can be embedded in training data, reducing the need for explicit prompts.
- **End Note**: The lecture concludes this segment, setting the stage for further exploration (e.g., vision-language models in the next section).
