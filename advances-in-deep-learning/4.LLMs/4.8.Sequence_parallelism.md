# Sequence Parallelism

This lecture focuses on **sequence parallelism**, a technique to train large language models (LLMs) with extremely long sequences by distributing the sequence across multiple GPUs. It builds on previously discussed parallelism techniques (data, pipeline, and tensor parallelism) and addresses the memory bottleneck in training, particularly with activation checkpointing. The lecture details the implementation of sequence parallelism, emphasizing the challenges with attention mechanisms, and introduces **ring attention** and **distributed FlashAttention** as solutions. It concludes by analyzing the trade-offs, including communication costs, and previews a shift to inference optimization in future segments.

---

## Introduction to Sequence Parallelism

- **Purpose**: Sequence parallelism is presented as the final piece of the puzzle for training large-scale LLMs, enabling the handling of longer sequences beyond the limitations of existing techniques.
- **Context**: The lecture assumes familiarity with prior training optimizations (e.g., from earlier segments) and focuses on extending sequence length $ N $ while managing memory constraints.
- **Problem Statement**: Even with techniques like activation checkpointing, peak memory consumption during training scales as $ O(N \cdot \sqrt{L}) $ or $ O(NL) $ (depending on checkpointing precision), where $ N $ is the sequence length and $ L $ is the number of layers. As $ N $ grows (e.g., 8,000 tokens in pre-training, 30,000 in fine-tuning), this fills TPU/GPU memory, necessitating advanced parallelism.

---

## Recap of Parallelism Techniques

The lecture recaps existing parallelism methods to motivate sequence parallelism:

1. **Data Parallelism**:
   - **Approach**: Distributes different input sequences (e.g., sequence 1, 2, 3) across multiple TPUs (e.g., TPU 1, 2, 3), each processing a sequence with its own copy of the model.
   - **Effect**: Does not reduce model size or memory per device; only increases throughput by parallelizing data processing.
   - **Limitation**: Ineffective for long-sequence memory issues.

2. **Pipeline Parallelism**:
   - **Approach**: Splits the model across GPUs along the layer dimension (e.g., GPU 1 handles layers 1–$ L/3 $, GPU 2 handles $ L/3+1 $ to $ 2L/3 $, GPU 3 handles $ 2L/3+1 $ to $ L $). Can use **Fully Sharded Data Parallelism (FSDP)** to distribute weights and synchronize during forward/backward passes.
   - **Effect**: Reduces memory per GPU by splitting weights, mitigates pipeline bubbles (latency from waiting for layer outputs) by grouping layers across GPUs.
   - **Limitation**: Each GPU still processes the full sequence length $ N $, offering no relief for long sequences.

3. **Tensor Parallelism**:
   - **Approach**: Splits individual layers (e.g., attention heads or MLP weights) across GPUs, synchronizing results within each layer.
   - **Effect**: Can theoretically reduce activation memory by distributing computations, but primarily accelerates forward passes. Weight memory is reduced by splitting across GPUs.
   - **Limitation**: Implementation is complex, and it does not effectively address sequence length-induced memory constraints.

---

## Sequence Parallelism Overview

- **Approach**: Splits the input sequence $ N $ across multiple GPUs (e.g., GPU 1 handles tokens 1–$ N/3 $, GPU 2 handles $ N/3+1 $ to $ 2N/3 $, GPU 3 handles $ 2N/3+1 $ to $ N $).
- **Compatibility**:
  - **MLPs**: Easily parallelized since they process each token independently, requiring only weight replication across GPUs.
  - **Normalization Layers (e.g., LayerNorm)**: Requires synchronization across all tokens for statistics (mean, variance), adding communication overhead. Alternatives like **RMSNorm** (normalizing by activation magnitude without mean subtraction) eliminate this need, simplifying parallelism.
  - **Attention**: The primary challenge, as each token’s attention computation depends on all previous tokens, necessitating inter-GPU communication.

---

## Detailed Implementation of Attention in Sequence Parallelism

### Attention Mechanism Challenges
- In a transformer, attention computes a weighted sum of values $ V $ based on queries $ Q $ and keys $ K $, where the attention score is $ QK^T $. For causal attention (common in LLMs), each token attends only to previous tokens.
- With sequence parallelism, a token on GPU 2 (e.g., token 5) must attend to tokens on GPU 1 (e.g., tokens 1–4), requiring data transfer across devices.

### Ring Attention
- **Concept**: A communication pattern where GPUs form a ring, passing keys $ K $ and values $ V $ sequentially to compute attention.
- **Steps** (as depicted in Figure 2):
  1. **Transmit Key Embeddings** (Figure 2a):
     - Each GPU sends its $ K $ and $ V $ to the next GPU in the ring.
     - All GPUs receive $ K $ and $ V $ from preceding GPUs, enabling computation of attention scores $ QK^T $ for all relevant tokens.
  2. **Compute Attention**:
     - Each GPU calculates the attention matrix (softmax of $ QK^T $) using received $ K $ and local $ Q $.
  3. **Transmit Value Embeddings** (Figure 2b):
     - $ V $ embeddings are sent around the ring, allowing each GPU to compute the final attention output $ \text{softmax}(QK^T)V $.
- **Implementation**: Traditional ring attention involves a full loop, where all GPUs exchange data, even if some tokens are irrelevant (e.g., future tokens in causal attention).
- **Drawback**: 
  - **Imbalance**: With causal attention, only previous tokens matter, so sending $ K $ and $ V $ to all GPUs (including those with future tokens) leads to idle time and unnecessary communication.
  - **Complexity**: Scales with the number of GPUs, increasing communication overhead.

### Distributed FlashAttention
- **Concept**: An optimized version of ring attention, leveraging **FlashAttention** (a memory-efficient attention algorithm) in a distributed setting to reduce idle time and communication.
- **Improvements**:
  - **Elimination of Preemptive Key Sending**: Traditional ring attention sends $ K $ and $ V $ before computing attention. Distributed FlashAttention delays this, computing attention only with necessary data.
  - **Reordering Computations**: Flips the dependency by sending $ Q $ and computing partial results, then sending back final outputs, ensuring all GPUs remain active.
- **Steps** (as depicted in the second image):
  - **DistFlashAttention**:
    - Uses FlashAttention’s fused attention kernel (combining $ QK^T $ computation and softmax) to minimize memory usage.
    - Eliminates sending $ K $ before $ V $, reducing redundant transfers.
  - **Options**:
    - **Option 1: Send $ K, V $**: Traditional approach, sending keys and values first.
    - **Option 2: Send $ Q $, Send Back Result**: Sends queries to preceding GPUs, computes attention, and returns results, halving the ring length by utilizing idle GPUs.
  - **Example Workflow** (second image):
    - 8 workers process a sequence, split across GPUs.
    - Initial state: Each worker holds $ (q_i, k_i, v_i) $ for its token segment.
    - Over 5 time steps, workers exchange $ Q $ and compute attention, finishing without idle phases (e.g., worker 1 computes with $ q_1, k_1, v_1 $ to $ q_8, k_8, v_8 $).
- **Efficiency**:
  - Reduces communication to $ O(\text{\# GPUs}) $ by focusing on causal dependencies.
  - Cuts idle time by 50% (e.g., half the GPUs are busy in vanilla ring attention due to causal masks).
  - Scales attention computation to match sequence parallelism’s benefits.
- **Implementation Note**: Users need not implement this manually; it’s provided in libraries (e.g., based on the referenced paper), but understanding the mechanism aids in optimizing large-scale training.

---

## Computational Impact of Sequence Parallelism

- **Memory and Runtime** (from the third image):
  - **Vanilla Training**: Peak memory $ O(NL) $, runtime $ O(N^2L) $, 1 forward call.
  - **Training with Checkpointing**: Peak memory $ O(NL^{1/2}) $, runtime $ O(2N^2L) $, 2 forward calls.
  - **Training with Checkpointing + Sequence Parallelism**: Peak memory $ O(NL / \#GPU) $, runtime $ O(2N^2L / \#GPU) $, 2 forward calls.
- **Speedup**: Sequence parallelism accelerates training by a constant factor equal to the number of GPUs, distributing the sequence load.
- **Communication Cost**: Attention’s quadratic nature ($ O(N^2) $) introduces communication overhead proportional to $ O(\#GPUs^2) $, limiting scalability.
  - Scales effectively up to 8–16 GPUs on specialized servers but not to hundreds across nodes due to network latency.

---

## Limitations and Trade-Offs

- **Scalability**: Sequence parallelism extends sequence length by a constant factor (e.g., 8x with 8 GPUs) but does not scale indefinitely due to communication overhead.
- **Attention Overhead**: The primary bottleneck, mitigated but not eliminated by ring attention and DistFlashAttention.
- **Hardware Dependency**: Requires high-bandwidth interconnects (e.g., NVLink) for efficient GPU communication, limiting applicability to specific server setups.

---

## Conclusion and Future Direction

- **Summary**: Sequence parallelism, combined with activation checkpointing, FSDP, and optimized attention (ring attention, DistFlashAttention), enables training LLMs on long sequences (e.g., 30,000 tokens). It’s the last major training trick, leveraging GPU distribution to overcome memory constraints.
- **Next Steps**: The lecture shifts focus to inference optimizations (e.g., caching, speculative decoding) and practical applications, concluding the training segment as of March 17, 2025.

---

### Additional Context
- **Trends**: Long-sequence training (e.g., 8,000–30,000 tokens) supports advanced tasks like multi-document reasoning, driving the need for sequence parallelism.
- **Research**: DistFlashAttention builds on Dao et al.’s FlashAttention (2022), adapting it for distributed systems, a key advancement in scalable LLM training.

This summary integrates all transcript points, image details, and additional explanations on ring and distributed FlashAttention, reflecting the state of LLM training techniques.