# Tasks and Datasets

This lecture focuses on evaluating the performance of large language models (LLMs) after their development through pre-training, instruction tuning, and alignment processes (e.g., RLHF or DPO). It explores systematic methods to assess LLMs beyond casual interaction, categorizing evaluations into three main areas: **text understanding**, **programming**, and **safety**. The lecture delves into specific tasks and benchmarks within these categories, discusses evaluation challenges, and highlights the growing difficulty of creating fair and unbiased benchmarks due to LLMs' extensive pre-training data and potential overfitting. It concludes by previewing future topics on infrastructure for serving and fine-tuning models.

---

## Overview of LLM Evaluation

The lecture begins by recapping the LLM development pipeline:
- **Pre-training**: LLMs absorb vast amounts of knowledge from the internet, likened to a "sponge."
- **Instruction Tuning**: The models are shaped into conversational agents capable of dialogue.
- **Alignment (RLHF/DPO)**: The models are refined to provide safe and appropriate responses, aligning with human preferences.

The primary challenge in evaluating LLMs is moving beyond subjective, chat-based assessments, which are biased and lack systematic rigor. The lecture introduces structured evaluation methods using datasets and tasks to assess LLM capabilities objectively.

---

## Categories of Evaluation Tasks

The lecture organizes LLM evaluations into three key categories, each addressing different aspects of model performance:

### 1. Text Understanding
- **Purpose**: Assess the core ability of LLMs to read, comprehend, and respond to text-based inputs.
- **Tasks**:
  - **Reading Comprehension**: Involves processing a text document (e.g., a book or business document) and answering questions based solely on that document, without requiring external knowledge.
    - **Input/Output**: A document and questions (e.g., "Who is Winnie the Pooh’s best friend?" or "What’s the name of the pig-like character?") yield a short, specific answer.
    - **Benchmark**: **DROP (Discrete Reasoning Over Paragraphs)** provides short paragraphs with unique-answer questions (e.g., a number, name, or date).
    - **Evaluation Challenge**: Ambiguity in English phrasing can lead to multiple valid answers (e.g., "John Casey" vs. "John was the kicker"), complicating assessment. Methods like substring matching or using other LLMs to judge meaning equivalence are imperfect.
  - **Common Sense Reasoning**: Requires external knowledge not present in the prompt, relying on the model’s training data.
    - **Input/Output**: A question or prompt (e.g., "How do you separate egg whites from the yolk using a water bottle?") with multiple-choice answers (e.g., "Squeeze the water bottle and press it against the yolk" vs. a distractor like "Press it against coke").
    - **Benchmark**: **PIQA (Physical Interaction Question Answering)** uses real-world-grounded questions from video datasets (e.g., cooking or science videos).
    - **Advantages**: Easier evaluation due to yes/no or multiple-choice formats; distinguishes internal knowledge from document-specific extraction.
    - **Limitation**: Evaluates recognition of correct answers rather than the model’s ability to generate them independently.
  - **World Knowledge**: Tests recall of basic factual knowledge about the world, similar to common sense reasoning.
    - **Input/Output**: Questions (e.g., "What is the embryological origin of the hyoid bone?" or "Why isn’t there a planet where the asteroid belt is located?") with multiple-choice answers.
    - **Benchmark**: **MMLU (Massive Multitask Language Understanding)** covers topics like anatomy and astronomy, focusing on factual recall and reasoning.
    - **Limitation**: Like common sense reasoning, it tests recall rather than generative ability.
  - **Symbolic Problem Solving**: Requires solving logic or math problems using information within the prompt, without external knowledge.
    - **Input/Output**: A problem (e.g., "Janet sells 16 eggs daily, eats 3, bakes muffins with 4, and sells the remainder at $2 per egg—how much does she make?") yields a numerical answer.
    - **Benchmark**: **GSM8K** includes math problems requiring step-by-step reasoning.
    - **Evaluation**: Easy due to concrete answers (e.g., numbers), though tokenization challenges can hinder math performance.
  - **Language Understanding**: Tests linguistic skills without external knowledge, focusing on syntax and semantics.
    - **Input/Output**: Sentences with ambiguous pronouns (e.g., "The trophy doesn’t fit into the brown suitcase because it is too large/small") require inferring the referent (trophy or suitcase).
    - **Benchmark**: **Winograd Schema Challenge** (and its scaled version) removes knowledge-dependent examples (e.g., "Lions aid zebras because they are predators") to focus purely on language skills.
    - **Advantage**: Isolates language proficiency from world knowledge.
  - **Mixed Evaluations**: Combine multiple tasks into larger benchmarks.
    - **Examples**: **Big-Bench** (204 tasks), **AGI Eval** (math, SAT, LSAT problems—though the name "AGI" is criticized as misleading due to models exploiting shortcuts), and **Mosaic** (35 benchmarks covering reading comprehension, common sense, world knowledge, symbolic problem solving, language understanding, and long-context reasoning).
    - **Trend**: Aggregating smaller benchmarks into comprehensive evaluations is increasingly popular for comparing LLMs.

### 2. Programming
- **Purpose**: Evaluate LLMs’ ability to generate and manipulate code, particularly Python due to its prevalence in training data and among developers.
- **Tasks**:
  - **Code Completion**: Models complete code snippets, as seen in tools like **GitHub Copilot**.
  - **Unit Test Generation**: Models write unit tests based on provided code in a dialogue format.
  - **Program Synthesis**: Models create programs from instructions and test cases.
- **Benchmarks**:
  - **HumanEval**: Provides code snippets with documentation for completion, evaluated for functional equivalence with hidden test cases.
  - **Math QA/Symbolic Reasoning in Python**: Translates text-based math problems (e.g., GSM8K) into Python code with variables, testing symbolic reasoning.
- **Observations**: LLMs excel at Python due to abundant training data and developer familiarity, but performance drops with less common languages like Haskell. They are also effective at generating unit tests, showcasing practical utility.

### 3. Safety Evaluations
- **Purpose**: Assess the presence of harmful content, biases, and misleading outputs in LLMs.
- **Aspects**:
  - **Social Biases**: Examine biases related to age, race, gender, and religion.
    - **Example**: Asking "What do you think the plumber’s name is?" often yields a male name due to historical data associations, while "nurse" leans toward female names.
  - **Toxic Content**: Test for swearing, insults, or harassment.
  - **Misinformation**:
    - **Lying**: Intentional manipulation or wrong answers.
    - **Hallucination**: Innocent fabrication due to lack of knowledge, a common undergrad experience where students "bluff" answers.
- **Example**: Prompting with "Debbie Allen was..." might lead to an erroneous association with a criminal, highlighting implicit biases from pre-training data.
- **Challenge**: Hallucination is hard to benchmark due to its subtle nature, while lying is more measurable but still complex.

---

## Challenges in Benchmarking

The lecture identifies significant challenges in creating fair and effective LLM evaluations:

1. **Data Contamination**:
   - Most benchmarks are publicly available online or derived from internet data, which LLMs are trained on.
   - Models trained after a benchmark’s release (even a few months later) likely encounter it during pre-training, biasing performance results.

2. **Overfitting and Commercial Pressure**:
   - LLM performance impacts business outcomes (e.g., stock prices), creating incentives to optimize models for specific benchmarks.
   - Developers may create training data mimicking benchmarks (e.g., MMLU), leading to a slippery slope where models overfit to these tests.
   - Repeated iterations of training based on benchmark feedback introduce human-derived information into the model, undermining fairness.

3. **Lack of Unbiased Evaluation**:
   - Overfitting renders traditional benchmarks unreliable, as models exploit patterns rather than demonstrate genuine intelligence.
   - The lecture suggests that fair evaluation remains an unsolved problem, with no consensus on a solution.

4. **AGI Misnomer**:
   - Benchmarks like AGI Eval imply artificial general intelligence (AGI) if passed, but models often use shortcuts, disproving true AGI claims.

---

## Alternative Evaluation: ChatBot Arena
- **Method**: Users ask the same question to two different chatbots, judge which response is better, and use these pairwise comparisons to compute an **Elo ranking**.
- **Advantages**: Distributed judgments across diverse questions eliminate the need for a fixed question set, providing a dynamic ranking of LLM performance.
- **Limitation**: Relies on human judgment, which may introduce bias, though the scale of comparisons helps mitigate this.

---

## Conclusion and Future Topics

The lecture concludes the overview of LLM training (pre-training, instruction tuning, alignment) and evaluation datasets. It previews the next class segments:
- **Infrastructure for Serving Models**: Techniques for efficient text generation and deployment.
- **Infrastructure for Training/Fine-Tuning**: Methods for fine-tuning pre-trained models on specific tasks, focusing on efficiency for non-scratch training.
- **Specialized Topics**: Advanced engineering aspects of LLMs.

This comprehensive evaluation framework, despite its challenges, provides a foundation for assessing LLMs across text understanding, programming, and safety, guiding their practical application and further development.
