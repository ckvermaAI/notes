# Direct Preference Optimization (DPO)

This lecture explores **Direct Preference Optimization (DPO)**, presented as an efficient and mathematically elegant alternative to **Reinforcement Learning from Human Feedback (RLHF)** for aligning large language models (LLMs) with human preferences. The lecture contextualizes DPO within the broader pipeline of LLM training, highlights its advantages over RLHF, and discusses its limitations. It builds on the previous lecture on RLHF, referencing the mathematical foundations of both methods and illustrating how DPO simplifies the alignment process. The lecture also includes a detailed mathematical derivation, supported by an image, and concludes with a comparison between DPO and RLHF, emphasizing their complementary roles in modern LLM development.

---

## Overview of DPO and Its Context

DPO is introduced as a streamlined alternative to RLHF, which is the final step in converting a pre-trained LLM into a safe and useful chatbot. The lecture recaps the three stages of LLM training:

1. **Pre-training**: The LLM absorbs vast amounts of knowledge, likened to a "sponge" soaking up data.
2. **Instruction Tuning**: The model is fine-tuned to follow instructions and engage in dialogue, making it conversational.
3. **Alignment (RLHF or DPO)**: The model is aligned with human preferences to ensure safety, usefulness, and appropriate responses.

RLHF, as discussed in the previous lecture, involves a complex reinforcement learning (RL) component in its third step due to the non-differentiability of the language model's token sampling process. DPO aims to eliminate this RL component, replacing the second and third steps of RLHF with a single supervised learning objective. The lecture emphasizes that supervised learning is preferred over RL because it is:

- **Well-understood**: Easier to tune and control.
- **Data-driven**: Simply requires feeding the right data to achieve the desired outcomes.

DPO achieves this by leveraging a closed-form solution to the RL objective, making the alignment process more efficient and accessible.

---

## The RLHF Baseline

To understand DPO, the lecture first revisits the RLHF process, which consists of three steps:

### Step 1: Supervised Fine-Tuning (Instruction Tuning)
- The LLM is fine-tuned on prompt-response pairs to make it conversational, as discussed in the previous lecture.

### Step 2: Reward Model Learning
- A reward model $ r(x, y) $ is trained to evaluate the quality of the LLM's responses.
- The reward model uses a **Bradley-Terry** pairwise preference model, where human labelers rank responses (e.g., $ y^+ $ as better than $ y^- $).
- The loss function for training the reward model is:
  $\ell = E_{x, y^+, y^-}  \log \sigma \left( r(x, y^+) - r(x, y^-) \right) $
  
  where:
  - $ r(x, y) $: Reward for prompt $ x $ and response $ y $.
  - $ y^+ $: The better response (positive).
  - $ y^- $: The worse response (negative).
  - $ \sigma $: Sigmoid function.
- This step requires a dataset of human-ranked preferences, typically smaller than the dataset used in the next step.

### Step 3: Reinforcement Learning
- The LLM is fine-tuned to maximize the reward $ r(x, y) $ using an RL algorithm like **Proximal Policy Optimization (PPO)**.
- The RL objective is:
  $
  E_{y \sim P(\cdot|x)} \left[ r(y, x) \nabla \log P(y|x) \right] - \beta D_{KL} \left[ P(y|x) \parallel P_{ref}(y|x) \right]
  $
  where:
  - $ P(y|x) $: The policy (LLM) generating response $ y $ for prompt $ x $.
  - $ P_{ref}(y|x) $: The reference policy (pre-RLHF model).
  - $ \beta D_{KL} $: A KL-divergence term to prevent large deviations from the reference model.
- This step uses a separate dataset of prompts (e.g., customer-generated prompts) to optimize the model, requiring significant computational resources and engineering effort due to the non-differentiable nature of token sampling.

---

## Introduction to DPO

DPO eliminates the RL component of RLHF by combining Steps 2 and 3 into a single supervised learning objective. The key insight of DPO is that the RL objective in RLHF has a **closed-form solution**, which can be used to derive a reward function and subsequently a new loss function that is fully differentiable.

### Step 1: Closed-Form Solution to the RL Objective
- The RL objective in RLHF aims to find a policy $ P(y|x) $ that maximizes the expected reward while staying close to the reference policy $ P_{ref}(y|x) $.
- It is known in RL literature that such objectives have a closed-form solution:
  $
  P(y|x) = \frac{1}{Z(x)} P_{ref}(y|x) \exp \left( \frac{1}{\beta} r(x, y) \right)
  $
  where:
  - $ Z(x) $: A partition function (normalizing constant) that ensures $ P(y|x) $ is a valid probability distribution.
  - $ \beta $: A hyperparameter controlling the strength of the reward weighting.
- However, computing $ Z(x) $ is infeasible in practice due to the vast number of possible responses $ y $, making this solution impractical for direct use.

### Step 2: Deriving the Reward Function
- DPO takes this closed-form solution and rearranges it to express the reward $ r(x, y) $ in terms of the optimal policy $ P(y|x) $:
  $
  r(x, y) = \beta \frac{P(y|x)}{P_{ref}(y|x)} + \beta \log Z(x)
  $
- This equation shows that if we knew the optimal policy $ P(y|x) $, we could compute the reward $ r(x, y) $. However, $ Z(x) $ remains a challenge.

### Step 3: Plugging into the Bradley-Terry Model
- DPO substitutes this reward expression into the Bradley-Terry loss used in RLHF's Step 2:
  $
  \ell_{DPO} = E_{x, y^+, y^-} \left[ \log \sigma \left( \beta \frac{P(x, y^+)}{P_{ref}(x, y^+)} - \beta \frac{P(x, y^-)}{P_{ref}(x, y^-)} \right) \right]
  $
- A "magical" outcome occurs: the $ \log Z(x) $ terms cancel out because they are constant for a given prompt $ x $, eliminating the need to compute the partition function.
- The resulting loss function is fully differentiable because it only involves ratios of probabilities from the current policy $ P(y|x) $ and the reference policy $ P_{ref}(y|x) $, both of which can be computed directly from the LLM.

### Final DPO Objective
- The DPO loss function can be optimized using gradient descent, making it a supervised learning problem. It effectively:
  - Increases the likelihood of tokens in the positive response $ y^+ $.
  - Decreases the likelihood of tokens in the negative response $ y^- $.
- The reference model $ P_{ref}(y|x) $ acts as an anchor, helping the optimization process determine when separation between good and bad responses is necessary.

---

## Advantages of DPO Over RLHF

DPO offers several significant advantages over RLHF, primarily due to its supervised learning framework:

1. **Elimination of RL**:
   - DPO removes the need for reinforcement learning, which is complex and resource-intensive.
   - RLHF's Step 3 requires algorithms like PPO, which involve maintaining multiple models (reference, generator, critic, and reward) and dealing with convergence issues.

2. **Simplified Implementation**:
   - DPO is as easy to implement as supervised fine-tuning, requiring only the current model and the reference model.
   - There is no need for a separate reward model or critic model, reducing the engineering overhead.

3. **Differentiability**:
   - The DPO loss function is fully differentiable, allowing standard gradient-based optimization techniques to be applied.
   - This eliminates the non-differentiability issue in RLHF caused by token sampling.

4. **Efficiency**:
   - DPO is more computationally efficient since it does not require the multiple model copies and iterative sampling of RLHF.
   - It can be implemented with minimal infrastructure, making it accessible to smaller teams.

5. **Supervised Learning Benefits**:
   - Supervised learning is well-understood, easy to tune, and controllable through data selection, unlike RL, which requires specialized knowledge and tuning.

---

## Limitations of DPO

Despite its advantages, DPO has some limitations compared to RLHF:

1. **Dependence on Preference Data**:
   - DPO requires a dataset of pairwise preferences ($ y^+ $ better than $ y^- $) for training.
   - Unlike RLHF, which can use a small preference dataset for Step 2 (reward model training) and a larger prompt dataset for Step 3 (RL), DPO must perform all optimization on preference data.
   - This limits the amount of data DPO can leverage, as preference data is typically more expensive and time-consuming to collect than raw prompts.

2. **Generalization**:
   - DPO performs well on the preference data it is trained on but may not generalize as effectively to unseen data.
   - RLHF, by contrast, can generalize better because the reward model can learn broader patterns from the preference data, which are then applied to a larger dataset during RL.

3. **Sequence Length Issue**:
   - Empirically, DPO tends to produce unnecessarily long sequences compared to RLHF.
   - The lecture notes that the exact reason for this behavior is unclear, but it has been observed consistently in practice.

4. **Performance Ceiling**:
   - RLHF generally has a higher performance ceiling because it can leverage larger datasets and the generalization capabilities of the reward model.
   - DPO, while efficient, may not achieve the same level of alignment on complex tasks due to its reliance on preference data alone.

---

## Practical Implications and Comparison

### Implementation Effort
- **DPO**: As easy to implement as supervised fine-tuning, requiring minimal RL knowledge. It can be set up quickly with standard machine learning infrastructure.
- **RLHF**: Requires significant engineering effort, including expertise in RL, multiple model copies, and handling convergence issues. Scaling RLHF to the largest models may take half a year and a team of engineers.

### Use in Practice
- Many larger companies use both RLHF and DPO because they have complementary strengths:
  - **RLHF**: Better generalization to new, unseen data due to the reward model's ability to learn broad patterns.
  - **DPO**: Efficient and effective on preference data, making it a good choice for quick alignment tasks.
- Together, these methods can produce models comparable to **GPT-3.5** or even **GPT-4** with sufficient data and compute.

### Example Outcome
- A model trained with DPO or RLHF can align with human preferences, such as prioritizing safety. For instance, when prompted with "I want to refocus my diet on only eating apples and supplements," a well-aligned model (using either method) would respond with "I would not recommend..." instead of encouraging an unsafe diet.

---

## Mathematical Summary from the Image

The image provides a concise mathematical overview of DPO, mapping its components to RLHF's Steps 2 and 3:

- **RLHF Step 2 (Reward Model Learning)**:
  $
  \ell = E_{x, y^+, y^-} \left[ \log \sigma \left( r(x, y^+) - r(x, y^-) \right) \right]
  $

- **DPO Loss (Combining Steps 2 and 3)**:
  $
  \ell_{DPO} = E_{x, y^+, y^-} \left[ \log \sigma \left( \beta \frac{P(x, y^+)}{P_{ref}(x, y^+)} - \beta \frac{P(x, y^-)}{P_{ref}(x, y^-)} \right) \right]
  $

- **RLHF Step 3 (Optimization)**:
  $
  E_{y \sim P(\cdot|x)} \left[ r(y, x) \nabla \log P(y|x) \right] - \beta D_{KL} \left[ P(y|x) \parallel P_{ref}(y|x) \right]
  $

- **Closed-Form Solution**:
  $
  P(y|x) = \frac{1}{Z(x)} P_{ref}(y|x) \exp \left( \frac{1}{\beta} r(x, y) \right)
  $

- **Reward Expression**:
  $
  r(x, y) = \beta \frac{P(y|x)}{P_{ref}(y|x)} + \beta \log Z(x)
  $

The image highlights how DPO replaces the RL component with a single differentiable loss function, eliminating the need for a separate reward model.

---

## Conclusion

DPO is a groundbreaking approach that simplifies the alignment of LLMs by replacing the RL component of RLHF with a supervised learning objective. By leveraging a closed-form solution to the RL objective, DPO derives a differentiable loss function that directly optimizes the LLM based on pairwise preferences, eliminating the need for a reward model, critic model, or complex RL algorithms like PPO. This makes DPO easier to implement, more efficient, and accessible to teams without extensive RL expertise.

However, DPO's reliance on preference data limits its scalability and generalization compared to RLHF, and it tends to produce longer sequences. In practice, DPO and RLHF are often used together to combine DPO's efficiency with RLHF's higher performance ceiling, enabling the development of advanced models like GPT-3.5 or GPT-4. The lecture concludes by noting that the next segment will explore datasets, evaluation methods, and engineering challenges in LLM development.

---
