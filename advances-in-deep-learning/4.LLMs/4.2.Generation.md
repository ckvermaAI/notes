# LLM Generation

This chapter explores the mechanisms by which large language models (LLMs) generate text, focusing on the process of producing human-readable outputs from trained autoregressive models. Below is a detailed summary of the key concepts, techniques, and challenges discussed, augmented with additional relevant details.

## Overview of LLMs and Text Generation

- **Definition of LLMs**: Large language models are autoregressive models designed to generate text by predicting the next token in a sequence, based on the probability distribution learned during training. Tokens can represent words, subwords, or other linguistic units, depending on the model's tokenizer.

- **Basic Mechanism of Text Generation**:
  - The process begins with an initial input (e.g., a prompt or a special token like the beginning of sequence token, `<BOS>`).
  - The model outputs a probability distribution over possible next tokens.
  - A token is selected from this distribution, appended to the input, and fed back into the model to predict the next token.
  - This iterative process continues until a stopping condition is met, typically the generation of an end of sequence token (`<EOS>`).

- **Key Challenge**: The model provides a probability distribution over possible next tokens, but the method of selecting a specific token from this distribution significantly impacts the quality and coherence of the generated text. This selection process, known as *sampling*, is the core focus of the chapter.

## Sampling Strategies for Token Selection

The chapter discusses various sampling strategies to convert the model's probability distribution into actual tokens, highlighting their strengths, weaknesses, and practical implications.

### 1. **Greedy Sampling**

- **Description**: Greedy sampling selects the token with the highest probability at each step, effectively choosing the most likely next token without considering future consequences.
  
- **Implementation**:
  - To enforce greedy sampling, the model's *temperature* parameter is set to 0. Temperature controls the sharpness of the probability distribution, and setting it to 0 ensures that only the highest-probability token is chosen.
  - Mathematically, if $p_t$ is the probability distribution over tokens at time step $t$, greedy sampling selects $\text{argmax}(p_t)$.

- **Advantages**:
  - Computationally efficient, as it requires minimal additional processing beyond evaluating the model's output probabilities.
  - Deterministic, meaning it produces the same output for a given input (modulo minor variations due to numerical precision).

- **Disadvantages**:
  - **Lack of Diversity**: Greedy sampling often produces repetitive or overly predictable text, as it fixates on the most frequent patterns in the training data. For example, in the demo using the LLaMA model, starting with the input "A" consistently generated medical texts (e.g., "A 25-year-old woman presents to the emergency department...") due to the prevalence of such patterns in the training corpus.
  - **Local Optimization**: Greedy sampling makes locally optimal decisions without considering the global context, leading to potentially incoherent or "weird" outputs. For instance, it might prioritize a sequence like "A 25-year-old" without ensuring the full sentence remains meaningful.
  - **Bias Toward Frequent Patterns**: The method is biased toward sequences that appear frequently in the training data, even if they are not representative of the desired output diversity.

- **Example Issue**: In the LLaMA demo, greedy sampling repeatedly generated medical texts because tokens like "A 25-year-old" were highly frequent in the training data, overshadowing other possibilities.

### 2. **Beam Search**

- **Description**: Beam search is an optimization technique that explores multiple possible sequences simultaneously, aiming to maximize the overall likelihood of the generated text. Instead of selecting a single token at each step, it maintains a fixed number of candidate sequences (the *beam width*) and extends each by considering the top-probability tokens.

- **Implementation**:
  - At each step, the model evaluates all possible extensions of the current candidate sequences and retains the top $k$ sequences with the highest cumulative probability (where $k$ is the beam width).
  - Mathematically, if $S_t$ represents the set of candidate sequences at time step $t$, beam search selects the top $k$ sequences from $S_t \times V$, where $V$ is the vocabulary of possible tokens, based on the joint probability $P(S)$.

- **Advantages**:
  - Optimizes for global likelihood, potentially producing more coherent sequences compared to greedy sampling.
  - Historically popular in natural language processing tasks before the widespread adoption of large language models.

- **Disadvantages**:
  - **Repetition and Over-Optimization**: Beam search can lead to repetitive outputs, especially when optimizing for likelihood causes the model to favor high-probability but redundant sequences. For example, the chapter mentions the model generating "the University of Texas, the University of Texas..." because the sequence has a high joint likelihood.
  - **Computational Cost**: Beam search is computationally expensive, as it requires evaluating and storing multiple sequences at each step, scaling with the beam width $k$.
  - **Mismatch Between Optimization and Quality**: Maximizing likelihood does not necessarily correlate with generating high-quality, human-like text. The chapter notes that "good optimization is not the same as good generation," highlighting that beam search may produce unnatural or overly rigid outputs.
  - **Obsolescence**: Beam search is largely considered a historical technique and is no longer widely used in modern LLM text generation, having been replaced by more effective sampling strategies.

### 3. **Random Sampling**

- **Description**: Random sampling selects the next token by sampling from the full probability distribution, where each token's selection probability is proportional to its assigned probability by the model.

- **Implementation**:
  - For a probability distribution $p_t$ over tokens at time step $t$, random sampling draws a token $x$ such that $P(x) = p_t(x)$.
  - This process is unbiased, meaning the distribution of generated sequences matches the model's learned distribution over all possible sequences.

- **Advantages**:
  - **Unbiased Generation**: Random sampling faithfully reflects the model's training distribution, ensuring that the generated text aligns with the statistical patterns learned during training.
  - **Diversity**: Unlike greedy sampling, random sampling introduces variability, producing more diverse outputs that can capture a wider range of possibilities.
  - **Computational Efficiency**: Like greedy sampling, it is computationally efficient, requiring only a single token selection per step.

- **Disadvantages**:
  - **Low-Probability Transitions**: Random sampling can occasionally select low-probability tokens, leading to incoherent or nonsensical outputs. For example, in the "Apple is a" example, random sampling might produce "Apple is a delicious company," which is grammatically correct but contextually odd, due to the small but non-zero probability of "company" following "delicious."
  - **Meandering Text**: The chapter notes that random sampling often generates "meandering, weird, long text" that lacks focus, as seen in demos where outputs jumped between unrelated topics like IPOs, necklaces, and academic papers.
  - **Human-Likeness**: While random sampling produces more human-like text compared to greedy sampling, it still struggles with coherence and relevance due to the risk of low-probability transitions.

### 4. **Top-$k$ Sampling**

- **Description**: Top-$k$ sampling restricts sampling to the $k$ most probable tokens at each step, discarding all others. The selected token is then sampled from this reduced set, with probabilities renormalized to sum to 1.

- **Implementation**:
  - For a probability distribution $p_t$ over tokens, top-$k$ sampling identifies the $k$ tokens with the highest probabilities, forming a subset $V_k$. The probabilities of tokens outside $V_k$ are set to 0, and the remaining probabilities are renormalized to form a new distribution $p'_t$, from which a token is sampled.
  - Mathematically, if $V_k = \{v_1, v_2, \ldots, v_k\}$ are the top $k$ tokens, then $p'_t(v_i) = \frac{p_t(v_i)}{\sum_{v_j \in V_k} p_t(v_j)}$ for $v_i \in V_k$, and $p'_t(v) = 0$ otherwise.

- **Advantages**:
  - **Reduced Low-Probability Transitions**: By limiting sampling to the most probable tokens, top-$k$ sampling reduces the likelihood of generating incoherent or nonsensical text, improving coherence compared to random sampling.
  - **Human-Like Outputs**: The chapter notes that top-$k$ samples "sound more human-like" due to the avoidance of low-probability transitions.

- **Disadvantages**:
  - **Bias Toward Frequent Patterns**: Similar to greedy sampling, top-$k$ sampling can become biased toward frequent patterns in the training data, especially if the top $k$ tokens are dominated by a specific genre or structure. For example, in the LLaMA demo, top-$k$ sampling often reverted to medical texts (e.g., "A 56-year-old man...") because medical examples dominated the top-$k$ tokens for initial prompts like "A."
  - **Fixed $k$ Challenge**: Choosing an appropriate value for $k$ is difficult. A small $k$ (e.g., $k=1$) reduces to greedy sampling, losing diversity, while a large $k$ approaches random sampling, reintroducing low-probability transitions. The chapter highlights that "fixing this $k$ is hard," as the optimal $k$ varies depending on the context and token distribution.
  - **Limited Adaptability**: The fixed $k$ does not adapt to the shape of the probability distribution, potentially excluding reasonable tokens in cases where the distribution is flat or including too many in cases where it is peaked.

### 5. **Top-$p$ Sampling (Nucleus Sampling)**

- **Description**: Top-$p$ sampling, also known as nucleus sampling, restricts sampling to the smallest set of tokens whose cumulative probability exceeds a threshold $p$ (the *nucleus*). This approach dynamically adjusts the number of considered tokens based on the probability distribution, unlike the fixed $k$ in top-$k$ sampling.

- **Implementation**:
  - Tokens are sorted by descending probability, and the smallest set $V_p$ is selected such that the cumulative probability of tokens in $V_p$ is at least $p$. Formally, $V_p$ is the smallest set satisfying $\sum_{v \in V_p} p_t(v) \geq p$.
  - Probabilities of tokens outside $V_p$ are set to 0, and the remaining probabilities are renormalized to form a new distribution $p'_t$, from which a token is sampled.
  - Mathematically, $p'_t(v) = \frac{p_t(v)}{\sum_{v' \in V_p} p_t(v')}$ for $v \in V_p$, and $p'_t(v) = 0$ otherwise.

- **Advantages**:
  - **Adaptability**: Top-$p$ sampling adjusts the number of considered tokens dynamically based on the shape of the probability distribution. For example, in cases with many equally probable tokens (e.g., after "Apple is a," where "company," "delicious," "fruit," etc., are likely), $V_p$ includes more tokens, while in cases with a single dominant token (e.g., after "Apple is a delicious," where "fruit" is highly likely), $V_p$ may include only one or two tokens.
  - **Improved Coherence**: By focusing on the most probable tokens while avoiding a fixed cutoff, top-$p$ sampling reduces low-probability transitions, producing more coherent and realistic text compared to random sampling.
  - **Diversity**: The LLaMA demo showed that top-$p$ sampling generated diverse outputs, such as anime-inspired characters and book references, rather than fixating on medical texts, demonstrating its ability to balance coherence and creativity.
  - **Widespread Use**: Top-$p$ sampling is the default sampling strategy in many state-of-the-art language models due to its effectiveness in producing "human-sounding text" with minimal low-probability transitions, provided the $p$ parameter is set appropriately.

- **Disadvantages**:
  - **Parameter Tuning**: While top-$p$ sampling is more robust than top-$k$, the choice of $p$ still requires careful tuning. A very small $p$ can overly restrict the token set, reducing diversity, while a very large $p$ (e.g., $p \approx 1$) approaches random sampling.
  - **Occasional Bias**: The chapter notes that top-$p$ sampling can still produce biased outputs in cases where the training data is heavily skewed, such as generating medical texts for prompts starting with "A 28-year-old," though this occurs less frequently than with greedy or top-$k$ sampling.

### 6. **Min-$p$ Sampling**

- **Description**: Min-$p$ sampling is an alternative to top-$p$ sampling, where tokens are included in the sampling set if their probability is at least $\alpha$ times the maximum probability, where $\alpha$ is a parameter between 0 and 1.

- **Implementation**:
  - For a probability distribution $p_t$, let $p_{\text{max}} = \max_v p_t(v)$ be the maximum probability. The sampling set $V_{\text{min-}p}$ includes all tokens $v$ such that $p_t(v) \geq \alpha \cdot p_{\text{max}}$.
  - Probabilities of tokens outside $V_{\text{min-}p}$ are set to 0, and the remaining probabilities are renormalized to form a new distribution $p'_t$, from which a token is sampled.

- **Advantages**:
  - **Similar to Top-$p$**: Min-$p$ sampling has a similar effect to top-$p$ sampling, focusing on high-probability tokens while dynamically adjusting the sampling set based on the distribution.

- **Disadvantages**:
  - **Less Understood**: The chapter notes that min-$p$ sampling is less widely used and understood compared to top-$p$ sampling. Optimal settings for $\alpha$ are not as well-established, and the LLaMA demo suggested that min-$p$ sampling often reverted to medical texts, indicating potential limitations in its current implementation.
  - **Ongoing Development**: Min-$p$ sampling is mentioned as a competing method, but its adoption is not yet widespread, and further research is needed to determine effective parameter settings.

## Role of Temperature in Sampling

- **Definition**: Temperature is a hyperparameter that controls the "sharpness" or "flatness" of the probability distribution before sampling. It is often described as influencing the "creativity" of the model, though the technical term is more precise.

- **Implementation**:
  - Temperature $T$ modifies the model's logits (raw outputs before the softmax) by scaling them by $1/T$. The modified logits are then passed through the softmax to produce the probability distribution.
  - Mathematically, if $z_t$ are the logits at time step $t$, the probability distribution with temperature $T$ is given by $p_t(v) = \frac{\exp(z_t(v)/T)}{\sum_{v'} \exp(z_t(v')/T)}$.
  - **Effect of $T$**:
    - As $T \to 0$, the distribution becomes sharply peaked, and the highest-probability token dominates, effectively reducing to greedy sampling.
    - As $T \to 1$, the distribution matches the model's original output, preserving the learned probabilities.
    - As $T \to \infty$, the distribution becomes uniform, increasing randomness and leading to "random garbage" outputs.

- **Practical Implications**:
  - **Low Temperature ($T < 1$)**: Enhances coherence by favoring high-probability tokens, but reduces diversity, potentially leading to repetitive or predictable text.
  - **High Temperature ($T > 1$)**: Increases diversity and creativity by flattening the distribution, but risks generating incoherent or nonsensical text. The chapter warns that setting $T \geq 2$ often produces outputs that are "very hard to follow."
  - **Implementation Note**: Adjusting temperature is computationally efficient, as it involves a simple scaling of logits before the softmax, rather than raising probabilities to a power.

## Stopping Generation

- **Mechanism**: Text generation stops when the model produces an end of sequence token (`<EOS>`), indicating that it believes the sequence is complete. The `<EOS>` token is assigned a high probability when the model determines the document or sentence has ended.

- **Special Tokens**:
  - **Beginning of Sequence Token (`<BOS>`)**: Used to initiate generation, providing a starting point for the model. It is particularly important in scenarios where no initial prompt is provided, allowing the model to generate text from scratch.
  - **End of Sequence Token (`<EOS>`)**: Signals the end of generation, allowing the model to produce finite outputs rather than continuing indefinitely.

- **Additional Role of `<BOS>`**:
  - Beyond its role in starting generation, the `<BOS>` token serves as an anchor for the model's attention mechanism. Many tokens in the sequence attend heavily to the `<BOS>` token, using it as a reference point in the attention matrix.
  - The chapter notes that models may "misuse" the `<BOS>` token by storing information in its representation, rather than in the model's weights. This behavior is somewhat mysterious and reflects how LLMs adapt their internal representations during training.

## Challenges and Limitations of Current Generation Methods

- **Tension Between Generalization and Fidelity**:
  - **Generalization**: LLMs must assign non-zero probabilities to a wide range of sequences, including rare or unusual ones, to avoid overly rigid outputs. For example, the model might assign a small but non-zero probability to "Apple is a delicious company" to account for potential creative uses of language.
  - **Fidelity**: At the same time, LLMs must prioritize sequences that are coherent and faithful to the training data, avoiding outputs that "sound weird" or deviate too far from human-like text.
  - **Balancing Act**: Sampling strategies like top-$p$ sampling aim to balance these goals by focusing on high-probability tokens while allowing some diversity, but achieving this balance remains challenging.

- **Repetition of Training Data**:
  - The chapter emphasizes that current generation methods often result in the model "repeating what it has seen during training." For example, the LLaMA demos showed outputs that mirrored medical texts, legal documents, or other frequent patterns in the training corpus.
  - This limitation highlights that LLMs, as trained, are primarily token predictors, not creative or instruction-following agents. They excel at reproducing patterns but struggle to generate novel or task-specific content without additional techniques.

- **Need for Further Enhancements**:
  - The chapter concludes by noting that while the discussed methods enable LLMs to generate somewhat realistic text, they are not yet "useful" for many practical applications. The generated text often lacks purpose or relevance to specific user needs.
  - To address this, the next segments will explore techniques for making LLMs follow instructions and perform useful tasks, moving beyond mere token prediction to more goal-oriented generation.

## Additional Insights and Context

- **Evolution of Sampling Strategies**: The progression from greedy sampling and beam search to random, top-$k$, and top-$p$ sampling reflects the field's's shift toward balancing coherence and diversity. Top-$p$ sampling, in particular, has become a cornerstone of modern LLM generation, as evidenced by its use in models like GPT-3, LLaMA, and others, due to its adaptability and effectiveness.

- **Practical Considerations**:
  - **Parameter Tuning**: In practice, the choice of sampling strategy and parameters (e.g., $k$, $p$, $T$) often depends on the application. For example, creative writing tasks may favor higher temperatures and larger $p$ values to encourage diversity, while technical writing tasks may favor lower temperatures and smaller $p$ values to ensure precision.
  - **Evaluation Metrics**: The chapter does not discuss evaluation metrics, but in practice, metrics like perplexity, BLEU, or human evaluation are used to assess generation quality. However, these metrics often fail to capture subjective aspects like creativity or relevance, underscoring the need for human judgment in evaluating LLM outputs.

- **Future Directions**: The mention of min-$p$ sampling and the need for better parameter understanding hints at ongoing research into more advanced sampling techniques. Recent developments (beyond the transcript) include methods like contrastive decoding, which explicitly penalize incoherent outputs, and reinforcement learning from human feedback (RLHF), which fine-tunes models to prioritize useful and relevant text over raw likelihood.

## Conclusion

This chapter provides a comprehensive overview of how LLMs generate text, focusing on the critical role of sampling strategies in translating probability distributions into coherent outputs. Greedy sampling offers simplicity but lacks diversity, beam search optimizes likelihood but sacrifices quality, random sampling introduces diversity but risks incoherence, top-$k$ sampling improves coherence but struggles with adaptability, and top-$p$ sampling emerges as the most effective and widely used method, balancing coherence and diversity. Temperature adjustments further refine these strategies, while special tokens like `<BOS>` and `<EOS>` control the generation process. However, the chapter underscores that current methods are limited to repeating training patterns, setting the stage for future discussions on making LLMs more useful and instruction-following.