# Summary of Lecture on Reinforcement Learning from Human Feedback (RLHF)

This lecture provides an in-depth exploration of **Reinforcement Learning from Human Feedback (RLHF)**, a critical process for transforming pre-trained large language models (LLMs) into safe, useful, and aligned chatbots. RLHF is the final step in a three-stage journey that includes pre-training, instruction tuning, and RLHF itself. The lecture outlines the purpose of RLHF, its three main steps, and the technical details involved, referencing the **InstructGPT** paper (Ouyang et al., 2022) as a foundational work. It also discusses alternative approaches like the Leave-One-Out (RLOO) method and addresses challenges such as non-differentiability and model degradation.

---

## Overview of RLHF and Its Role

RLHF shapes the outputs of LLMs based on human preferences, ensuring that the model aligns with desired qualities such as **safety**, **usefulness**, and **style** (e.g., artistic tone in poetry). It acts as the final step to refine a pre-trained LLM into a chatbot that not only provides accurate answers but also avoids harmful or undesirable responses. The process involves:

1. **Pre-training**: The LLM is initially filled with knowledge, likened to a "sponge" absorbing vast amounts of data.
2. **Instruction Tuning**: The model is fine-tuned to follow instructions and engage in dialogue, making it conversational.
3. **RLHF**: The model is further refined to align with human preferences, ensuring it avoids harmful outputs and provides responses that meet user expectations.

The lecture emphasizes that RLHF ensures the model "does no harm" by learning to identify and avoid unsafe, passive-aggressive, or unhelpful responses, while also encouraging specific styles of answering.

---

## The Three Steps of RLHF

RLHF consists of three distinct steps, each building on the previous one to create a well-aligned model. These steps are illustrated in the provided images and detailed in the transcript.

### Step 1: Supervised Fine-Tuning (Instruction Tuning)

- **Purpose**: Convert the pre-trained LLM into a model capable of following instructions and engaging in dialogue.
- **Process**:
  - Human labelers write prompts (e.g., "Tell me about apples") and provide the expected responses.
  - The model is fine-tuned on these prompt-response pairs to learn how to respond appropriately.
  - Prompts can be generated by annotators, include few-shot examples, or come from early adopters/customers of GPT models.
- **Data**:
  - In the original InstructGPT paper, only **13,000 samples** were used for this step.
- **Outcome**: The model becomes conversational but may still lack alignment with human preferences, necessitating further refinement.

### Step 2: Reward Model Learning

- **Purpose**: Train a reward model to evaluate the quality of the LLM's responses based on human feedback.
- **Process**:
  - A prompt is provided, and the instruction-tuned LLM generates multiple responses (e.g., A, B, C, D).
  - Human labelers rank these responses from best to worst (e.g., D > C > A = B, as shown in the image for the prompt "Explain the moon landing to a 6-year-old").
  - The ranked data is used to train a reward model, which learns to assign higher rewards to better responses.
- **Model Details**:
  - The reward model is a smaller **6 billion parameter (6B)** model, compared to the LLM's **175 billion parameters (175B)**.
  - The smaller size prevents overfitting due to limited data and reduces computational demands.
- **Loss Function**:
  - The reward model is trained using a **Bradley-Terry** pairwise preference model.
  - The loss function is defined as:
    $
    \ell = E_{x,y^+,y^-} \left[ \log \sigma \left( r(x,y^+) - r(x,y^-) \right) \right]
    $
    where:
    - $ r(x,y) $: Reward for prompt \( x \) and response \( y \).
    - $ y^+ $: The better response (positive).
    - $ y^- $: The worse response (negative).
    - $ \sigma $: Sigmoid function.
  - This loss ensures the reward model assigns a higher score to the preferred response.
- **Data**:
  - InstructGPT used **6,600 annotator-generated prompts** and **26,000 customer-generated prompts** to train the reward model, totaling a small dataset by modern standards.
- **Outcome**: A reward model capable of scoring responses based on human preferences, capturing qualities like safety, usefulness, and tone.

### Step 3: Reinforcement Learning to Optimize the Policy

- **Purpose**: Fine-tune the LLM to maximize the reward predicted by the reward model, ensuring the model's outputs align with human preferences.
- **Process**:
  - A new prompt is sampled from a dataset (e.g., "Write a story about frogs").
  - The LLM (policy) generates a response.
  - The reward model evaluates the response, assigning a reward \( r_k \).
  - The reward is used to update the LLM's policy using reinforcement learning (RL).
- **Algorithm**:
  - **Proximal Policy Optimization (PPO)** is used to maximize the expected reward while preventing drastic deviations from the original model.
  - The PPO objective is:
    $
    E_{y \sim P(\cdot|x)} \left[ r(y,x) \nabla \log P(y|x) \right] - \beta D_{KL} \left[ P(y|x) \parallel P_{ref}(y|x) \right]
    $
    where:
    - $ P(y|x) $: The policy (LLM) generating response $ y $ for prompt $ x $.
    - $ P_{ref}(y|x) $: The reference policy (pre-RLHF model).
    - $ \beta D_{KL} $: A KL-divergence term to penalize large deviations from the reference model, ensuring stability.
  - **Action**: Predicting the next token in the sequence.
- **Data**:
  - InstructGPT used **32,000 customer-generated prompts** for this step, with no additional human-provided data.
- **Models Required**:
  - PPO requires **four models**:
    1. **Reference Model**: The pre-RLHF model (or an earlier version).
    2. **Generator**: The LLM being optimized (175B parameters).
    3. **Critic**: A smaller model to estimate value functions (often 6B parameters).
    4. **Reward Model**: The model trained in Step 2 (6B parameters).
  - This setup makes PPO computationally expensive and complex to implement.
- **Challenges**:
  - **Non-Differentiability**: The LLM generates tokens via sampling, which is not differentiable. Feeding the sampled tokens back into the model for reward evaluation further complicates gradient computation.
  - RL is necessary to handle this non-differentiability, as traditional gradient-based methods (e.g., backpropagation) cannot be applied directly.
- **Outcome**: The LLM is fine-tuned to produce responses with high rewards, aligning its outputs with human preferences.

---

## Alternative Approach: Leave-One-Out (RLOO) Estimator

The lecture introduces a simpler alternative to PPO called **Reinforcement Learning with Leave-One-Out (RLOO)**, which treats RLHF as a **bandit problem** rather than a sequential decision-making problem.

- **Key Differences**:
  - **No Sequential Actions**: Instead of treating each token prediction as an action, RLOO considers the generation of a full response as a single action.
  - **Simplified Setup**: RLOO requires only two models:
    1. **Generator**: The LLM being fine-tuned.
    2. **Reward Model**: The model from Step 2.
  - This reduces the engineering complexity compared to PPO's four-model requirement.
- **Process**:
  - For a given prompt (e.g., "I want to refocus my diet on only eating apples and supplements"), the LLM generates multiple responses (e.g., three responses).
  - The reward model evaluates each response:
    - "This is a great idea..." (high reward).
    - "Sure, here is how you..." (medium reward).
    - "I would not recommend..." (low reward, preferred for safety).
  - The **Reinforce algorithm** is applied to compute gradients:
    - For each response, the reward is compared to the average reward of the other responses (leave-one-out baseline).
    - If a response has a higher reward than the average of the others, its gradient is followed to encourage similar outputs.
    - If a response has a lower reward, the gradient is reversed to discourage such outputs.
  - The objective is:
    $
    E_{y \sim P(\cdot|x)} \left[ (r(y,x) - b) \nabla \log P(y|x) \right]
    $
    where $ b $ is the baseline (average reward of other responses).
- **Advantages**:
  - **Lightweight**: RLOO can be implemented in a day or two using public infrastructure.
  - **Memory Efficient**: It fits within the same memory footprint needed for fine-tuning the LLM.
  - **Effective**: The "Back to the Basics" paper demonstrates that RLOO outperforms PPO and other alternatives in many cases.
- **Outcome**: The LLM learns to prioritize safe and useful responses (e.g., warning against an apple-only diet) without the complexity of PPO.

---

## Challenges and Solutions in RLHF

### Challenge 1: Non-Differentiability
- **Issue**: The LLM's token sampling process is non-differentiable, and feeding sampled tokens back into the model for reward evaluation further breaks gradient flow.
- **Solution**: RL algorithms like PPO or Reinforce are used to optimize the model despite non-differentiability. Techniques like Gumbel softmax could theoretically make sampling differentiable, but they are impractical due to the need to re-feed inputs into the model.

### Challenge 2: Model Degradation
- **Issue**: Over-optimizing with RLHF can make the model overly conservative, refusing to answer borderline questions or forgetting pre-training knowledge.
- **Solutions**:
  1. **KL-Divergence Regularization**:
     - A KL-divergence term in the PPO objective ensures the fine-tuned model does not deviate too far from the instruction-tuned model.
     - This is seen in the PPO objective: $ -\beta D_{KL} \left[ P(y|x) \parallel P_{ref}(y|x) \right] $.
  2. **Mixing Pre-Training Data**:
     - Incorporating pre-training gradients during RLHF ensures the model retains its general knowledge and does not overfit to the reward model.

---

## Example Application

The lecture provides a practical example of RLHF's impact:
- **Prompt**: "I want to refocus my diet on only eating apples and supplements."
- **Pre-RLHF Response**: The model might naively respond with "Sure, here is how you..." or "This is a great idea...", potentially encouraging an unsafe diet.
- **Post-RLHF Response**: After RLHF, the model learns to prioritize safety, responding with "I would not recommend...", explaining why an apple-only diet is a bad idea due to nutritional deficiencies.

This alignment with safety and usefulness is a direct result of the reward model prioritizing responses that caution against harmful actions, as determined by human feedback.

---

## Why PPO Was Used in InstructGPT

- **Historical Context**: OpenAI developed PPO during its earlier robotics research, giving the team extensive expertise in applying PPO to RL problems.
- **Complexity**: While PPO was effective, it is overly complex for RLHF, requiring four models and significant engineering effort.
- **Modern Alternatives**: Simpler methods like RLOO have emerged, offering comparable or better performance with less overhead.

---

## Conclusion

RLHF is a powerful technique for aligning LLMs with human preferences, ensuring they are safe, useful, and stylistically appropriate. The process involves three steps: supervised fine-tuning, reward model training, and policy optimization using RL. While the original InstructGPT paper used PPO, simpler alternatives like RLOO offer a more lightweight approach, treating RLHF as a bandit problem and requiring fewer resources. Challenges like non-differentiability and model degradation are addressed through RL algorithms and regularization techniques. Ultimately, RLHF enables LLMs to provide thoughtful, safe responses, as demonstrated by the example of discouraging an unsafe diet.

The lecture concludes by noting that the next segment will explore a variant of RLHF that is even less painful to implement, indicating ongoing advancements in this field.
