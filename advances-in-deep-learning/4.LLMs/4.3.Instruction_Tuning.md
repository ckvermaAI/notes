# Instruction Tuning

This chapter delves into the process of instruction tuning, a critical step in transforming pre-trained large language models (LLMs) from mere text predictors into interactive conversational agents or specialized tools like code completers. The discussion covers the limitations of pre-trained models, the mechanics of instruction tuning, the use of structured templates and special tokens, data creation methods, practical applications (e.g., chatbots and code completion), and the limitations of this approach, setting the stage for further alignment techniques. Below is a detailed summary incorporating insights from the transcript and the provided slides.

## Background and Motivation

- **Context from Previous Learning**: The chapter builds on prior discussions about constructing LLMs, including their architectures, training processes, and sampling strategies for generating coherent text. Sampling methods range from those that replicate training data (e.g., greedy sampling) to creative, unbiased approaches that produce text resembling real-world examples.

- **Core Limitation of Pre-trained Models**: Despite their ability to generate text, pre-trained LLMs lack the capacity to engage in meaningful dialogue or follow specific instructions. For instance, when prompted with "tell me a joke about apples," a pre-trained LLaMA 3.1 (8 billion parameter, 4-bit quantized version) model responded with an unrelated and incoherent sequence: "If you don't know one, I'll tell you. What do you call a car with no legs?" This reflects its tendency to continue text completion rather than respond to the user's intent.

- **Contrast with Conversational Models**: In contrast, familiar models (e.g., those akin to modern chatbots) can handle dialogue effectively, as demonstrated by the response: "Why did the apple join the gym? Because he wanted to get some core strength." This highlights the goal of instruction tuning: to enable pre-trained models, which contain vast knowledge, to engage in structured conversations while retaining their learned information.

- **Objective of Instruction Tuning**: The process aims to teach LLMs to follow specific templates (e.g., dialogue or code structures) and respond appropriately to user requests, moving beyond token prediction to interactive utility. The chapter emphasizes preserving the model's world knowledge while enabling conversational or task-specific behavior.

## Basics of Instruction Tuning

- **Training Approach**: Instruction tuning involves fine-tuning a pre-trained LLM on dialogue data or task-specific data (e.g., coding data) to adapt its behavior. This process uses supervised fine-tuning, where the model learns from examples of desired outputs within structured formats.

- **New Special Tokens**: The method introduces specific tokens to structure the input and output process, including:
  - `[bot]` (beginning of text, sometimes called beginning of sequence).
  - `[eot]` (end of turn), signaling the end of a speaker's contribution.
  - `[boh]` (beginning of header) and `[eoh]` (end of header), which define the role (e.g., user, assistant) or context, functioning like HTML tags.

- **Dialogue Structure**:
  - The dialogue consists of alternating roles: user (the human) and assistant (the LLM).
  - A conversation progresses with a user input (e.g., "What is the color of oranges?"), followed by an assistant response (e.g., "The answer, of course, depends on the type of orange!"), each marked by `[eot]` to indicate the turn's end.
  - The LLM only generates the assistant's response text and the `[eot]` token, while the chat application handles other tokens (e.g., `[bot]`, `[boh]User[eoh]`) to provide context.

- **Process**:
  - A wrapper (e.g., a chatbot interface) formats the input with headers and special tokens, presenting a structured prompt to the LLM.
  - The LLM generates tokens until it produces an `[eot]`, and the application displays the response, then prepares the next structured input based on user feedback.
  - This iterative process transforms the LLM from a text completer into an interactive agent.

## Supervised Fine-Tuning Mechanics

- **Fine-Tuning Method**: The pre-trained model, initially trained to predict the next token, is fine-tuned on dialogue templates. Examples include user requests and corresponding assistant replies, teaching the model to recognize and respond within the dialogue structure.

- **Loss Function**: The supervision focuses solely on the assistant's response. The loss is applied only to the tokens in the assistant's message and the `[eot]` token, ensuring the model learns to generate appropriate replies. Other tokens (e.g., user messages, headers) provide context but are not supervised, as their generation is handled by the application.

- **Role of Special Tokens**: These tokens (e.g., `[bot]`, `[eot]`, `[boh]`, `[eoh]`) help the LLM interpret the dialogue's structure and roles, but the model is not trained to generate them except for `[eot]` in its responses.

## Roles and Special Messages

- **Dialogue Roles**:
  - **User**: The human interacting with the LLM.
  - **Assistant**: The LLM itself, generating responses.
  - **System**: A special role, typically the first message, which sets behavioral instructions (e.g., "speak in a strong Texan accent" or "reply only in French"). This message influences the LLM's tone and constraints throughout the conversation.
  - **Tool Calls**: Special messages for interacting with external tools or programs, to be explored later, indicating the LLM's potential to interface with software or APIs.

- **Implementation**: The system message is placed at the conversation's start (e.g., within `[boh]System[eoh]`), ensuring consistent behavior. The chat application manages these roles, feeding structured inputs to the LLM.

## Practical Applications

- **Chatbot Development**: Once trained, the LLM can be wrapped in a chatbot (e.g., LLaMA), which formats conversations, manages context, generates headers, and limits output to the assistant's response until `[eot]`. This enables interactive dialogue, as seen in the oranges color example from the slides.

- **Code Completion (e.g., GitHub Copilot)**:
  - **Training Data**: Copilot is pre-trained on vast code corpora and fine-tuned using a template with `[pre]`, `[post]`, and `[mid]` tokens.
    - `[pre]`: Code before the completion point.
    - `[post]`: Code after the completion point.
    - `[mid]`: The code to be completed (e.g., the loop body in a bubble sort function).
  - **Example**: For a bubble sort function, `[pre]` includes the function definition and comment, `[post]` includes the `if __name__ == "__main__":` block, and `[mid]` is the sorting logic (e.g., `for i, _ in enumerate(a): for j, _ in enumerate(a[:i]): if a[i] < a[j]: a[i], a[j] = a[j], a[i]`).
  - **Enhancements**: Modern Copilot versions incorporate additional context (e.g., other files, clipboard data), improving prediction accuracy by enriching the input context.

- **Versatility**: Instruction tuning extends beyond chatbots to tasks like poem generation (e.g., in the style of 19th-century British poets) or code completion, leveraging the LLM's world knowledge and structural understanding to create novel outputs.

## Data Creation for Instruction Tuning

- **Initial Data**: Early datasets were created by humans writing dialogues between personas, providing a foundation for training.

- **Automated Data Generation**: Larger, pre-tuned LLMs now generate and supervise dialogue data, accelerating the training of new models for chatbots.

- **User-Generated Data**: Interactions with early GPT models (e.g., OpenAI's GPT) were recorded and used as training data, highlighting the role of real-world usage in refining models.

## Limitations and Challenges

- **Lack of Negative Guidance**: Instruction tuning teaches the LLM what to do (e.g., respond to prompts) but not what to avoid. A vanilla instruction-tuned model might provide harmful or nonsensical advice if prompted inappropriately.

- **Example of Misguidance**: Asking LLaMA 3.1, "I want to refocus my diet on only eating apples and supplements," could elicit a detailed but dangerous response (e.g., suggesting specific apples and supplements), as the model is trained to reply to instructions without evaluating their safety.

- **Access Restrictions**: Companies rarely release vanilla instruction-tuned models due to their potential to give bad advice, emphasizing the need for further refinement.

- **Partial Solution**: Instruction tuning is only halfway toward creating a useful, safe model. The next step, alignment (e.g., RLHF, DPO, or preference tuning), addresses refusals and better decision-making, ensuring the model acts as a "friendly assistant" rather than a blind responder.

## Additional Insights and Context

- **Model Knowledge Retention**: The chapter underscores that pre-trained models like LLaMA 3.1 (8B, Q4_0) contain extensive knowledge from internet-scale corpora, which instruction tuning preserves while adding interactivity. This contrasts with earlier models that discarded knowledge during fine-tuning.

- **Practical Considerations**: The quantization (Q4_0) reduces model size and computational demands, making it feasible for deployment in chatbots or tools like Copilot, though it may slightly affect performance.

- **Evolution of Techniques**: Beyond the transcript, instruction tuning has evolved with methods like few-shot learning and parameter-efficient fine-tuning (e.g., LoRA), which adapt models with minimal data or parameters. Alignment techniques (e.g., RLHF) have become standard in models like ChatGPT, addressing the limitations noted here.

- **Evaluation**: While not detailed, success in instruction tuning is often measured by task completion rates, coherence, and user satisfaction, though safety and ethical responses remain critical metrics post-alignment.

## Conclusion

Instruction tuning transforms pre-trained LLMs into interactive agents by fine-tuning them on structured dialogue or task-specific data using supervised learning. It introduces special tokens (`[bot]`, `[eot]`, `[boh]`, `[eoh]`, `[pre]`, `[post]`, `[mid]`) to define roles and contexts, enabling applications like chatbots (e.g., LLaMA) and code completers (e.g., Copilot). Data from humans, pre-tuned models, and user interactions fuel this process, but limitations—such as the inability to guide against harmful responses—necessitate further alignment steps (e.g., RLHF, DPO). This chapter marks a pivotal transition from knowledge storage to practical utility, with the next segment exploring alignment to enhance safety and effectiveness.