# Paged Attention

This lecture introduces **Paged Attention**, a key technique to accelerate inference in large language models (LLMs) by caching keys and values (KV cache) during autoregressive generation. It contrasts the computational challenges of inference with training, highlights the efficiency gains from caching, and addresses the resulting memory overhead. The lecture explores optimizations like grouped-query attention, pruning, and connections to state-space models to manage the KV cache, presenting these as open research areas as of March 17, 2025. The accompanying images provide a comparative table of computational complexities and illustrate different attention mechanisms.

---

## Introduction to Paged Attention

- **Purpose**: Paged Attention is presented as a favorite and transformative trick that enables fast inference for LLMs, particularly for large models (e.g., 7–8 billion parameters), making them practical for real-time use.
- **Context**: The lecture builds on prior discussions of training and generation, emphasizing the need to address inference inefficiencies.

---

## Recap of Training vs. Generation Challenges

- **Training**:
  - Processes entire sequences layer by layer in a single forward pass, with memory and runtime optimized by techniques like activation checkpointing and sequence parallelism.
  - Peak memory: $ O(NL) $ (vanilla) or $ O(NL^{1/2}) $ (with checkpointing).
  - Runtime: $ O(N^2L) $ (vanilla) or $ O(2N^2L) $ (with checkpointing).
  - Forward calls: 1 (vanilla) or 2 (with checkpointing).
- **Generation (Inference)**:
  - Operates autoregressively, generating one token at a time by feeding the previous output back into the model.
  - Peak memory: $ O(N) $, as only the current sequence is held.
  - Runtime: $ O(N^3L) $, cubic due to $ N $ forward passes, each with $ O(N^2L) $ attention computation.
  - Forward calls: $ N $, one per token.
  - **Problem**: The cubic runtime ($ O(N^3L) $) is impractical for long sequences, unlike the quadratic runtime of training ($ O(N^2L) $).

- **Key Difference**: Inference’s token-by-token dependency requires recomputing attention over the entire sequence for each new token, unlike training’s parallel processing.

---

## Mechanism of Paged Attention

- **Core Insight**:
  - In causal attention (common in LLMs), each token’s query attends only to previous keys and values, which remain constant after generation.
  - Unlike queries, keys and values from prior tokens do not depend on future outputs, enabling caching.
- **Implementation**:
  - **Caching Keys and Values (KV Cache)**: Store $ K $ and $ V $ from previous attention computations for reuse.
  - **Origin**: Inspired by paging in computer memory management, where data is cached for quick access.
  - **Modern Terminology**: Often referred to as KV caching rather than "Paged Attention."
  - **Effect on Computation**:
    - Without caching: Each token requires $ O(N^2L) $ attention computation.
    - With caching: Attention becomes $ O(NL) $ per token, as $ K $ and $ V $ are precomputed and reused, reducing the overall generation runtime to $ O(N^2L) $.
- **Memory Cost**:
  - Peak memory increases to $ O(NL) $, matching vanilla training, as $ K $ and $ V $ are cached for each layer.
  - This contrasts with generation’s original $ O(N) $ memory, creating a new bottleneck.

- **Impact**:
  - Aligns inference runtime ($ O(N^2L) $) with training, eliminating the cubic inefficiency.
  - Introduces a memory imbalance where inference may require more memory than training (e.g., $ O(NL) $ vs. $ O(NL^{1/2}) $ with checkpointing), reversing traditional model behavior.

---

## Comparative Analysis (from Image 1)

The image provides a table comparing computational complexities across training, generation, and Paged Attention:

| **Metric**      | **Training** | **Training - Checkpointing** | **Generation** | **Paged Attention** |
|-----------------|--------------|------------------------------|----------------|---------------------|
| **Peak Memory** | $ O(NL) $  | $ O(NL^{1/2}) $            | $ O(N) $     | $ O(NL) $         |
| **Runtime**     | $ O(N^2L) $ | $ O(2N^2L) $               | $ O(N^3L) $  | $ O(N^2L) $       |
| **# Forward Calls** | 1        | 1                            | $ N $        | $ N $             |

- **Notes**:
  - Training with checkpointing uses 2 forward calls in prior lectures, but here it’s listed as 1, possibly indicating an optimized setup or error in the transcript/image alignment.
  - Paged Attention retains $ N $ forward calls (one per token) but reduces runtime from cubic to quadratic by caching.

---

## Optimizations for KV Cache Management

The lecture identifies the memory overhead of KV caching as an open problem and proposes several strategies:

1. **Grouped-Query Attention** (from Image 2):
   - **Variants**:
     - **Multi-Head Attention**: Each head has unique $ K $, $ V $, and $ Q $ (e.g., 8 heads with 8 sets of $ K $, $ V $).
     - **Grouped-Query Attention**: Groups queries share $ K $ and $ V $ (e.g., 4 groups with shared $ K $, $ V $ per group).
     - **Multi-Query Attention**: All queries share a single $ K $ and $ V $.
   - **Effect**: Reduces KV cache size by a factor equal to the group size (e.g., 4x reduction with 4 groups), as fewer $ K $ and $ V $ pairs are stored.
   - **Trade-Off**: May slightly degrade attention quality due to shared representations.

2. **Pruning**:
   - **Concept**: Evicts less relevant $ K $ and $ V $ from the cache based on predicted future attention needs.
   - **Insight**: Not all cached elements are attended to, allowing selective removal.
   - **Risk**: Loss of accuracy if critical $ K $ or $ V $ are pruned, requiring a trade-off between memory savings and performance.
   - **Research**: Empirical studies (e.g., papers on KV cache pruning) explore heuristics to identify unimportant entries.

3. **State-Space Models (SSMs)**:
   - **Connection**: SSMs maintain a fixed-size hidden state updated token-by-token, analogous to a constant-size KV cache.
   - **Approach**: Learns to update the state during training, potentially replacing the growing KV cache with a recurrent mechanism.
   - **Benefit**: Caps memory usage, making inference more efficient for long sequences.
   - **Research Potential**: Links to lower-rank representations and SSMs (e.g., Mamba) suggest innovative cache management strategies.

---

## Open Problems and Research Opportunities

- **Memory Inefficiency**: Vanilla KV caching stores all $ K $ and $ V $ per layer, leading to $ O(NL) $ memory, which can exceed training memory requirements (e.g., $ O(NL^{1/2}) $ with checkpointing).
- **Research Directions**:
  - Developing more efficient caching (e.g., adaptive pruning, compression).
  - Exploring connections to SSMs and lower-rank approximations to maintain cache size.
- **Encouragement**: The lecturer highlights this as an exciting area, urging students to contribute ideas, with references to ongoing papers (e.g., Pyramid-Q) for further exploration.

---

## Conclusion

- **Summary**: Paged Attention (KV caching) transforms inference from a cubic ($ O(N^3L) $) to a quadratic ($ O(N^2L) $) runtime, matching training efficiency. However, it increases memory to $ O(NL) $, creating a new challenge. Optimizations like grouped-query attention, pruning, and SSM-inspired approaches aim to mitigate this, but remain active research topics.
- **Context (March 17, 2025)**: The technique’s impact is evident in modern LLMs, enabling rapid responses from large models, while memory management continues to evolve.

---

### Additional Context
- **Historical Note**: KV caching traces back to early transformer optimizations (e.g., Vaswani et al., 2017), with "Paged Attention" likely referencing early caching papers, later generalized as KV caching.
- **Applications**: Essential for real-time applications (e.g., chatbots, translation), where low latency is critical.
- **Trends**: Growing sequence lengths (e.g., 30,000 tokens) amplify the need for efficient caching, driving research into scalable solutions.

This summary captures all transcript points, integrates image details, and adds relevant context on attention mechanisms and research trends.