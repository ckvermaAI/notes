# Open Source LLMs Infrastructure

This lecture explores the infrastructure for interacting with and training large language models (LLMs), focusing on open-source and open models. It categorizes models into closed, open, and open-source groups, detailing their accessibility and reproducibility. The lecture then discusses three levels of interaction with open models—online APIs, local inference, and inference servers—followed by an overview of training frameworks, including Hugging Face, PyTorch (TorchTune/TorchTitan), and DeepSpeed ecosystems. It emphasizes the ease of use, scalability, and privacy considerations of these tools, encouraging experimentation with available GPUs.

---

## Introduction to Open Source LLMs Infrastructure

- **Context**: Follows discussions on complex LLM training techniques (e.g., sequence parallelism, KV caching, speculative decoding), which are challenging to implement manually.
- **Purpose**: Highlights that users can leverage open-source libraries and frameworks to avoid implementing these optimizations, focusing instead on utilizing pre-built infrastructure for inference and training.
- **Components of LLMs**:
  - **Architecture**: Typically transformer-based, often described in research papers, though details for the largest models may be insufficient for reproduction.
  - **Weights**: Include both model weights and tokenizer weights (for converting text to token IDs).
  - **Training Code and Data**: Necessary for retraining, but increasingly unavailable due to complexity, proprietary data, or legal concerns (e.g., scraped internet data exposing liabilities).

---

## Categorization of LLMs (from Image)

- **Closed Models** (Only outputs available):
  - Examples: GPT-3.5/4 (OpenAI), Claude 1/2/3 (Anthropic), Gemini/Bard (Google), Mistral Medium/Large.
  - Characteristics:
    - Architecture is roughly known but not reproducible.
    - No access to weights; interaction only via APIs or online servers.
    - Business model: Revenue generated through API requests.
    - Unlikely to be open-sourced unless the company faces bankruptcy.
- **Open Models** (Model weights available):
  - Examples: LLaMA 1/2/3 (Meta), Mistral 7B/Mixtral (Mistral AI), Qwen, Gemma (Google), Grok (xAI), Falcon, and many more.
  - Characteristics:
    - Architecture is well-documented (e.g., Python code in PyTorch).
    - Weights (for both model and tokenizer) are publicly downloadable.
    - Largest category, widely used for local inference.
- **Open-Source Models** (Training code, datasets, weights released):
  - Examples: Vicuna, OpenChat, OLMo, OpenFlamingo, BLIP/InstructBLIP, LLaVA, and many more.
  - Characteristics:
    - Fully reproducible with access to training code and data.
    - Rare due to high training costs (millions of dollars) and proprietary infrastructure.
    - Often fine-tuned from open models (e.g., Vicuna and LLaVA fine-tuned from LLaMA) or use data generated by closed/open models.
  - Note: Companies may mislabel open models as open-source; true open-source requires retrainability by independent parties.

- **Trends**:
  - Fewer models share training code/data due to complexity, proprietary data, and legal risks (e.g., earlier models used scraped data indiscriminately).
  - Modern models (as of March 17, 2025) are designed to avoid legal risks, with companies more cautious about data usage.

---

## Interacting with Open Models: Three Levels

### Level 0: Online APIs
- **Platforms**:
  - **Hugging Face (Hugging Chat)**: Hosts most open and open-source models, allows chatting with models like LLaMA 3.1 70B.
  - **LM Arena (formerly LMSYS)**: Compares responses from multiple models, logging user preferences for research.
- **Features**:
  - No hardware required; runs on cloud servers.
  - Easy for quick testing (e.g., asking LLaMA 3.1 70B to compute: 12,340 + 54,321 ).
  - LM Arena allows side-by-side model comparison, with modes for blind or identified comparisons.
- **Limitations**:
  - Not private; interactions are logged (e.g., Hugging Chat logs data for research).
  - Rate limits apply due to hosting costs.
  - Not all models are available.
  - Programmatic interaction via UI is challenging.
- **Use Case**: Best for initial exploration and understanding model capabilities without local hardware.

### Level 1: Local Inference
- **Tools**:
  - **Ollama**: A wrapper around LLaMA.cpp, provides a simple Python API for chatting with models.
  - **LLaMA.cpp**: Directly accesses model weights, offering more control without a server intermediary.
- **Setup**:
  - Runs on consumer hardware (e.g., M-series MacBooks, Windows laptops with GPUs, gaming PCs).
  - Optimized libraries for specific hardware (e.g., Mac acceleration libraries).
  - Example: Using Ollama to chat with a smaller LLaMA model (e.g., LLaMA 8B), which may struggle with math (e.g.,  12,340 + 54,321 ).
- **Python API**:
  - **Ollama**: Uses a dictionary-based chat template (e.g., `{"role": "user", "content": "What is 12,340 + 54,321?"}`), extends conversation by appending messages.
  - **LLaMA.cpp**: Provides direct model access, bypassing server interaction for greater control.
- **Advantages**:
  - Quick for testing; widely used, ensuring bugs (e.g., corrupted weights, tokenizer issues) are caught and fixed by the community.
  - Can be private if telemetry is disabled, allowing use of personal or proprietary data.
  - Fast on modern hardware (e.g., runs efficiently on a 9-year-old GPU or newer Macs).
- **Disadvantages**:
  - Limited to smaller models (e.g., 8B) due to hardware constraints.
  - Not scalable for multiple users; risks memory issues if handling multiple conversations (e.g., loading LLaMA twice).
- **Use Case**: Ideal for single-user, local experimentation with privacy needs.

### Level 2: Inference Servers
- **Tools**:
  - **vLLM**: Preferred for its speed and robustness, supports KV caching (Paged Attention), under active development.
  - **Hugging Face TGI (Text Generation Interface)**: Another inference engine, often used for hosting trained models.
  - **FastChat**: A third option, less highlighted in the lecture.
- **Setup**:
  - **vLLM Installation**: `pip install vllm`, downloads models from Hugging Face, and hosts them locally (e.g., `tensor_parallel_size=8` to split across 8 GPUs).
  - **Interaction**: Sends HTTP requests to endpoints like `/v1/chat/completions` (OpenAI-compatible format), using JSON to specify model, messages, and parameters.
  - **APIs**: Supports OpenAI’s library for Python requests, or `curl` for quick testing, running locally without OpenAI’s costs.
- **Features**:
  - Handles multiple users by batching requests, caching state between requests for acceleration.
  - 10x faster than local inference (e.g., Ollama, LLaMA.cpp) on the same hardware due to request bundling and efficient resource use.
  - Scales to multiple GPUs or nodes, suitable for high-throughput scenarios.
  - Private, as data stays within the user’s ecosystem.
- **Requirements**: Requires GPUs (e.g., single GPU for small models, 8 GPUs for larger ones like 70B).
- **Use Case**: Best for serving many users or processing large datasets, such as deploying a model for a service.

---

## Training Frameworks

### Hugging Face Ecosystem
- **Components**:
  - **Hugging Face Hub**: Central repository for models, datasets, and related resources.
  - **Transformers Library**: Supports a wide range of architectures, efficient components, and training code.
  - **Accelerate**: Enables multi-GPU training by parallelizing computations.
  - **TGI (Text Generation Interface)**: Inference engine for hosting trained models, not for training.
- **Features**:
  - Largest ecosystem with the most users, thorough documentation, and extensive tutorials.
  - Supports multiple backends (PyTorch, TensorFlow via Keras), exports to various formats, and covers most models.
- **Use Case**: Ideal for broad applicability and ease of use, especially for beginners.

### PyTorch Ecosystem (TorchTune/TorchTitan)
- **Components**:
  - **Hugging Face Hub**: Reused for model/dataset hosting.
  - **TorchTune**: Lean equivalent to Transformers, focused on PyTorch-only support with minimal code (e.g., 100 lines for a transformer).
  - **TorchTitan**: Large-scale parallel training framework (used internally by Meta for LLaMA), primarily providing FSDP2 (a more flexible Fully Sharded Data Parallel implementation for hybrid sharding across GPUs/nodes).
- **Features**:
  - Leaner and easier to understand than Hugging Face, with minimalistic code for architectures, components, and training.
  - PyTorch-only, supporting fewer models but offering clarity (e.g., 2–3 files for a transformer implementation).
  - FSDP2 enhances efficiency for multi-GPU/node training.
- **Use Case**: Preferred for users seeking lightweight, transparent frameworks, though it supports fewer models and has a smaller community.

### DeepSpeed Ecosystem
- **Integration**: Plugs into Hugging Face’s Transformers and Accelerate libraries.
- **Key Feature**: Provides DeepSpeed Zero-3 optimizer, an alternative to FSDP for efficient multi-GPU training.
- **Use Case**: Suitable for users within the Hugging Face ecosystem who prefer DeepSpeed’s optimization over FSDP.

- **Lecturer’s Preference**: TorchTune/TorchTitan for its minimalism, ease of understanding, and quick start, despite supporting fewer models and having a smaller community.

---

## Practical Considerations

- **Hardware Requirements**:
  - Inference: Local inference runs on consumer hardware (e.g., M-series MacBooks, gaming PCs); inference servers need GPUs (e.g., 8 GPUs for 70B models).
  - Training: Requires GPUs for efficiency; 8B models need at least 1 GPU, while 70B models need 8 GPUs to fit in memory. Training on CPUs or M-series laptops is impractical for larger models.
- **Community and Support**:
  - All frameworks have large communities, extensive documentation, and examples.
  - Bugs are quickly identified and fixed due to widespread use.
- **Encouragement**: The lecturer recommends experimenting with these frameworks, especially if GPUs are available, as fine-tuning models is both educational and enjoyable.

---

## Conclusion

- **Summary**: The lecture categorizes LLMs into closed, open, and open-source models, focusing on open and open-source for practical use. Interaction occurs at three levels: online APIs (Hugging Chat, LM Arena), local inference (Ollama, LLaMA.cpp), and inference servers (vLLM, TGI, FastChat). Training frameworks include Hugging Face (Transformers, Accelerate), PyTorch (TorchTune, TorchTitan), and DeepSpeed, each with distinct strengths. These tools abstract complex optimizations (e.g., sequence parallelism, KV caching), enabling users to focus on application.
- **Context (March 17, 2025)**: Open models dominate practical use, with frameworks like vLLM and TorchTune leading in inference and training efficiency, respectively.

---

### Additional Context
- **Hugging Face’s Dominance**: As of 2025, Hugging Face remains the central hub for LLMs, hosting over 500,000 models and datasets, making it indispensable for research and deployment.
- **TorchTune/TorchTitan Growth**: Emerging as a lean alternative, TorchTune supports popular models like LLaMA and Mistral 7B, gaining traction for its simplicity.
- **Legal Trends**: Increased scrutiny on training data (e.g., lawsuits against OpenAI for web scraping) has pushed companies to prioritize legal compliance, affecting open-source model availability.

This summary captures all transcript points, integrates the image’s model categorization, and adds context on trends and framework usage.