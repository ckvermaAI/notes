# LLM Architectures


## 1. Overview of LLM Architectures

LLM architectures are composed of two primary components:
- **Tokenizer**: Converts text into a format understandable by the deep learning model (a stream of tokens) and converts the model's output back into text.
- **Deep Network**: Processes the tokenized input to reason and generate outputs, typically text.

These components work together in a sandwich-like structure, where the tokenizer prepares text for the deep network, and the network's outputs are converted back into text via the tokenizer.

---

## 2. Tokenization

### 2.1 Purpose of Tokenization
The tokenizer's primary role is to:
- Convert input text into a stream of tokens, which are numerical representations of words, subwords, or characters.
- Convert the deep network's outputs (continuous vectors or probability distributions) back into human-readable text.

Tokenization is essential because deep networks cannot directly process raw text. Instead, they require numerical inputs, typically continuous vectors, for processing.

### 2.2 Understanding Text
Text is fundamentally a sequence of characters, where each character is represented by a numerical value:
- In English, characters are encoded within a range of 0 to 255 (ASCII standard).
- For languages with larger character sets (e.g., Chinese), UTF-8 encoding is used, where a single symbol may be represented by up to four characters. Special characters indicate the start and continuation of multi-character symbols (e.g., emojis).

### 2.3 Challenges in Tokenization
- **Raw Character Conversion**: Directly converting characters into integers (e.g., ASCII values) and feeding them into a deep network is inefficient. This is because characters lack a meaningful linguistic order (e.g., the numerical difference between 'A' and 'B' does not imply a linguistic relationship).
- **Network Outputs**: Deep networks output continuous vectors or probability distributions over discrete values, not text. The tokenizer must map these outputs back to text, ensuring valid sequences (especially for UTF-8 encoded symbols).

### 2.4 Tokenization Strategies
Several strategies exist for tokenization, with varying levels of efficiency:

#### 2.4.1 Character-Level Tokenization
- **Process**: Each character is mapped to an integer (e.g., 0 to 255 for ASCII) and fed into the network.
- **Issues**:
  - Characters lack linguistic meaning when treated as integers, leading to poor network performance.
  - Long sequences result from character-level tokenization, making it computationally inefficient for LLMs, which struggle with very long sequences.
- **Improvement**: Instead of feeding raw integers, characters can be converted into one-hot encodings (binary vectors indicating the presence of a character). These encodings are then processed by an embedding layer, which maps each character to a learned continuous vector.
- **Embedding Layer**: The first layer of the deep network is often a linear layer (matrix multiplication). For a one-hot vector $v$ (with a 1 at the character's index and 0s elsewhere), multiplying it by a weight matrix $E$ yields a single column of $E$, i.e., $E \cdot v$. This column is the embedding of the character, denoted as $e$. This process is implemented efficiently in frameworks like PyTorch using `torch.nn.Embedding`, avoiding the need to explicitly create one-hot vectors.

#### 2.4.2 Output Tokenization
- **Process**: The network outputs a probability distribution over possible characters (e.g., 0 to 255). The tokenizer selects a character based on this distribution.
- **Challenges**:
  - For English (ASCII), this is straightforward, as all sequences are valid.
  - For UTF-8, invalid sequences can be produced (e.g., incomplete multi-character symbols), leading to errors during detokenization.
- **Symmetry**: Character-level tokenization is symmetric, with an embedding layer mapping characters to vectors and a classifier mapping vectors back to a distribution over characters. In some models, the embedding and classifier weights are tied (shared) to reduce memory usage.

#### 2.4.3 Byte Pair Encoding (BPE)
- **Overview**: BPE is the dominant tokenization strategy in modern LLMs, originally developed as a text compression method. It builds a vocabulary by merging frequent character pairs into tokens.
- **Process**:
  1. Start with a character-level tokenization of the entire dataset, where each character is a token.
  2. Specify a desired vocabulary size $n$ (a hyperparameter, e.g., 40,000 to 250,000).
  3. Count the most frequent pair of adjacent tokens in the dataset.
  4. Merge this pair into a single new token, adding it to the vocabulary.
  5. Replace all occurrences of the pair in the dataset with the new token.
  6. Repeat steps 3â€“5 until the vocabulary reaches size $n$.
- **Preprocessing**: Before BPE, text is split into words, numbers, and special strings, with boundaries (e.g., whitespace) preserved. Whitespace is typically prepended to words for better handling.
- **Example**:
  - Input string: "the cat in the hat".
  - Initial tokens: ['t', 'h', 'e', ' ', 'c', 'a', 't', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'h', 'a', 't'].
  - Merge frequent pair 't' and 'h' into 'th'.
  - Updated tokens: ['th', 'e', ' ', 'c', 'a', 't', ' ', 'i', 'n', ' ', 'th', 'e', ' ', 'h', 'a', 't'].
  - Continue merging (e.g., 'th' and 'e' into 'the', ' ' and 'c' into ' c', etc.).
- **Tokenization Order**: During tokenization, merges are applied in the order they were learned (earliest merges first), ensuring consistency between training and inference.
- **Detokenization**: Each token corresponds to a fixed set of characters, making detokenization straightforward by replacing tokens with their character sequences.

#### 2.4.4 Advantages of BPE
- **Efficiency**: BPE reduces sequence length compared to character-level tokenization, making it more computationally feasible for LLMs.
- **Vocabulary**: BPE creates a large vocabulary, mapping parts of words or entire words to tokens, improving the model's ability to handle diverse text.

#### 2.4.5 Issues with BPE
- **Non-Unique Mappings**: There is no one-to-one mapping between strings and tokens, as multiple tokenizations are possible for the same string. This can confuse the model during generation, as it may encounter tokenizations not seen during training.
- **Counting and Manipulation**: LLMs struggle with tasks requiring character-by-character analysis (e.g., counting letters in a word) due to BPE's tokenization, which groups characters into tokens.
- **Math Challenges**: Numbers are tokenized from front to back (e.g., "12345" might be split into "123" and "45"), making arithmetic difficult, as the model cannot easily align digits or understand place values.
- **Exploits**: Rare tokens (e.g., a Reddit username tokenized as a single token) can confuse the model if they appear infrequently in the training data, causing erratic behavior.

#### 2.4.6 Other Tokenization Methods
- **WordPiece**: An older tokenization method, primarily used by Google, but largely superseded by BPE in modern LLMs.
- **Special Tokens**: Tokenizers often include special tokens to indicate the start/end of a sentence, separators, or classification tasks, enhancing model performance.

### 2.5 Tokenizer Demonstration
Using the TikTokenizer app, the chapter illustrates tokenization with the Llama 3 model:
- **English Text**: "Hello Llama 3.1" is tokenized into separate tokens for "Hello", " Llama", "3", ".", and "1", reflecting frequent patterns in the training data.
- **Special Characters**: Frequent UTF-8 characters are grouped into single tokens, while rare ones are split.
- **Chinese Text**: Some Chinese characters are encoded as single tokens, while others are split into multiple tokens, indicating limited training data for Chinese.
- **Repetitive Characters**: Sequences like "AAAA" are tokenized as a single token if frequent in the training data, but "AAAAA" might be split (e.g., "AAAA" + "A").
- **Spaces**: Multiple spaces are tokenized differently based on frequency (e.g., seven spaces might be a single token due to Python indentation patterns).

---

## 3. Deep Network Architectures

The deep network is the core of an LLM, processing tokenized inputs to generate outputs. Four main architectures are discussed, with a focus on Transformer-based models and emerging sequence models.

### 3.1 General Structure
- **Input**: The network takes tokenized inputs, converted into embeddings (continuous vectors) by the tokenizer.
- **Output**: The network produces features (continuous vectors) or probability distributions over tokens, which are then detokenized into text.
- **Symmetry**: Most architectures process $N$ input tokens to produce $N$ output features, except for encoder-only models, which may produce fewer outputs.

### 3.2 Transformer-Based Architectures

#### 3.2.1 Encoder-Decoder Architecture (Original Transformer)
- **Overview**: Introduced in the "Attention is All You Need" paper, this is the original Transformer architecture, split into two parts: encoder and decoder.
- **Encoder**:
  - Takes an input sequence (e.g., a prompt or question) and processes it using bidirectional self-attention, allowing each token to attend to all other tokens in the sequence.
  - Outputs are not directly supervised during pre-training; instead, they are fused into the decoder via cross-attention.
  - Used for encoding context or instructions.
- **Decoder**:
  - Generates text autoregressively, predicting one token at a time.
  - Uses causal (masked) self-attention, where each token can only attend to itself and previous tokens, preventing "looking into the future."
  - Incorporates cross-attention to attend to the encoder's outputs, integrating context into generation.
- **Components**:
  - **Self-Attention**: Computes attention scores for each token, allowing it to weigh the importance of other tokens. For the encoder, this is bidirectional; for the decoder, it is causal.
  - **Cross-Attention**: In the decoder, attends to the encoder's outputs, integrating context.
  - **Feed-Forward Network (FFN)/Multi-Layer Perceptron (MLP)**: Processes each token independently, adding non-linearity and capacity.
  - **Residual Connections and Normalization**: Stabilize training by adding the input to the output of each block (residual) and normalizing the result.
  - **Positional Embeddings**: Added to input embeddings to indicate token positions, as Transformers lack inherent positional awareness.
- **Training**:
  - Objective: Next token prediction, where the model predicts the next token in a sequence given all previous tokens.
  - Teacher Forcing: During training, the ground truth sequence is used as input, shifted by one token to supervise the output. This allows parallel computation of losses for all positions in the sequence, leveraging the causal attention mask.
  - Example: For "the cat in the hat came back with a splat," the input might be "the cat in the hat" and the target "cat in the hat came," training the model to predict each subsequent token.
- **Usage**: Encoder-decoder architectures are less common today, largely replaced by decoder-only models, but are still used in tasks requiring strong context encoding (e.g., machine translation).

#### 3.2.2 Encoder-Only Architecture
- **Overview**: Focuses solely on the encoder part of the original Transformer, producing fixed-size feature vectors (embeddings) rather than generating text.
- **Example**: BERT (Bidirectional Encoder Representations from Transformers).
- **Structure**:
  - Uses bidirectional self-attention, allowing each token to attend to all others in the sequence.
  - Outputs a sequence of feature vectors, often used for embedding or classification tasks.
- **Training Objectives**:
  - **Masked Language Modeling (MLM)**: Randomly masks some input tokens, and the model predicts the masked tokens based on the unmasked context. This leverages bidirectional attention, unlike next token prediction.
  - **Next Sentence Prediction (NSP)**: Takes two sentences separated by a special separator token [SEP], with a classifier token [CLS] at the start. The model predicts whether the sentences are consecutive or unrelated, enhancing sentence-level understanding.
- **Applications**:
  - Produces high-quality embeddings for tasks like text classification, similarity comparison, or text-image alignment.
  - Not suitable for text generation, though it can be fine-tuned for such tasks with limited success.
- **Limitations**:
  - Does not scale well with additional data, as it cannot efficiently memorize large datasets.
  - Losing popularity compared to decoder-only models, which offer better generative capabilities and decent embeddings.

#### 3.2.3 Decoder-Only Architecture
- **Overview**: The most popular architecture today, focusing solely on the decoder part of the Transformer, optimized for text generation.
- **Example**: GPT (Generative Pre-trained Transformer) series, Llama, Mistral.
- **Structure**:
  - Uses causal self-attention, where each token attends only to itself and previous tokens, enabling autoregressive generation.
  - Similar to the decoder in the encoder-decoder model but without cross-attention to an encoder.
  - Includes special tokens:
    - **Start Token**: Marks the beginning of a sequence, storing fixed information independent of the input.
    - **Delimiter Token**: Separates parts of the input (e.g., question and answer).
    - **Extract Token**: Added at the end for classification tasks, allowing attention to the entire sequence (unlike BERT, where [CLS] is at the start).
- **Training**:
  - Objective: Next token prediction, using teacher forcing. The ground truth sequence is input, shifted by one token, and the model predicts each subsequent token in parallel.
  - Data: Trained on massive text corpora, often encompassing the entire internet (filtered and cleaned) and licensed datasets, totaling billions of tokens.
  - Efficiency: Teacher forcing enables parallel computation, amortizing costs across the sequence, a significant improvement over pre-Transformer sequence models.
- **Scaling**:
  - Models like GPT-1 (2018) had 100 million parameters, 12 layers, and a hidden size of 768.
  - GPT-3 scaled to over 100 billion parameters, with larger hidden sizes, more layers, and more attention heads.
  - Modern models (e.g., Llama, Mistral) continue this trend, adding innovations like:
    - **Sliding Window Attention** (Mistral): Limits attention to a fixed window of past tokens, improving efficiency while hierarchically capturing longer contexts.
    - **Mixture of Experts (MoE)** (Mistral, rumored in GPT-4): Uses a routing function to select specialized sub-networks for each layer, enhancing efficiency and performance.
    - **Grouped Query Attention (GQA)** (Llama): Makes attention more efficient, especially during inference.
- **Capabilities**:
  - **Data Compression**: Decoder-only models are highly efficient at memorizing data, compressing information at approximately 2 bits per parameter (e.g., a 4-bit quantized model stores significant knowledge).
  - **Generation**: Excellent generative models, producing coherent text from minimal prompts (e.g., starting with "A" and generating medical or legal text).
  - **Embeddings**: Can produce decent embeddings, reducing the need for encoder-only models.
  - **Encoder-Decoder Mimicry**: Can mimic encoder-decoder behavior by not applying a loss to the "encoded" part of the input, though it uses causal attention throughout (unlike bidirectional attention in true encoders).
- **Information Storage**:
  - **Weights**: During pre-training, knowledge is stored in the model's weights, enabling it to regurgitate training data.
  - **Context/Prompt**: During fine-tuning, the model learns to use information in its context, enabling instruction-following and task-specific behavior.
- **Significance**:
  - Introduced by OpenAI in 2018, decoder-only models (starting with GPT) revolutionized LLMs, initially perceived as a response to encoder-only models like BERT but proving far more powerful.
  - Today, "LLM" typically refers to decoder-only models due to their dominance in generative tasks.

### 3.3 Sequence Models
- **Overview**: An alternative to Transformers, sequence models replace causal self-attention with recurrent neural networks (RNNs), processing tokens sequentially.
- **Historical Context**:
  - Pre-Transformer sequence models (e.g., RNNs, LSTMs) were popular but were largely replaced by Transformers due to inefficiency in training and handling long sequences.
  - Modern sequence models (e.g., state space models like Mamba) aim to compete with Transformers under specific conditions.
- **Structure**:
  - Inputs and outputs are identical to decoder-only models (embeddings in, distributions over tokens out).
  - Replaces attention with a recurrent update of a hidden state $h_t$ at each time step $t$:
    - Input embedding: $u_t$.
    - Hidden state update: $h_t = A \cdot h_{t-1} + B \cdot u_t$.
    - Output: $y_t = C \cdot h_t$.
  - Multiple layers stack these updates, similar to Transformer layers.
- **Training**:
  - Unlike traditional RNNs, state space models can be unrolled and trained in parallel, similar to Transformers, using a convolution-like implementation.
  - Objective: Next token prediction, identical to decoder-only models.
- **Inference**:
  - Highly efficient, as the model updates a fixed-size hidden state step-by-step, without needing to store or recompute past features (unlike Transformers, which require significant engineering for fast inference).
- **Challenges**:
  - **Context Compression**: Sequence models must compress arbitrarily long sequences into a fixed-size hidden state, a potential bottleneck compared to Transformers, which can attend to all past tokens.
  - **Long Sequences**: The ability to handle long sequences depends on whether the hidden state can effectively store all necessary information. If not, Transformers may remain superior.
- **Future Prospects**:
  - State space models could outperform Transformers if they overcome the context compression bottleneck, potentially leading to hybrid architectures or replacing Transformers entirely.
  - Currently, they are gaining traction but are not yet dominant.

---

## 4. Next Steps
The chapter concludes by outlining future topics:
- **Text Generation**: How decoder-only models generate sequences from probability distributions over tokens.
- **Instruction Tuning**: Molding pre-trained models into useful tools (e.g., chatbots, coding assistants) by teaching them to follow instructions.
- **Preference Tuning**: Guiding models to avoid dangerous or misleading answers.
- **Evaluation**: Measuring model performance, a challenging task as models and datasets grow.

---

## 5. Additional Context
- **Transformers' Impact**: Transformers revolutionized NLP by enabling parallel training (via teacher forcing) and efficient handling of long sequences, surpassing pre-Transformer models like RNNs and LSTMs, which processed sequences sequentially and struggled with vanishing gradients.
- **Data Scale**: Modern LLMs are trained on datasets comprising trillions of tokens, sourced from the internet, books, and licensed corpora. This scale, combined with architectural efficiency, drives their remarkable capabilities.
- **Ethical Considerations**: The use of internet data raises legal and ethical concerns, particularly regarding fair use and copyright, leading to increased reliance on licensed datasets.
- **Quantization**: Techniques like 4-bit quantization (e.g., in Llama 3.1) reduce model size and inference costs, making LLMs more practical for deployment while maintaining performance.
