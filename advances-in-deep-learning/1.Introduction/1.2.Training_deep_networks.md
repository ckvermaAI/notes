# Summary of the Lecture on Training Deep Networks

## Overview
Training a deep network involves three major components:
1. **Architecture**: The model structure, typically convolutional or transformer-based.
2. **Dataset**: Input data paired with labels or derived outputs.
3. **Loss Function**: Evaluates how well the model predictions align with the ground truth.

The goal is to optimize the network parameters \( \theta \) to minimize the loss function over the dataset.

---

## Purpose of Loss Functions
### Types of Loss Functions:
1. **Regression Loss** (for continuous labels):
   - **L1 Loss**: Absolute difference.
   - **L2 Loss** (Mean Squared Error): Squared difference.
   - Choice between L1 and L2 depends on the precision requirements.

2. **Classification Loss** (for discrete labels):
   - **Binary Classification**: Cross-entropy loss for true/false outcomes.
   - **Multi-class Classification**: Cross-entropy loss across multiple categories (e.g., dog, cat, cow).

3. **Embedding Loss**:
   - Used for tasks like matching text to images or comparing pairs of data.
   - Often reduced to specialized versions of classification loss (e.g., cross-entropy).

### Objective:
- The loss function \( L(f_\theta(x), y) \) measures the discrepancy between the model's predictions and labels.
- The training objective is to **minimize the expected loss** over the entire dataset using gradient descent.

---

## Optimizers
### Key Optimizer: **Adam (and AdamW)**
- **Why Popular?**
  - Combines the benefits of momentum and adaptive learning rates.
  - Works well across most tasks with minimal tuning.

### Adam Algorithm:
1. Maintains two momentum terms:
   - **First Momentum**: Weighted average of past gradients.
   - **Second Momentum**: Weighted average of squared gradients (gradient magnitude).
2. Normalizes these terms to stabilize updates.
3. **Weight Decay** (AdamW): Adds regularization to prevent unbounded growth of weights.

### Characteristics:
- **Parameters**: 
  - Model with \( n \) parameters requires \( 4n \) floating-point values due to Adam's tracking of gradients and momentum terms.
- **Efficiency**: Works well for general tasks but adds memory overhead due to its additional parameters.

---

## Training Workflow
1. **Define the Architecture**:
   - Choose or design a convolutional or transformer-based model.
2. **Prepare the Dataset**:
   - Collect and preprocess data, ensuring labels are available or derivable.
3. **Train with Loss and Optimizer**:
   - Minimize the loss using gradient descent and an optimizer like Adam/AdamW.

---

## Key Insights
- Training deep networks boils down to:
  - Minimizing a differentiable loss function over a dataset.
  - Using optimizers to iteratively adjust weights for better performance.
- The balance between memory efficiency and performance is critical for training large models.

