# Summary of the Lecture on Training Deep Networks

## Overview
Training a deep network involves three major components:
1. **Architecture**: The model structure, typically convolutional or transformer-based.
2. **Dataset**: Input data paired with labels or derived outputs.
3. **Loss Function**: Evaluates how well the model predictions align with the ground truth.

The goal is to optimize the network parameters $ \theta $ to minimize the loss function over the dataset.

---

## Purpose of Loss Functions
### Types of Loss Functions:
1. **Regression Loss** (for continuous labels):
   - **L1 Loss**: Absolute difference.
   - **L2 Loss** (Mean Squared Error): Squared difference.
   - Choice between L1 and L2 depends on the precision requirements.

2. **Classification Loss** (for discrete labels):
   - **Binary Classification**: Cross-entropy loss for true/false outcomes.
   - **Multi-class Classification**: Cross-entropy loss across multiple categories (e.g., dog, cat, cow).

3. **Embedding Loss**:
   - Used for tasks like matching text to images or comparing pairs of data.
   - Often reduced to specialized versions of classification loss (e.g., cross-entropy).

### Objective:
- The loss function $ L(f_\theta(x), y) $ measures the discrepancy between the model's predictions and labels.
- The training objective is to **minimize the expected loss** over the entire dataset using gradient descent.

---

## Optimizers
### Key Optimizer: **Adam (and AdamW)**
- **Why Popular?**
  - Combines the benefits of momentum and adaptive learning rates.
  - Works well across most tasks with minimal tuning.

### Adam Algorithm:
1. The Adam optimizer maintains additional parameters to track the first moment (mean of gradients) and second moment (uncentered variance of gradients) for each parameter in the model. These are typically stored as two separate tensors, each of the same shape as the model parameters.
2. Normalizes these terms to stabilize updates.
3. **Weight Decay** (AdamW): Adds regularization to prevent unbounded growth of weights.

### Characteristics:
- **Parameters**: 
  - Model with $ n $ parameters requires $ 3n $ floating-point values due to Adam's tracking of gradients and momentum terms.
- **Efficiency**: Works well for general tasks but adds memory overhead due to its additional parameters.

---

## Training Workflow
1. **Define the Architecture**:
   - Choose or design a convolutional or transformer-based model.
2. **Prepare the Dataset**:
   - Collect and preprocess data, ensuring labels are available or derivable.
3. **Train with Loss and Optimizer**:
   - Minimize the loss using gradient descent and an optimizer like Adam/AdamW.

---

## Key Insights
- Training deep networks boils down to:
  - Minimizing a differentiable loss function over a dataset.
  - Using optimizers to iteratively adjust weights for better performance.
- The balance between memory efficiency and performance is critical for training large models.

