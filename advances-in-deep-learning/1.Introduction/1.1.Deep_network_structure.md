# Summary of the Lecture on Deep Network Structure

## What is a Deep Network?
- A deep network is essentially a **differentiable function** \( $f(x)$ \) with learnable parameters \( $\theta$ \), designed to be optimized using gradient descent.
- Structured as a **stack of simple layers**, often forming a **computation graph** where outputs of some layers feed into others.

## Elements of a Deep Network
### Types of Layers
1. **Linear Layers**:
   - Include standard matrix multiplications and convolutions.
   - Hold most of the model's parameters (weights).
2. **Non-Linear Layers**:
   - Include:
     - **Normalization Layers**: Ensure values are on the right scale, aiding training.
     - **Activation Functions**: Add complexity by introducing non-linearities.
     - **Advanced Layers**: Such as attention (e.g., single-head attention) for information exchange and pooling for summarizing information.
   - Fewer trainable parameters but computationally more complex.

### Common Building Blocks
1. **Multi-Layer Perceptron (MLP)**:
   - Stacks of linear layers and activations.
   - A universal function approximator but prone to overfitting if too large.
2. **Transformer Block**:
   - Combines multi-head attention and MLP.
   - Includes residual connections for better training stability.
3. **Convolutional Block**:
   - Uses convolutions instead of linear layers with residual connections and normalization.
   - Produces image-like outputs with strided layers reducing input size and enriching features.

## Types of Deep Networks (Example)
1. **Convolutional Networks**:
   - Ideal for structured data like images with fixed width, height, and depth.
   - Often shrink input dimensions through strides while extracting richer features.
   - Used in generative models to reconstruct inputs.

2. **Transformers**:
   - Work with unordered inputs (e.g., tokens/embeddings).
   - Transform sets of input vectors into sets of output vectors of the same size.
   - Use **positional encodings** for ordered data like text or images.

## Input and Output Transformations
### Inputs:
- **Real-Valued Inputs**: Directly fed into the network.
- **Categorical Inputs**:
  - Transformed into **one-hot encodings** or **embeddings** (learned representations).

### Outputs:
- **Real-Valued Outputs**: Used for regression problems.
- **Categorical Outputs**: Predicted as probability distributions over categories to remain differentiable.

## Key Insights
- Deep networks are **modular**, allowing different components (e.g., MLPs, attention mechanisms) to be assembled based on the task.
- Both inputs and outputs must be transformed into real-valued vectors for effective gradient-based optimization.

## Summary
- A deep network is a **large, differentiable function** structured using convolutional or transformer-based architectures.
- Inputs and outputs are transformed as needed to enable training and inference.
