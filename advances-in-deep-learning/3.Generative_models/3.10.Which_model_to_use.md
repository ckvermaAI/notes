# Which Generative Model Should We Use?

## 1. High-Level Overview of Generative Models

Generative models aim to learn a data distribution and generate new samples from it. The major types include:

### **A. Autoencoders (AE)**
- Autoencoders compress input data into a latent representation and reconstruct it back.
- Consist of an **encoder** (compressing the data) and a **decoder** (reconstructing the input).
- Mostly used for **dimensionality reduction and denoising**, but not optimal for high-quality generation.

### **B. Variational Autoencoders (VAE)**
- A probabilistic extension of autoencoders that models the latent space using a probability distribution.
- Uses a **latent space regularization** (KL divergence) to ensure smooth sampling.
- Useful when **interpretability and efficient latent space exploration** are needed.
- However, generated samples tend to be blurry.

### **C. Generative Adversarial Networks (GANs)**
- Consist of a **generator** (G) and a **discriminator** (D) in an adversarial setting.
- G tries to generate realistic samples, while D tries to distinguish real from fake.
- Produces **high-quality, sharp images**, but suffers from **mode collapse** and unstable training.

### **D. Flow-Based Models**
- Use invertible transformations to directly model the data distribution.
- Enables **exact likelihood estimation** and **efficient sampling**.
- Often computationally expensive compared to other methods.

### **E. Diffusion Models**
- A sequence of transformations is applied to gradually remove noise from pure Gaussian noise.
- Very powerful and produces **high-quality images**.
- Requires **large compute resources** for training.

### **F. Auto-Regressive Models**
- Predicts the next data point given previous data points (e.g., PixelCNN, GPT).
- Works well for **sequential data like text and images**.
- Can model highly detailed distributions but is **slow at generation**.

---

## 2. Best Models for Generation Tasks

| Model Type          | Pros | Cons | Best Use Case |
|---------------------|------|------|--------------|
| **VAE** | Efficient, interpretable latent space | Blurry outputs | If latent space representation is important |
| **Diffusion Models** | High-quality generation, strong mode coverage | Computationally expensive | If high-quality samples are required and compute is available |
| **Auto-Regressive Models** | Good for sequential data, strong modeling capacity | Slow sampling | If output quality is a priority (e.g., text, images) |

---

## 3. When to Use Each Model (Based on Conditions)

From the reference image:

### **A. If Pre-Trained Models Exist**
- **Big Compute Available → Train a Diffusion Model**
- **Small Compute Available → Fine-Tune a Diffusion Model**

### **B. If No Pre-Trained Models Exist (Own Domain)**
- **Big Data, Big Compute → Train a Diffusion Model**
- **Small Data, Small Compute → Train a Variational Autoencoder (VAE)**

Additional insights:
- Diffusion models are the best choice when large compute resources are available.
- Auto-regressive models can be used when generation quality is critical but speed is not a concern.
- VAEs are useful when **data is scarce and interpretability is needed**.
