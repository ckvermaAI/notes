# **Variational Autoencoders (VAEs)**  

## **A) What are Autoencoders?**  

### **Structure:**  
An **autoencoder** is a type of neural network designed for **unsupervised learning**. It consists of two main components:  
1. **Encoder**: Maps input data \( $x$ \) to a lower-dimensional latent space \( $z$ \).  
2. **Decoder**: Reconstructs \( $x$ \) from \( $z$ \), trying to preserve key features.  

Mathematically:  
$z = f(x)  \quad \text{(Encoder)}$; $\hat{x} = g(z)  \quad \text{(Decoder)}$
  

### **Training Objective:**  
Autoencoders are trained to minimize **reconstruction loss**, typically measured as:  
$L = || x - \hat{x} ||^2$
  
This ensures that the output \( $\hat{x}$ \) closely resembles the input \( $x$ \).  

---

## **B) General Advantages and Limitations of Autoencoders as Generative Models**  

### **Advantages:**  
‚úÖ **Feature Learning**: Autoencoders can learn meaningful latent representations.  
‚úÖ **Dimensionality Reduction**: Similar to PCA but non-linear, making it more flexible.  
‚úÖ **Noise Reduction**: Can be used for denoising by training on noisy inputs and clean outputs.  

### **Limitations:**  
‚ùå **Deterministic Mapping**: Standard autoencoders encode each input into a fixed point in the latent space. This makes them poor generative models.  
‚ùå **Lack of Variability**: They fail to generate novel samples since their latent space does not capture a probability distribution.  
‚ùå **Overfitting Risk**: If trained improperly, they may memorize inputs instead of learning general representations.  

These limitations motivate the introduction of **Variational Autoencoders (VAEs)**.  

---

## **C) Variational Autoencoders (VAEs) ‚Äì A Probabilistic Autoencoder**  

Instead of encoding inputs to fixed latent points, **VAEs learn a probability distribution over the latent space**. This is achieved by:  

1. **Defining a probabilistic encoder** \( $Q(z|x)$ \), which estimates a distribution over latent variables given the input.  
2. **Assuming a prior** on the latent space, typically a standard normal distribution \( $P(z) = \mathcal{N}(0, I)$ \).  
3. **Using a probabilistic decoder** \( $P_D(x|z)$ \) that generates samples from \( z \).  

Mathematically, the posterior \( $P_E(z|x)$ \) is intractable, so we approximate it with \( $Q(z|x)$ \) using **KL-divergence minimization**:  $D_{KL}(Q(z|x) || P_E(z|x))$  

---

## **D) Evidence Lower Bound (ELBO) and the Reparameterization Trick**  

### **ELBO ‚Äì Evidence Lower Bound**  

We aim to maximize the marginal log-likelihood of the data:  
$\log P(x) = D_{KL}(Q(z|x) || P_E(z|x)) + \mathbb{E}_{z \sim Q} \left[ \log \frac{P(z) P_D(x|z)}{Q(z|x)} \right]$  

Since \( $D_{KL}(\cdot)$ \) is always non-negative, maximizing the **ELBO** is equivalent to maximizing \( $\log P(x)$ \):  

$\log P(x) - D_{KL}(Q(z|x) || P_E(z|x)) = \mathbb{E}_{z \sim Q} \left[ \log \frac{Q(z|x)}{P(z) P_D(x|z)} \right]$  

### **Reparameterization Trick**  

To backpropagate through a stochastic node, we use the **reparameterization trick**:  

1. Instead of sampling directly from \( $Q(z|x)$ \), we express it as:  
   $z = \mu_Q(x) + \sigma_Q(x) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$
2. This allows gradients to flow through \( $\mu_Q(x)$ \) and \( $\sigma_Q(x)$ \), making training possible.  

---

## **E) Final Notes ‚Äì Issues and Limitations of VAEs**  

üî¥ **Pixel-level \( L_2 \) Loss**: VAEs typically use squared error loss, which can lead to **blurry reconstructions**.  
üî¥ **Gaussian Assumption in Latent Space**:  
   - The assumption that \( $Q(z|x)$ \) follows a normal distribution limits flexibility.  
   - Leads to **inefficient sphere packing** in high-dimensional latent spaces, leaving large areas of unused space.  
üî¥ **Sample Quality**: VAEs tend to produce lower-quality images compared to GANs, though they offer better interpretability.  

---

## **Conclusion**  

VAEs provide a **probabilistic approach** to autoencoders, allowing for **generative modeling** by learning a latent distribution. While they have advantages in **structured sampling** and **smooth interpolation**, they suffer from **blurriness** and **restrictive Gaussian assumptions**. Future improvements involve **better priors, hierarchical VAEs, and alternative loss functions**.  

---
