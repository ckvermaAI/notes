# Flow-Based Models

## Recap of VAE and GAN

### Variational Autoencoder (VAE)
- **Process**: Image → Latent Space → Image
- **Loss Function**: Encourages the latent space to follow a Gaussian distribution.

### Generative Adversarial Network (GAN)
- **Process**: Gaussian → Image
- **Loss Function**: Compares generated samples with real data distribution.

## What Are Flow-Based Models?

Flow-based models are a family of generative models that learn an invertible transformation between a simple prior distribution (e.g., Gaussian) and the data distribution. The key feature of these models is that they allow both:
1. **Efficient Sampling**: Since the transformation is invertible, we can generate new data points from a simple latent distribution.
2. **Exact Likelihood Computation**: The change-of-variable formula allows direct computation of the likelihood of a data point.

### Objective of Flow-Based Models
- Assume a generative function $(G: z \rightarrow x)$ that is invertible.
- Define prior distribution $P(Z) = \mathcal{N}(0,1)$.
- Compute $P(x)$ using the change-of-variable formula:
$P(x) = P(z) \left| \det \left( \frac{\partial z}{\partial x} \right) \right|$

- Maximize log-likelihood $(\log P(x))$ on training images.

## Example of an Invertible Layer
An essential component of flow-based models is an **invertible transformation**. One such transformation splits input variables into two groups and applies an affine transformation to one group using the other.

### Steps:
1. **Split inputs** into two groups: \( x_1, x_2 \).
2. **Transformation**:
   - $y_1 = x_1$
   - $y_2 = \exp(s(x_1)) \odot x_2 + t(x_1)$
3. **Inverse transformation**:
   - $x_1 = y_1$
   - $x_2 = \exp(-s(y_1)) \odot (y_2 - t(y_1))$

This transformation ensures that both forward and inverse computations are easy, making the model efficient for both sampling and density estimation.

## Advantages and Limitations of Flow-Based Models

### **Advantages**
- **Exact Log-Likelihood**: Unlike VAEs (which approximate likelihood), flow-based models compute exact likelihood.
- **Efficient Sampling**: Unlike GANs (which rely on adversarial training), these models directly generate samples from a known distribution.
- **Invertibility**: Each layer is reversible, making it possible to map between latent and data spaces without loss.

### **Limitations**
- **Computational Cost**: The requirement to compute the determinant of the Jacobian matrix makes training expensive.
- **Architectural Constraints**: The transformations must be carefully designed to remain invertible and computationally efficient.
- **Less Expressive Than GANs**: While they provide better likelihood estimation, the quality of generated samples may not match that of GANs.

## Flow-Based Models as Sampling-Based and Density Estimation-Based Models

Flow-based models fall into **both** categories:
1. **Sampling-Based Model**: Since they provide an invertible transformation, we can generate high-quality samples from a Gaussian prior.
2. **Density Estimation Model**: The change-of-variable formula allows exact likelihood computation, making these models useful for estimating data distributions.

Thus, flow-based models bridge the gap between VAEs (which are good at density estimation) and GANs (which are good at sample generation), making them a powerful tool in generative modeling.

---
