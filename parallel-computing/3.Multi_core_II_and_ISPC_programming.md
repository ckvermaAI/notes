# Multi-core Architecture II and ISPC Programming

This lecture, part of Stanford CS149 (Parallel Computing), dives into advanced concepts of multi-core architecture, focusing on hardware multithreading, ISPC (Intel SPMD Program Compiler) programming, and the critical role of memory bandwidth in modern computing. The lecture builds on previous discussions about multi-core processors, SIMD (Single Instruction, Multiple Data) execution, and introduces hardware multithreading as a mechanism to improve processor utilization. It also explores the ISPC programming model, emphasizing the distinction between semantics and implementation in parallel computing, and concludes with a discussion on memory bandwidth as a limiting factor in achieving high efficiency on modern processors like GPUs.

---

## 1. Review of Hardware Multithreading

### 1.1 Core Concepts
- **Hardware Multithreading Overview**: Hardware multithreading allows a processor to manage multiple threads by maintaining separate execution contexts (state, such as registers) for each thread on the core. This does not increase the core's ALU (Arithmetic Logic Unit) resources but improves their utilization by avoiding stalls during high-latency operations like memory accesses.
  - **Takeaway 1**: A multi-threaded processor avoids stalls by executing instructions from other threads when one thread is waiting for a long-latency operation (e.g., memory access). The latency of the operation itself is unchanged; only the processor's utilization improves.
  - **Takeaway 2**: Programs with more arithmetic operations per memory access require fewer threads to hide memory stalls, as there are fewer opportunities for latency to impact performance.

- **Types of Hardware Multithreading**:
  - **Interleaved Multithreading (Temporal Multithreading)**: Each clock cycle, the core selects one thread and executes an instruction from that thread on the core’s ALUs. This ensures the core remains busy even if one thread stalls.
  - **Simultaneous Multithreading (SMT)**: The core can execute instructions from multiple threads in the same clock cycle, utilizing multiple ALUs. For example, Intel’s Hyper-Threading supports two threads per core, allowing the core to pick instructions from either thread to fill its execution units.

- **Processor Utilization**:
  - A multi-threaded core can achieve 100% utilization by switching between threads during stalls. For example, if a thread stalls for 12 cycles due to a memory access, and each thread provides 3 cycles of arithmetic work, 5 threads are needed to cover the stall (1 stalled thread + 4 additional threads to cover the 12-cycle latency).
  - However, this comes at a cost: each thread takes longer to complete because it shares the core with other threads, increasing the completion latency for individual threads.

### 1.2 Example: Hiding Memory Latency
- **Scenario**: A program performs 3 arithmetic operations followed by a memory access with a 12-cycle latency. Without multithreading, the processor is busy for 3 cycles and stalls for 12, resulting in a utilization of $3 / (3 + 12) = 1/5 = 20\%$.
- **With Multithreading**: Adding more threads allows the core to execute arithmetic from other threads during the stall. With 5 threads, the core can cover the 12-cycle stall (each thread provides 3 cycles of work, so 4 additional threads cover the remaining 12 cycles), achieving 100% utilization.
- **Impact of Program Characteristics**: If the program is modified to perform 6 arithmetic operations per memory access, the utilization increases to $6 / (6 + 12) = 33\%$. Only 3 threads are needed for 100% utilization (each thread provides 6 cycles of work, covering the 12-cycle stall with 2 additional threads).

### 1.3 Costs and Trade-offs
- **Chip Space**: Multithreading requires additional chip space to store execution contexts for multiple threads.
- **Thread Completion Latency**: Each thread takes longer to complete because it shares the core with other threads.
- **Design Choices**: A designer can trade off between the number of threads and the size of each thread’s register file. For example, supporting more threads might mean smaller register files per thread, or fewer threads might allow for larger register files.

---

## 2. Modern Processor Architecture

### 2.1 Example Processor Design
- **Fake Chip**: A 16-core processor where each core is 4-way multithreaded and can execute one 8-wide SIMD instruction per clock.
  - **Peak Throughput**: $16 \times 8 = 128$ data elements processed per clock.
  - **Latency Hiding**: To hide memory latency, the processor needs $16 \times 4 \times 8 = 512$ independent tasks (threads or data elements) to fully utilize its resources.

- **Intel Core Approximation**: A modern Intel core (e.g., in a "myth" machine) might be 2-way multithreaded (Hyper-Threading), superscalar (can execute multiple instructions per clock), and support SIMD (e.g., 8-wide vector instructions).
  - **Execution Units**: The core might have 3 8-wide vector ALUs, allowing up to 3 vector operations per clock, or $3 \times 8 = 24$ floating-point operations per clock.
  - **Superscalar and Multithreading**: The core can fetch instructions from both threads, find independent instructions (e.g., a mix of scalar and vector operations), and execute them in parallel to saturate its ALUs.

- **NVIDIA GPU Core**: An NVIDIA GPU core might support 64 threads, execute 4 32-wide vector operations per clock, and have 144 such cores. To fully utilize the GPU, it needs $64 \times 144 \times 32 \approx 300,000$ independent tasks, making it suitable for highly parallel workloads like deep neural networks (DNNs) but inefficient for smaller tasks.

### 2.2 GPU SIMT (Single Instruction Multiple Thread)
- **SIMT Execution**: Unlike Intel CPUs, which emit vector instructions, NVIDIA GPUs execute scalar instructions but detect when multiple threads (up to the SIMD width) are at the same program counter. The GPU then executes these threads simultaneously on SIMD ALUs.
  - **Masking**: If a thread diverges (e.g., thread 6 is not executing the same instruction), its ALU lane is masked off, reducing efficiency.

---

## 3. ISPC Programming

### 3.1 Introduction to ISPC
- **ISPC Overview**: ISPC (Intel SPMD Program Compiler) is a language designed for single program, multiple data (SPMD) programming. It allows developers to write a single program that runs on multiple data elements in parallel, leveraging SIMD and multithreading.
  - **SPMD Model**: A single program is executed by multiple "program instances," each working on different data. ISPC abstracts the assignment of work to these instances, allowing the system to optimize execution.
  - **Key Variables**:
    - `programCount`: The total number of program instances (e.g., 8 if the gang size is 8).
    - `programIndex`: The ID of the current program instance (e.g., 0 to 7 for 8 instances).

- **Example Program**: Compute the sine of each element in an array `x` of size `N` and store the results in `result`.
  - **C++ Code (main.cpp)**:
    ```cpp
    #include "sinx.h"
    int main(int argc, void** argv) {
        int N = 1024;
        int terms = 5;
        float* x = new float[N];
        float* result = new float[N];
        // initialize x here
        sinx(N, terms, x, result);
        return 0;
    }
    ```
  - **C++ Code (sinx.cpp)**:
    ```cpp
    void sinx(int N, int terms, float* x, float* result) {
        for (int i = 0; i < N; i++) {
            float value = x[i];
            float numer = x[i] * x[i] * x[i];
            int denom = 6; // 3!
            int sign = -1;
            for (int j = 1; j <= terms; j++) {
                value += sign * numer / denom;
                numer *= x[i] * x[i];
                denom *= (2 * j + 2) * (2 * j + 3);
                sign *= -1;
            }
            result[i] = value;
        }
    }
    ```
  - **ISPC Code (sinx.ispc)**:
    ```ispc
    export void ispc_sinx(uniform int N, uniform int terms, uniform float* x, uniform float* result) {
        for (uniform int i = 0; i < N; i += programCount) {
            int idx = i + programIndex;
            float value = x[idx];
            float numer = x[idx] * x[idx] * x[idx];
            uniform int denom = 6; // 3!
            uniform int sign = -1;
            for (uniform int j = 1; j <= terms; j++) {
                value += sign * numer / denom;
                numer *= x[idx] * x[idx];
                denom *= (2 * j + 2) * (2 * j + 3);
                sign *= -1;
            }
            result[idx] = value;
        }
    }
    ```

### 3.2 Semantics vs. Implementation
- **Semantics**: The meaning of the program—what answer does it compute? For the `sinx` program, each element `result[i]` should contain the sine of `x[i]` computed using a Taylor series approximation with `terms` iterations.
- **Implementation**: How the program is executed on a parallel machine—how are iterations assigned to program instances, and in what order are operations performed?

- **ISPC Execution Model**:
  - When the C++ `main` function calls the ISPC function `ispc_sinx`, ISPC spawns a "gang" of program instances (e.g., 8 instances if `programCount = 8`).
  - Each instance has its own copy of local variables (e.g., `value`, `numer`) and executes the program logic with a unique `programIndex` (0 to 7).
  - Upon completion, control returns to the C++ caller.

- **Work Assignment**:
  - **Interleaved Assignment**: In the first ISPC version, iterations are interleaved across instances. Instance 0 computes elements 0, 8, 16, ..., instance 1 computes elements 1, 9, 17, ..., and so on. This is achieved by the loop `for (uniform int i = 0; i < N; i += programCount)` and `int idx = i + programIndex`.
  - **Blocked Assignment**: In a modified version, each instance computes a contiguous block of the array (e.g., instance 0 computes elements 0 to $N/8 - 1$, instance 1 computes $N/8$ to $2N/8 - 1$, etc.), achieved by adjusting the loop bounds.
  - **Dynamic Assignment**: Another version uses a dynamic approach where a global counter (`nextIter`) is atomically incremented to assign iterations to instances.

### 3.3 Foreach Abstraction
- **Foreach Construct**: ISPC provides a `foreach` construct to abstract the assignment of iterations to program instances:
  ```ispc
  foreach (i = 0 ... N) {
      // do work for iteration i here...
  }
  ```
- **Possible Implementations**:
  - **Program Instance 0 Executes All Iterations**: A single instance executes all iterations sequentially.
  - **Interleave Iterations**: Iterations are interleaved across instances (as in the first ISPC version).
  - **Block Iterations**: Iterations are divided into contiguous blocks (as in the second ISPC version).
  - **Dynamic Assignment**: Iterations are dynamically assigned using a global counter.

- **Key Insight**: All these implementations are valid as long as the program computes the correct result. The `foreach` construct allows the system to choose the best implementation, freeing the programmer from manual work assignment.

---

## 4. Memory Bandwidth as a Critical Resource

### 4.1 Bandwidth vs. Latency
- **Latency**: The time to complete one operation (e.g., a memory access taking 12 cycles).
- **Bandwidth**: The rate of data transfer (e.g., 8 bytes per clock from memory).
- **Analogy (Highway 101)**:
  - Driving from San Francisco to Stanford (50 km at 100 km/h) takes 30 minutes (latency).
  - With one car on the road at a time, the throughput is 2 cars per hour.
  - Increasing bandwidth can be achieved by:
    - Driving faster (reduces latency to 15 minutes, increases throughput to 4 cars per hour).
    - Adding lanes (e.g., 4 lanes increase throughput to 8 cars per hour, latency unchanged).
    - Driving bumper-to-bumper (e.g., spacing cars 1 km apart at 100 km/h results in 1 car every 36 seconds, or 100 cars per hour).

- **Pipelining**: By dividing the road into segments (or a task into stages, like laundry with washing, drying, and folding), throughput can be increased without reducing latency. For laundry:
  - Washing: 45 minutes, Drying: 60 minutes, Folding: 15 minutes.
  - Latency for one load: $45 + 60 + 15 = 120$ minutes.
  - Throughput: Limited by the slowest stage (drying, 60 minutes), so 1 load per hour.

### 4.2 Bandwidth in Computing
- **Example Program**: A program performs a sequence of instructions: load 64 bytes, add, add, repeated across many threads.
  - **Processor**: Executes 1 math operation per clock, can issue loads in parallel, and receives 8 bytes per clock from memory.
  - **Execution**:
    - A load takes 8 cycles to transfer 64 bytes (8 bytes per clock).
    - With a limit of 3 outstanding loads, the processor stalls when the request queue is full, waiting for memory to complete a load before issuing another.
    - Result: Memory is fully utilized (8 bytes per clock), but the processor is stalled most of the time, leading to low overall efficiency.

- **Thought Experiment (GPU Efficiency)**:
  - **Task**: Element-wise multiplication of two vectors $A$ and $B$ with millions of elements: load $A[i]$, load $B[i]$, compute $A[i] \times B[i]$, store $C[i]$.
  - **Memory Traffic**: 3 memory operations (12 bytes) per multiplication.
  - **NVIDIA V100 GPU**:
    - Can perform 5120 FP32 multiplications per clock at 1.6 GHz, or $5120 \times 1.6 \times 10^9 \approx 8$ trillion operations per second.
    - Requires $8 \times 10^{12} \times 12 \approx 96$ TB/sec of bandwidth to keep ALUs busy.
    - Actual bandwidth: ~900 GB/sec, leading to an efficiency of less than 1%.
  - **Comparison with CPU**: An 8-core Xeon E5v4 CPU at 3.2 GHz with a 76 GB/sec memory bus achieves ~3% efficiency on the same task.

### 4.3 Strategies for Efficient Programs
- **Minimize Memory Accesses**:
  - Reuse data (temporal locality).
  - Share data across threads (inter-thread cooperation).
  - Perform additional arithmetic instead of storing/reloading values (arithmetic is "free" compared to memory access).
- **Main Point**: Programs must access memory infrequently to utilize modern processors efficiently. Caches only help if data is reused, not for streaming access patterns like the vector multiplication example.

---

## 5. Thread Scheduling and OS Role

- **Thought Experiment**:
  - A C program spawns 2 threads on a processor with 2 cores, each with 2 execution contexts, and can execute one 8-wide SIMD instruction per clock.
  - **Question**: Who maps threads to execution contexts? **Answer**: The operating system (OS).
  - **Scheduling Decision**: The OS should place one thread on each core to maximize resource utilization, as placing both threads on the same core would lead to contention for execution resources.
  - **Five Threads Scenario**: With 5 threads, the OS might distribute them as evenly as possible (e.g., 3 threads on one core, 2 on the other), but this could lead to uneven utilization and potential interference (e.g., cache contention).

- **OS vs. Hardware Decisions**:
  - The OS assigns threads to execution contexts (a slow process, taking hundreds of thousands of cycles).
  - The hardware decides which instructions to execute each clock cycle (a fast decision, made billions of times per second).

---

## 6. Key Takeaways for Modern Parallel Processors

- **Requirements for Efficiency**:
  1. **Sufficient Parallel Work**: To utilize all execution units across cores and within each core.
  2. **Coherent Control Flow**: Groups of parallel work items must execute the same instructions to leverage SIMD execution.
  3. **Excess Parallel Work**: To enable interleaving of threads to hide memory stalls.

- **Terminology**:
  - **Instruction Stream**: A sequence of instructions executed by a thread.
  - **Multi-core Processor**: Multiple independent cores on a chip.
  - **SIMD Execution**: Single instruction applied to multiple data elements.
  - **Coherent Control Flow**: Threads executing the same instruction sequence.
  - **Hardware Multithreading**:
    - **Interleaved Multithreading**: One thread’s instruction per clock.
    - **Simultaneous Multithreading**: Multiple threads’ instructions per clock.

---

## 7. Additional Notes

- **Why ISPC?**: ISPC is used in the course to illustrate parallel programming concepts clearly. It avoids the complexity of auto-parallelizing arbitrary C++ code and provides explicit control over parallelism through `programCount` and `programIndex`.
- **Further Reading**: The lecture recommends "The Story of ISPC" by Matt Pharr for insights into why ISPC achieves better performance than C++ for parallel workloads.
- **Practical Implications**: The concepts discussed (multithreading, SIMD, bandwidth limitations) are critical for writing efficient parallel programs, especially for applications like machine learning and computer graphics, which have high arithmetic-to-memory ratios.