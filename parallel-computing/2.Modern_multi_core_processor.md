# Modern Multi-Core Processors

This lecture, part of Stanford's CS149 course in Fall 2023, focuses on modern multi-core processors from a software engineering perspective, emphasizing parallel execution, memory latency challenges, and optimization strategies. The lecture builds on previous discussions about computer programs, processor execution, and memory, introducing three key forms of parallelism—superscalar execution, SIMD (Single Instruction, Multiple Data), and multi-core processing—along with techniques like multi-threading to hide memory stalls. Below is a detailed summary of the lecture, incorporating insights from the transcript and slides, with additional context where relevant.

---

## Introduction and Review of Key Concepts

The lecture begins with a review of foundational concepts from the previous session to ensure understanding:

- **What is a Computer Program?**
  - A computer program is a list of instructions or commands that a processor executes. These instructions modify the state of the machine, which includes **registers** (fast, on-chip storage) and **main memory** (larger, off-chip storage like DRAM).
  - The processor's role is to execute these commands, with control units (fetch/decode) determining the next operation and execution units performing the actual computation.

- **Processor Execution and Dependencies**
  - Instructions in a program are often represented as a dependency graph, where some instructions must wait for others to complete (e.g., instruction 2 may depend on instructions 0 and 1 if it needs their results).
  - However, not all instructions are dependent. Independent instructions can be executed in parallel, a concept known as **Instruction-Level Parallelism (ILP)**, which modern processors exploit to improve performance.

- **Memory as an Abstraction**
  - Memory is logically an array of bytes, where each address corresponds to a value. A program can read or write to specific addresses (e.g., "read the value at address 42" or "write 'foobar' to address 42").
  - The implementation of memory involves **DRAM** (Dynamic Random-Access Memory), which is off-chip and slower due to its physical distance from the processor. DRAM is "dynamic" because it loses data when power is cut and even reading a value destroys it, requiring the value to be rewritten.

- **Caches to Reduce Memory Latency**
  - To mitigate the latency of accessing DRAM (which can take ~250 cycles), modern systems use **caches**, smaller and faster on-chip storage that replicates frequently accessed data.
  - Caches operate at the granularity of **cache lines** (e.g., 64 bytes on Intel chips, simplified to 4 bytes in the lecture's examples). When a processor requests a single byte (e.g., address 5), the cache fetches the entire cache line (e.g., addresses 4–7).
  - **Cache Misses and Hits:**
    - A **cold miss** occurs the first time data is accessed, as it’s not in the cache (e.g., accessing address 0 for the first time).
    - A **hit** occurs when the data is already in the cache (e.g., subsequent accesses to addresses 1, 2, 3 after fetching the cache line starting at address 0).
    - A **capacity miss** happens when the cache is full, and a line must be evicted to make space (e.g., evicting the least recently used line when accessing a new address like 4).
    - The lecture briefly mentions **conflict misses**, which occur due to the cache's organization (e.g., when data can only be placed in specific slots, causing evictions even if the cache isn’t full), but this is deferred for later discussion.
  - Caches exploit two types of locality:
    - **Temporal Locality:** If data is accessed once, it’s likely to be accessed again soon, so keeping it in the cache speeds up future accesses.
    - **Spatial Locality:** If a program accesses address 0, it’s likely to access nearby addresses (e.g., 1, 2, 3), so fetching a whole cache line prefetches nearby data.
  - The lecture highlights the latency hierarchy: an L1 cache hit takes ~4 cycles, while a DRAM access can take ~250 cycles, emphasizing the importance of caches in avoiding performance bottlenecks.

---

## Today's Focus: Parallel Execution and Multi-Core Processors

The lecture introduces the main topic: understanding modern multi-core processors to achieve high throughput and address memory latency challenges. The goals are to:

- Understand two key parallel execution methods: **multi-core** and **SIMD**.
- Address memory latency challenges through **multi-threading**.
- Help students optimize parallel programs and identify workloads that benefit from parallel machines.

---

## Example Program: Computing $sin(x)$ Using Taylor Expansion

The lecture uses a running example to illustrate parallel execution concepts. The program computes $sin(x)$ for an array of $N$ floating-point numbers using the Taylor expansion:

$sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots$

### Code Overview
```c
void sinx(int N, int terms, float* x, float* y)
{
    for (int i=0; i<N; i++)
    {
        float value = x[i];
        float numer = x[i] * x[i] * x[i];
        int denom = 6; // 3!
        int sign = -1;

        for (int j=1; j<=terms; j++)
        {
            value += sign * numer / denom;
            numer *= x[i] * x[i];
            denom *= (2*j+2) * (2*j+3);
            sign *= -1;
        }
        y[i] = value;
    }
}
```

- **Input:** An array `x` of $N$ floating-point numbers.
- **Output:** An array `y` where `y[i] = sin(x[i])`.
- **Inner Loop:** Approximates $sin(x[i])$ using the Taylor series for a specified number of `terms`.
- **Outer Loop:** Iterates over each element of the array, computing $sin(x[i])$ independently for each `i`.

The diagram illustrates the process: for each element `x[i]` in the input array, the program computes `y[i]`, storing the result in the output array.

---

## Three Forms of Parallel Execution

The lecture introduces three orthogonal methods of parallel execution, each with distinct characteristics:

### 1. Superscalar Execution
- **Definition:** Exploits ILP within a single instruction stream by processing multiple independent instructions in parallel within a core.
- **Mechanism:** The hardware dynamically identifies independent instructions (e.g., if instruction 2 doesn’t depend on instruction 1, they can execute simultaneously).
- **Implementation:** A superscalar processor has multiple execution units (e.g., two ALUs) and fetch/decode units to handle multiple instructions per clock cycle.
- **Example:** A single-core superscalar processor can execute up to two instructions per clock from a single instruction stream if they are independent.
- **Key Point:** This is a hardware optimization invisible to the programmer; the program remains a single instruction stream, but the processor reorders instructions for efficiency.

### 2. SIMD (Single Instruction, Multiple Data)
- **Definition:** Multiple ALUs within a core execute the same instruction on different data elements simultaneously.
- **Mechanism:** A single instruction stream controls multiple ALUs (e.g., 8 ALUs in the lecture’s example). Each ALU operates on a different data element, using vector registers (e.g., 8-wide vectors).
- **Efficiency:** Ideal for **data-parallel workloads** where the same operation is applied to many data elements (e.g., computing $sin(x)$ for each element of an array).
- **Vectorization:** Can be explicit (via compiler-generated vector instructions) or implicit (handled by hardware).
- **Example:** A quad-core SIMD processor with 8-wide vector instructions can process 32 elements per clock (4 cores × 8 elements per core).
- **Program Transformation:** The example program is rewritten to use vector instructions:
  - Variables are converted to 8-wide vectors (e.g., `float` becomes a vector of 8 floats).
  - The outer loop iterates in steps of 8 (`for i=0; i<N; i+=8`).
  - Each iteration processes 8 elements simultaneously using vector operations.
- **Benefit:** Amortizes the cost of fetch/decode over many ALUs, improving efficiency for coherent workloads.

### 3. Multi-Core Processing
- **Definition:** Uses multiple processing cores, each capable of executing a completely different instruction stream.
- **Mechanism:** Each core has its own fetch/decode unit, execution unit, and execution context (registers). Software creates threads to expose parallelism to the hardware (e.g., via a threading API like Pthreads).
- **Example:** A dual-core processor can execute two instruction streams simultaneously, one on each core.
- **Trade-Off:** To fit more cores on a chip, architects may reduce per-core resources (e.g., smaller caches, less superscalar logic), making each core slower but allowing more parallel work overall.
- **Program Transformation:** The example program is modified to use two threads:
  - One thread processes the first half of the array (`x[0]` to `x[N/2-1]`).
  - The other thread processes the second half (`x[N/2]` to `x[N-1]`).
  - This ensures each core works on a different portion of the array, exploiting the outer loop’s parallelism.

---

## Combining Parallel Execution Methods

The lecture demonstrates how these methods can be combined:

- **16-Core Processor with SIMD:** A 16-core processor, where each core has 8-wide SIMD, can process $16 \times 8 = 128$ elements per clock cycle.
- **Example Hardware:**
  - An Intel CPU with 10 cores.
  - NVIDIA RTX 4090 with 144 streaming multiprocessors (SMs), each acting as a core.
  - Apple’s Bionic chip with 6 heterogeneous cores (2 high-performance, 4 efficiency cores).
- **Peak Performance:** A 4-core processor with 3-way superscalar and 8-wide SIMD can achieve $4 \times 3 \times 8 = 96$ operations per clock. An NVIDIA GPU with 80 cores, each with 32-wide SIMD, achieves $80 \times 32 = 2560$ operations per clock.

---

## Challenges: Instruction Stream Coherence and Divergence

### Coherent Execution
- **Definition:** A program where all iterations of a loop execute the same instruction sequence (e.g., all elements follow the same control path).
- **Importance:** Coherent execution is necessary for SIMD to be efficient, as all ALUs must execute the same instruction.
- **Example:** In the $sin(x)$ program, all iterations perform the same computation, making it ideal for SIMD.

### Divergent Execution
- **Definition:** A lack of instruction stream coherence, where different iterations take different control paths (e.g., some execute an `if` branch, others an `else` branch).
- **Impact on SIMD:** In a divergent program, SIMD ALUs may idle:
  - Example: A loop with an `if-else` statement where 3 out of 8 iterations take the `if` branch and 5 take the `else` branch.
  - The processor executes the `if` branch for all 8 ALUs, masking off the 5 that don’t need it (3/8 utilization), then executes the `else` branch, masking off the 3 that don’t need it (5/8 utilization).
- **Worst Case:** If only 1 out of 8 iterations takes the `if` branch (which is expensive) and 7 take the `else` branch (which is cheap), the processor runs at 1/8 utilization, as 7 ALUs are idle during the `if` branch.
- **Vector Widths:** Modern hardware varies in vector width (e.g., 4-wide on mobile ARM chips, 32-wide on high-end GPUs), amplifying the impact of divergence on wider vectors.

---

## Hiding Memory Stalls with Multi-Threading

### The Problem: Memory Latency
- Multi-core and SIMD processors increase the demand for memory accesses, but smaller caches (to fit more cores) and reduced prefetching logic increase the likelihood of cache misses.
- A cache miss to DRAM can stall a thread for ~250 cycles, leaving ALUs idle.

### Solution: Multi-Threading
- **Concept:** A core can hold state for multiple threads (e.g., 4 hardware threads per core). If one thread stalls on a memory access, the core switches to another thread, keeping the ALUs busy.
- **Diagram:** A core with 4 execution contexts (one per thread) runs 4 threads. When Thread 1 stalls on a load, the core switches to Thread 2, then Thread 3, and so on. By the time Thread 4 stalls, Thread 1’s data may be ready, allowing continuous execution.
- **Utilization:** This approach achieves 100% ALU utilization, as the core is always executing instructions from some thread.
- **Trade-Off:** The latency to complete any single thread increases (e.g., Thread 1 finishes later because it waits while other threads run), but overall throughput improves because the core remains busy.

### Real-Life Analogy
- The lecturer compares this to everyday tasks: while waiting for laundry to finish or water to boil, you do other tasks (e.g., homework). Similarly, a processor switches to another thread during a stall, ensuring continuous work.

---

## Additional Concepts

### Prefetching
- Some processors use **prefetching** to predict and preload data into the cache (e.g., in a predictable access pattern like iterating through an array). However, this is less effective for irregular patterns (e.g., linked list traversal or array lookups based on computed indices).

### Cache Hierarchy
- Modern systems use a hierarchy of caches (L1, L2, L3) to balance latency and capacity:
  - L1: Small, fast (~4 cycles).
  - L2/L3: Larger, slower.
  - DRAM: Largest, slowest (~250 cycles).
- Larger caches have higher latency and energy costs, a trade-off in multi-core designs.

### Historical Context
- Vector instructions (e.g., SSE on x86) emerged in the early 2000s, initially 4-wide for graphics operations (RGBA), and have since expanded to wider vectors (e.g., 32-wide on GPUs).

---

## Key Jargon

- **Instruction Stream Coherence ("Coherent Execution"):** When all iterations of a loop follow the same control path, enabling efficient SIMD execution.
- **Divergent Execution:** When iterations take different control paths, reducing SIMD efficiency.
- **Capacity Miss:** A cache miss due to limited cache size, requiring eviction of a line.
- **Conflict Miss:** A cache miss due to the cache’s organization (e.g., data mapping restrictions), even if the cache isn’t full.
- **Cold Miss:** A cache miss because the data has never been accessed before.
- **Temporal Locality:** The tendency to access the same data repeatedly in a short time.
- **Spatial Locality:** The tendency to access data at nearby addresses.

---

## Conclusion

The lecture provides a comprehensive overview of modern multi-core processors, focusing on three forms of parallelism—superscalar, SIMD, and multi-core—and their interplay. It emphasizes the importance of coherent execution for SIMD efficiency, the role of multi-threading in hiding memory stalls, and the trade-offs in processor design (e.g., smaller caches for more cores). Students are encouraged to apply these concepts to optimize parallel programs, with the lecture setting the stage for further discussions on memory systems and superscalar execution in the next session. The example program and diagrams effectively illustrate how to exploit parallelism at different levels, from instruction-level to thread-level, while addressing the critical challenge of memory latency.